{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "384198fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ML Applicaiton: Applying ML Models to the transition data\n",
    "\n",
    "### Tasks included:\n",
    "- Reading transition matrices \n",
    "- Reading P/AUC scores \n",
    "- Apply ML models \n",
    "- Select features and apply ML models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdd8f50",
   "metadata": {},
   "source": [
    "## Imports and system info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae36650c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "List of OS platforms and codes\n",
      "___________________________________________\n",
      "0 Darwin\n",
      "1 Windows\n",
      "2 Linux\n",
      "===> \"1 - Windows\" OS is detected.\n",
      "\n",
      "===========================================\n",
      "Processor (CPU) details: \n",
      "___________________________________________\n",
      "{'python_version': '3.7.10.final.0 (64 bit)', 'cpuinfo_version': [8, 0, 0], 'cpuinfo_version_string': '8.0.0', 'arch': 'X86_64', 'bits': 64, 'count': 40, 'arch_string_raw': 'AMD64', 'vendor_id_raw': 'GenuineIntel', 'brand_raw': 'Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz', 'hz_advertised_friendly': '2.2000 GHz', 'hz_actual_friendly': '2.2010 GHz', 'hz_advertised': [2200000000, 0], 'hz_actual': [2201000000, 0], 'l2_cache_size': 2621440, 'stepping': 1, 'model': 79, 'family': 6, 'l3_cache_size': 26214400, 'flags': ['3dnow', '3dnowprefetch', 'abm', 'acpi', 'adx', 'aes', 'apic', 'avx', 'avx2', 'bmi1', 'bmi2', 'clflush', 'cmov', 'cx16', 'cx8', 'dca', 'de', 'ds_cpl', 'dtes64', 'dts', 'erms', 'est', 'f16c', 'fma', 'fpu', 'fxsr', 'hle', 'ht', 'ia64', 'intel_pt', 'invpcid', 'lahf_lm', 'mca', 'mce', 'mmx', 'monitor', 'movbe', 'msr', 'mtrr', 'osxsave', 'pae', 'pat', 'pbe', 'pcid', 'pclmulqdq', 'pdcm', 'pge', 'pni', 'popcnt', 'pqe', 'pqm', 'pse', 'pse36', 'rdrnd', 'rdseed', 'rtm', 'sep', 'serial', 'smap', 'smep', 'smx', 'ss', 'sse', 'sse2', 'sse4_1', 'sse4_2', 'ssse3', 'tm', 'tm2', 'tsc', 'tscdeadline', 'vme', 'vmx', 'x2apic', 'xsave', 'xtpr'], 'l2_cache_line_size': 256, 'l2_cache_associativity': 6}\n",
      "Brand_raw = Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\n",
      "Arch_string_raw = AMD64\n",
      "Arch = X86_64\n",
      "Count = 40\n",
      "Python_version = 3.7.10.final.0 (64 bit)\n",
      "___________________________________________\n",
      "Processor (CPU) usage: \n",
      "___________________________________________\n",
      "svmem(total=137322393600, available=20919742464, percent=84.8, used=116402651136, free=20919742464)\n",
      "Cpu_usage = 100.0\n",
      "Ram_usage = 84.8\n",
      "Total_ram = 127.9\n",
      "Used_ram = 108.4\n",
      "Available_ram = 19.5\n",
      "\n",
      "===========================================\n",
      "Processor (GPU) details: \n",
      "___________________________________________\n",
      "For GPU based tasks. There are 8 GPUs in the system and 7 are available. \n",
      "Available GPU IDs with MaxLoad>=0.5 and MaxMem>=0.5 are: [1, 2, 3, 4, 5, 6, 7]\n",
      "___________________________________________\n",
      "Processor (GPU) usage: \n",
      "___________________________________________\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 79% |\n",
      "|  1 |  0% |  4% |\n",
      "|  2 |  0% |  4% |\n",
      "|  3 |  0% |  4% |\n",
      "|  4 |  0% |  4% |\n",
      "|  5 |  0% |  4% |\n",
      "|  6 |  0% |  4% |\n",
      "|  7 | 29% | 13% |\n",
      "\n",
      "===========================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'Windows',\n",
       " {'brand_raw': 'Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz',\n",
       "  'arch_string_raw': 'AMD64',\n",
       "  'arch': 'X86_64',\n",
       "  'count': 40,\n",
       "  'python_version': '3.7.10.final.0 (64 bit)',\n",
       "  'CPU_usage': 100.0,\n",
       "  'RAM_usage': 84.8,\n",
       "  'Total_RAM': 127.9,\n",
       "  'Used_RAM': 108.4,\n",
       "  'Available_RAM': 19.5},\n",
       " [1, 2, 3, 4, 5, 6, 7],\n",
       " 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import HumachLab_Global \n",
    "HumachLab_Global.get_system_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ccd1c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\OneDrive - Deakin University\\_MyResearch\\PhD_Research\\HML_IHC_Sleep_Data_Analysis\n",
      "C:\\Users\\aliem\\OneDrive - Deakin University\\_MyResearch\\PhD_Research\\HML_IHC_Sleep_Data_Analysis\\HumachLab\n",
      "\n",
      "===========================================\n",
      "List of OS platforms and codes\n",
      "___________________________________________\n",
      "0 Darwin\n",
      "1 Windows\n",
      "2 Linux\n",
      "===> \"1 - Windows\" OS is detected.\n",
      "\n",
      "===========================================\n",
      "Processor (CPU) details: \n",
      "___________________________________________\n",
      "{'python_version': '3.7.10.final.0 (64 bit)', 'cpuinfo_version': [8, 0, 0], 'cpuinfo_version_string': '8.0.0', 'arch': 'X86_64', 'bits': 64, 'count': 40, 'arch_string_raw': 'AMD64', 'vendor_id_raw': 'GenuineIntel', 'brand_raw': 'Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz', 'hz_advertised_friendly': '2.2000 GHz', 'hz_actual_friendly': '2.2010 GHz', 'hz_advertised': [2200000000, 0], 'hz_actual': [2201000000, 0], 'l2_cache_size': 2621440, 'stepping': 1, 'model': 79, 'family': 6, 'l3_cache_size': 26214400, 'flags': ['3dnow', '3dnowprefetch', 'abm', 'acpi', 'adx', 'aes', 'apic', 'avx', 'avx2', 'bmi1', 'bmi2', 'clflush', 'cmov', 'cx16', 'cx8', 'dca', 'de', 'ds_cpl', 'dtes64', 'dts', 'erms', 'est', 'f16c', 'fma', 'fpu', 'fxsr', 'hle', 'ht', 'ia64', 'intel_pt', 'invpcid', 'lahf_lm', 'mca', 'mce', 'mmx', 'monitor', 'movbe', 'msr', 'mtrr', 'osxsave', 'pae', 'pat', 'pbe', 'pcid', 'pclmulqdq', 'pdcm', 'pge', 'pni', 'popcnt', 'pqe', 'pqm', 'pse', 'pse36', 'rdrnd', 'rdseed', 'rtm', 'sep', 'serial', 'smap', 'smep', 'smx', 'ss', 'sse', 'sse2', 'sse4_1', 'sse4_2', 'ssse3', 'tm', 'tm2', 'tsc', 'tscdeadline', 'vme', 'vmx', 'x2apic', 'xsave', 'xtpr'], 'l2_cache_line_size': 256, 'l2_cache_associativity': 6}\n",
      "Brand_raw = Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\n",
      "Arch_string_raw = AMD64\n",
      "Arch = X86_64\n",
      "Count = 40\n",
      "Python_version = 3.7.10.final.0 (64 bit)\n",
      "___________________________________________\n",
      "Processor (CPU) usage: \n",
      "___________________________________________\n",
      "svmem(total=137322393600, available=20881551360, percent=84.8, used=116440842240, free=20881551360)\n",
      "Cpu_usage = 100.0\n",
      "Ram_usage = 84.8\n",
      "Total_ram = 127.9\n",
      "Used_ram = 108.5\n",
      "Available_ram = 19.4\n",
      "\n",
      "===========================================\n",
      "Processor (GPU) details: \n",
      "___________________________________________\n",
      "For GPU based tasks. There are 8 GPUs in the system and 7 are available. \n",
      "Available GPU IDs with MaxLoad>=0.5 and MaxMem>=0.5 are: [1, 2, 3, 4, 5, 6, 7]\n",
      "___________________________________________\n",
      "Processor (GPU) usage: \n",
      "___________________________________________\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 79% |\n",
      "|  1 |  0% |  4% |\n",
      "|  2 |  0% |  4% |\n",
      "|  3 |  0% |  4% |\n",
      "|  4 |  0% |  4% |\n",
      "|  5 |  0% |  4% |\n",
      "|  6 |  0% |  4% |\n",
      "|  7 | 25% | 13% |\n",
      "\n",
      "===========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Importing necessary modules\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir(os.getcwd())\n",
    "print(f\"{os.getcwd()}\\HumachLab\")\n",
    "sys.path.append(f\"{os.getcwd()}\\HumachLab\")\n",
    "sys.path.insert(0, os.path.abspath('./HumachLab'))\n",
    "\n",
    "import itertools as it\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import copy\n",
    "from pprint import pprint\n",
    "\n",
    "import glob\n",
    "\n",
    "import math\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib import rc, rcParams\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.utils import resample\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import HumachLab_Global\n",
    "from HumachLab import * \n",
    "# from HumachLab.HumachLab_Global import *\n",
    "# import HumachLab_Global\n",
    "HumachLab_Global.get_system_info()\n",
    "\n",
    "import mne\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]\n",
    "plt.rcParams[\"figure.figsize\"] = (20,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a13582",
   "metadata": {},
   "source": [
    "## Get directory list: Subject-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bb5a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Explore the contents/files in the directory\n",
    "'''\n",
    "\n",
    "def get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=None, exclude=None):\n",
    "    '''\n",
    "    directory: valid path string, path_type: p_file|p_dir, containes: string, extension: valid string file extension \n",
    "    '''\n",
    "    os_path = os.path\n",
    "    list_of_paths = []\n",
    "        \n",
    "    path_keywords = \"*\"\n",
    "    if containes:\n",
    "        path_keywords = f\"{path_keywords}{containes}*\"\n",
    "    \n",
    "    if extension:\n",
    "        path_keywords = f\"{path_keywords}.{extension}\"\n",
    "        \n",
    "    complete_path = f\"{directory}/{path_keywords}\"\n",
    "    print(f\"============> {path_keywords}, {path_type}, {complete_path}\")\n",
    "    \n",
    "    all_paths = glob.glob(complete_path) \n",
    "    all_temp_paths = None\n",
    "    list_of_paths = None\n",
    "    \n",
    "    if path_type:\n",
    "        if path_type==\"p_file\":\n",
    "            all_temp_paths = [path.replace(\"\\\\\", \"/\") for path in all_paths if (os_path.exists(path) and os_path.isfile(path))]\n",
    "        if path_type==\"p_dir\":\n",
    "            all_temp_paths = [path.replace(\"\\\\\", \"/\") for path in all_paths if (os_path.exists(path) and os_path.isdir(path))]   \n",
    "    else:\n",
    "        all_temp_paths = [path.replace(\"\\\\\", \"/\") for path in all_paths]\n",
    "        \n",
    "    if exclude:\n",
    "        # print(all_temp_paths)\n",
    "        # print(len(all_temp_paths), exclude)\n",
    "        # list_of_paths = [path for path in all_temp_paths for ex in exclude if ex not in path]\n",
    "        # list_of_paths = [path for ex in exclude for path in all_temp_paths if ex not in path]\n",
    "        list_of_paths = [path for path in all_temp_paths if not any((ex in path) for ex in exclude)]\n",
    "        # list_of_paths = [path for ex in exclude if any(ex not in path for path in all_temp_paths)]\n",
    "        # any(substring in string for substring in substring_list)\n",
    "        # print(len(list_of_paths))\n",
    "    else:\n",
    "        list_of_paths = all_temp_paths.copy()\n",
    "    \n",
    "    return list_of_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4679efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Results//_Combined/STP_From_Same_Stages', './Results//_Classification')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_directory = \"./Results/\" \n",
    "data_dir = \"/_Combined/\"\n",
    "# data_directory = \"./Results/\" \n",
    "# tran_directory = \"/Transition_Matrices\"\n",
    "# result_directory = \"./Results/_Combined\" \n",
    "result_subdirctory = \"/_Classification\"\n",
    "prob_cal_from_all = False  \n",
    "result_subdir = ['STP_From_Same_Stages', 'STP_From_All_Stages'][int(prob_cal_from_all)]\n",
    "data_directory = f\"{root_directory}{data_dir}{result_subdir}\"\n",
    "result_directory = f\"{root_directory}{result_subdirctory}\" \n",
    "wake_state_trimmed = True  \n",
    "result_subdirectory = f\"Subject_One_Night{'_TrimW' if wake_state_trimmed else ''}\"  ###\"Subject_Combined_Record\"  \"Subject_Separate_Record\"  \"Subject_One_Night\" \"Subject_One_Night_TrimW\"   ## Change for new type of result \n",
    "data_subdirectory = [\"CAP_Sleep\", \"Sleep_EDFX\", \"SDRC\"] \n",
    "metadata_subdirectory = [\"CAP_Sleep\", \"Sleep_EDFX\", \"SDRC\"] \n",
    "tran_matrix_type = [\"count\", \"dura\", \"proba\"] \n",
    "annotation_type = ['annot', 'tran'] \n",
    "tran_step = 3\n",
    "exclude_contents_in_dataset_directory = [\"SHA256SUMS\", \"RECORDS\"]\n",
    "exclude_contents_in_result_directory = [\"SHA256SUMS\", \"RECORDS\", \"all_annotions\", \"annot_sequence\", \"transition_sequence\", \"hypno\", \"DATASET_CHANGELOG\"]\n",
    "sleep_stage_labels = ['W', 'S1', 'S2', 'S3', 'S4', 'REM']\n",
    "sleep_stage_labels_dict = {'W':0, 'S1':1, 'S2':2, 'S3':3, 'S4':4, 'REM':5}\n",
    "sleep_stage_names_dict = {'W':0, 'S1':1, 'S2':2, 'S3':3, 'S4':4, 'R':5}\n",
    "list_of_paths = None \n",
    "data_directory, result_directory\n",
    "\n",
    "# directory = dataset_directory\n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=None, exclude=None) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=\"p_file\", containes=None, extension=None, exclude=None) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=\"p_dir\", containes=None, extension=None, exclude=None) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=\"nfle\", extension=None, exclude=None) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=\"edf\", exclude=None) \n",
    "# list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=None, exclude=exclude_contents_in_dataset_directory) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=None, exclude=None) \n",
    "# # pprint(list_of_paths)\n",
    "# list_of_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5dccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddaa1cd3",
   "metadata": {},
   "source": [
    "### Get basic information and transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "446b95e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category_Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Total_Count</th>\n",
       "      <th>Male_Count</th>\n",
       "      <th>Female_Count</th>\n",
       "      <th>Total_AgeRange</th>\n",
       "      <th>Male_AgeRange</th>\n",
       "      <th>Female_AgeRange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Bruxism</td>\n",
       "      <td>brux</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23 - 34</td>\n",
       "      <td>23 - 34</td>\n",
       "      <td>0 - 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Sleep-Disordered Breathing</td>\n",
       "      <td>sdb</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>65 - 78</td>\n",
       "      <td>65 - 78</td>\n",
       "      <td>0 - 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Insomnia</td>\n",
       "      <td>ins</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>47 - 82</td>\n",
       "      <td>54 - 82</td>\n",
       "      <td>47 - 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Narcolepsy</td>\n",
       "      <td>narco</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18 - 44</td>\n",
       "      <td>24 - 43</td>\n",
       "      <td>18 - 44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Nocturnal Frontal Lobe Epilepsy</td>\n",
       "      <td>nfle</td>\n",
       "      <td>40</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>14 - 67</td>\n",
       "      <td>14 - 44</td>\n",
       "      <td>16 - 67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Periodic Leg Movements</td>\n",
       "      <td>plm</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>40 - 62</td>\n",
       "      <td>40 - 62</td>\n",
       "      <td>50 - 52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>REM Behavior Disorder</td>\n",
       "      <td>rbd</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>58 - 82</td>\n",
       "      <td>58 - 82</td>\n",
       "      <td>73 - 76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>23 - 42</td>\n",
       "      <td>23 - 34</td>\n",
       "      <td>24 - 42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108</td>\n",
       "      <td>66</td>\n",
       "      <td>42</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>16 - 76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Sleep Disorders</td>\n",
       "      <td>dis</td>\n",
       "      <td>92</td>\n",
       "      <td>59</td>\n",
       "      <td>33</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>16 - 76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>23 - 42</td>\n",
       "      <td>23 - 34</td>\n",
       "      <td>24 - 42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108</td>\n",
       "      <td>66</td>\n",
       "      <td>42</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>16 - 76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>SC</td>\n",
       "      <td>n</td>\n",
       "      <td>153</td>\n",
       "      <td>71</td>\n",
       "      <td>82</td>\n",
       "      <td>25 - 101</td>\n",
       "      <td>26 - 97</td>\n",
       "      <td>25 - 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>ST</td>\n",
       "      <td>n</td>\n",
       "      <td>44</td>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>18 - 79</td>\n",
       "      <td>18 - 79</td>\n",
       "      <td>20 - 60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>Total</td>\n",
       "      <td>n</td>\n",
       "      <td>197</td>\n",
       "      <td>101</td>\n",
       "      <td>96</td>\n",
       "      <td>19 - 101</td>\n",
       "      <td>19 - 97</td>\n",
       "      <td>21 - 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>197</td>\n",
       "      <td>101</td>\n",
       "      <td>96</td>\n",
       "      <td>18 - 101</td>\n",
       "      <td>18 - 97</td>\n",
       "      <td>20 - 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>Total</td>\n",
       "      <td>n</td>\n",
       "      <td>197</td>\n",
       "      <td>101</td>\n",
       "      <td>96</td>\n",
       "      <td>19 - 101</td>\n",
       "      <td>19 - 97</td>\n",
       "      <td>21 - 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>Insomnia</td>\n",
       "      <td>ins</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>28-62</td>\n",
       "      <td>41-53</td>\n",
       "      <td>29-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>18-63</td>\n",
       "      <td>18-63</td>\n",
       "      <td>19-57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>Total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>18-63</td>\n",
       "      <td>18-63</td>\n",
       "      <td>19-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>Sleep Disorders</td>\n",
       "      <td>dis</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>28-62</td>\n",
       "      <td>41-53</td>\n",
       "      <td>29-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>18-63</td>\n",
       "      <td>18-63</td>\n",
       "      <td>19-57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     #     Dataset                    Category_Name Category  Total_Count  \\\n",
       "0    1   CAP_Sleep                          Bruxism     brux            2   \n",
       "1    2   CAP_Sleep       Sleep-Disordered Breathing      sdb            4   \n",
       "2    3   CAP_Sleep                         Insomnia      ins            9   \n",
       "3    4   CAP_Sleep                       Narcolepsy    narco            5   \n",
       "4    5   CAP_Sleep  Nocturnal Frontal Lobe Epilepsy     nfle           40   \n",
       "5    6   CAP_Sleep           Periodic Leg Movements      plm           10   \n",
       "6    7   CAP_Sleep            REM Behavior Disorder      rbd           22   \n",
       "7    8   CAP_Sleep          No Pathology (Controls)        n           16   \n",
       "8   10   CAP_Sleep                            Total      NaN          108   \n",
       "9   11   CAP_Sleep                  Sleep Disorders      dis           92   \n",
       "10  12   CAP_Sleep          No Pathology (Controls)        n           16   \n",
       "11  13   CAP_Sleep                            Total      NaN          108   \n",
       "12   1  Sleep_EDFX                               SC        n          153   \n",
       "13   2  Sleep_EDFX                               ST        n           44   \n",
       "14   3  Sleep_EDFX                            Total        n          197   \n",
       "15   4  Sleep_EDFX          No Pathology (Controls)        n          197   \n",
       "16   5  Sleep_EDFX                            Total        n          197   \n",
       "17   1        SDRC                         Insomnia      ins           11   \n",
       "18   8        SDRC          No Pathology (Controls)        n           11   \n",
       "19  10        SDRC                            Total      NaN           22   \n",
       "20  11        SDRC                  Sleep Disorders      dis           11   \n",
       "21  12        SDRC          No Pathology (Controls)        n           11   \n",
       "\n",
       "    Male_Count  Female_Count Total_AgeRange Male_AgeRange Female_AgeRange  \n",
       "0            2             0        23 - 34       23 - 34           0 - 0  \n",
       "1            4             0        65 - 78       65 - 78           0 - 0  \n",
       "2            4             5        47 - 82       54 - 82         47 - 59  \n",
       "3            2             3        18 - 44       24 - 43         18 - 44  \n",
       "4           21            19        14 - 67       14 - 44         16 - 67  \n",
       "5            7             3        40 - 62       40 - 62         50 - 52  \n",
       "6           19             3        58 - 82       58 - 82         73 - 76  \n",
       "7            7             9        23 - 42       23 - 34         24 - 42  \n",
       "8           66            42        14 - 82       14 - 82         16 - 76  \n",
       "9           59            33        14 - 82       14 - 82         16 - 76  \n",
       "10           7             9        23 - 42       23 - 34         24 - 42  \n",
       "11          66            42        14 - 82       14 - 82         16 - 76  \n",
       "12          71            82       25 - 101       26 - 97        25 - 101  \n",
       "13          30            14        18 - 79       18 - 79         20 - 60  \n",
       "14         101            96       19 - 101       19 - 97        21 - 101  \n",
       "15         101            96       18 - 101       18 - 97        20 - 101  \n",
       "16         101            96       19 - 101       19 - 97        21 - 101  \n",
       "17           2             9          28-62         41-53           29-62  \n",
       "18           6             5          18-63         18-63           19-57  \n",
       "19           8            14          18-63         18-63           19-62  \n",
       "20           2             9          28-62         41-53           29-62  \n",
       "21           6             5          18-63         18-63           19-57  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_demography_info(tmp_dir): \n",
    "#     list_of_tran_mat_paths = [] \n",
    "    all_demography_df = pd.read_csv(f\"{tmp_dir}/all_demography.csv\", index_col=False) \n",
    "    all_demography_detail_df = pd.read_csv(f\"{tmp_dir}/all_demography_detail.csv\", index_col=False)  \n",
    "    return all_demography_df, all_demography_detail_df \n",
    "\n",
    "\n",
    "info_type=\"sub\" ##\"sub\"/\"file\"  ## Change for new type of result \n",
    "annot_type = annotation_type[0]   ## Change for different data preparation for 'annot' and 'tran' \n",
    "\n",
    "all_demography_df, all_demography_detail_df = get_demography_info(data_directory) \n",
    "all_demography_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ad14a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Explore the contents/files in the directory\n",
    "# '''\n",
    "\n",
    "# def get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=None, exclude=None):\n",
    "#     '''\n",
    "#     directory: valid path string, path_type: p_file|p_dir, containes: string, extension: valid string file extension \n",
    "#     '''\n",
    "#     os_path = os.path\n",
    "#     list_of_paths = []\n",
    "        \n",
    "#     path_keywords = \"*\"\n",
    "#     if containes:\n",
    "#         path_keywords = f\"{path_keywords}{containes}*\"\n",
    "    \n",
    "#     if extension:\n",
    "#         path_keywords = f\"{path_keywords}.{extension}\"\n",
    "        \n",
    "#     complete_path = f\"{directory}/{path_keywords}\"\n",
    "#     print(f\"============> {path_keywords}, {path_type}, {complete_path}\")\n",
    "    \n",
    "#     all_paths = glob.glob(complete_path) \n",
    "#     all_temp_paths = None\n",
    "#     list_of_paths = None\n",
    "    \n",
    "#     if path_type:\n",
    "#         if path_type==\"p_file\":\n",
    "#             all_temp_paths = [path.replace(\"\\\\\", \"/\") for path in all_paths if (os_path.exists(path) and os_path.isfile(path))]\n",
    "#         if path_type==\"p_dir\":\n",
    "#             all_temp_paths = [path.replace(\"\\\\\", \"/\") for path in all_paths if (os_path.exists(path) and os_path.isdir(path))]   \n",
    "#     else:\n",
    "#         all_temp_paths = [path.replace(\"\\\\\", \"/\") for path in all_paths]\n",
    "        \n",
    "#     if exclude:\n",
    "#         # print(all_temp_paths)\n",
    "#         # print(len(all_temp_paths), exclude)\n",
    "#         # list_of_paths = [path for path in all_temp_paths for ex in exclude if ex not in path]\n",
    "#         # list_of_paths = [path for ex in exclude for path in all_temp_paths if ex not in path]\n",
    "#         list_of_paths = [path for path in all_temp_paths if not any((ex in path) for ex in exclude)]\n",
    "#         # list_of_paths = [path for ex in exclude if any(ex not in path for path in all_temp_paths)]\n",
    "#         # any(substring in string for substring in substring_list)\n",
    "#         # print(len(list_of_paths))\n",
    "#     else:\n",
    "#         list_of_paths = all_temp_paths.copy()\n",
    "    \n",
    "#     return list_of_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88356992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_directory = \"./Results/\" \n",
    "# data_directory = \"/_Combined\"\n",
    "# prob_cal_from_all = False \n",
    "# data_subdir = ['STP_From_Same_Stages', 'STP_From_All_Stages'][int(prob_cal_from_all)] \n",
    "# wake_state_trimmed = False \n",
    "# data_subdirectory = f\"Subject_One_Night{'_TrimW' if wake_state_trimmed else ''}\"  ###\"Subject_Combined_Record\"  \"Subject_Separate_Record\"  \"Subject_One_Night\" \"Subject_One_Night_TrimW\"   ## Change for new type of result \n",
    "# # result_directory = f\"./Results/_Classification/{result_subdir}\" \n",
    "# result_directory = \"./Results/_Classification\" \n",
    "# metadata_subdirectory = [\"CAP_Sleep\", \"Sleep_EDFX\", \"SDRC\"] \n",
    "# dataset_list = [\"CAP_Sleep\", \"Sleep_EDFX\", \"SDRC\"] \n",
    "# tran_matrix_type = [\"count\", \"dura\", \"proba\"] \n",
    "# annotation_type = ['annot', 'tran']\n",
    "# tran_step = 2\n",
    "# exclude_contents_in_dataset_directory = [\"SHA256SUMS\", \"RECORDS\"]\n",
    "# exclude_contents_in_result_directory = [\"SHA256SUMS\", \"RECORDS\", \"all_annotations\", \"annot_sequence\", \"transition_sequence\", \"hypno\", \"DATASET_CHANGELOG\"]\n",
    "# sleep_stage_labels = ['W', 'S1', 'S2', 'S3', 'S4', 'REM']\n",
    "# sleep_stage_labels_dict = {'W':0, 'S1':1, 'S2':2, 'S3':3, 'S4':4, 'REM':5}\n",
    "# sleep_stage_names_dict = {'W':0, 'S1':1, 'S2':2, 'S3':3, 'S4':4, 'R':5}\n",
    "# list_of_paths = None \n",
    "\n",
    "\n",
    "# info_type=\"sub\" ##\"sub\"/\"file\"  ## Change for new type of result\n",
    "# annot_type = annotation_type[0]   ## Change for different data preparation for 'annot' and 'tran' \n",
    "\n",
    "\n",
    "# directory = dataset_directory\n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=None, exclude=None) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=\"p_file\", containes=None, extension=None, exclude=None) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=\"p_dir\", containes=None, extension=None, exclude=None) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=\"nfle\", extension=None, exclude=None) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=\"edf\", exclude=None) \n",
    "# list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=None, exclude=exclude_contents_in_dataset_directory) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=None, exclude=None) \n",
    "# # pprint(list_of_paths)\n",
    "# list_of_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea1e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dbd1a2c",
   "metadata": {},
   "source": [
    "### Get basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c29fbb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category_Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Total_Count</th>\n",
       "      <th>Male_Count</th>\n",
       "      <th>Female_Count</th>\n",
       "      <th>Total_AgeRange</th>\n",
       "      <th>Male_AgeRange</th>\n",
       "      <th>Female_AgeRange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Bruxism</td>\n",
       "      <td>brux</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23 - 34</td>\n",
       "      <td>23 - 34</td>\n",
       "      <td>0 - 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Sleep-Disordered Breathing</td>\n",
       "      <td>sdb</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>65 - 78</td>\n",
       "      <td>65 - 78</td>\n",
       "      <td>0 - 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Insomnia</td>\n",
       "      <td>ins</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>47 - 82</td>\n",
       "      <td>54 - 82</td>\n",
       "      <td>47 - 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Narcolepsy</td>\n",
       "      <td>narco</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18 - 44</td>\n",
       "      <td>24 - 43</td>\n",
       "      <td>18 - 44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Nocturnal Frontal Lobe Epilepsy</td>\n",
       "      <td>nfle</td>\n",
       "      <td>40</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>14 - 67</td>\n",
       "      <td>14 - 44</td>\n",
       "      <td>16 - 67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Periodic Leg Movements</td>\n",
       "      <td>plm</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>40 - 62</td>\n",
       "      <td>40 - 62</td>\n",
       "      <td>50 - 52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>REM Behavior Disorder</td>\n",
       "      <td>rbd</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>58 - 82</td>\n",
       "      <td>58 - 82</td>\n",
       "      <td>73 - 76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>23 - 42</td>\n",
       "      <td>23 - 34</td>\n",
       "      <td>24 - 42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108</td>\n",
       "      <td>66</td>\n",
       "      <td>42</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>16 - 76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Sleep Disorders</td>\n",
       "      <td>dis</td>\n",
       "      <td>92</td>\n",
       "      <td>59</td>\n",
       "      <td>33</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>16 - 76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>23 - 42</td>\n",
       "      <td>23 - 34</td>\n",
       "      <td>24 - 42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108</td>\n",
       "      <td>66</td>\n",
       "      <td>42</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>16 - 76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>SC</td>\n",
       "      <td>n</td>\n",
       "      <td>153</td>\n",
       "      <td>71</td>\n",
       "      <td>82</td>\n",
       "      <td>25 - 101</td>\n",
       "      <td>26 - 97</td>\n",
       "      <td>25 - 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>ST</td>\n",
       "      <td>n</td>\n",
       "      <td>44</td>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>18 - 79</td>\n",
       "      <td>18 - 79</td>\n",
       "      <td>20 - 60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>Total</td>\n",
       "      <td>n</td>\n",
       "      <td>197</td>\n",
       "      <td>101</td>\n",
       "      <td>96</td>\n",
       "      <td>19 - 101</td>\n",
       "      <td>19 - 97</td>\n",
       "      <td>21 - 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>197</td>\n",
       "      <td>101</td>\n",
       "      <td>96</td>\n",
       "      <td>18 - 101</td>\n",
       "      <td>18 - 97</td>\n",
       "      <td>20 - 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>Total</td>\n",
       "      <td>n</td>\n",
       "      <td>197</td>\n",
       "      <td>101</td>\n",
       "      <td>96</td>\n",
       "      <td>19 - 101</td>\n",
       "      <td>19 - 97</td>\n",
       "      <td>21 - 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>Insomnia</td>\n",
       "      <td>ins</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>28-62</td>\n",
       "      <td>41-53</td>\n",
       "      <td>29-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>18-63</td>\n",
       "      <td>18-63</td>\n",
       "      <td>19-57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>Total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>18-63</td>\n",
       "      <td>18-63</td>\n",
       "      <td>19-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>Sleep Disorders</td>\n",
       "      <td>dis</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>28-62</td>\n",
       "      <td>41-53</td>\n",
       "      <td>29-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>18-63</td>\n",
       "      <td>18-63</td>\n",
       "      <td>19-57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     #     Dataset                    Category_Name Category  Total_Count  \\\n",
       "0    1   CAP_Sleep                          Bruxism     brux            2   \n",
       "1    2   CAP_Sleep       Sleep-Disordered Breathing      sdb            4   \n",
       "2    3   CAP_Sleep                         Insomnia      ins            9   \n",
       "3    4   CAP_Sleep                       Narcolepsy    narco            5   \n",
       "4    5   CAP_Sleep  Nocturnal Frontal Lobe Epilepsy     nfle           40   \n",
       "5    6   CAP_Sleep           Periodic Leg Movements      plm           10   \n",
       "6    7   CAP_Sleep            REM Behavior Disorder      rbd           22   \n",
       "7    8   CAP_Sleep          No Pathology (Controls)        n           16   \n",
       "8   10   CAP_Sleep                            Total      NaN          108   \n",
       "9   11   CAP_Sleep                  Sleep Disorders      dis           92   \n",
       "10  12   CAP_Sleep          No Pathology (Controls)        n           16   \n",
       "11  13   CAP_Sleep                            Total      NaN          108   \n",
       "12   1  Sleep_EDFX                               SC        n          153   \n",
       "13   2  Sleep_EDFX                               ST        n           44   \n",
       "14   3  Sleep_EDFX                            Total        n          197   \n",
       "15   4  Sleep_EDFX          No Pathology (Controls)        n          197   \n",
       "16   5  Sleep_EDFX                            Total        n          197   \n",
       "17   1        SDRC                         Insomnia      ins           11   \n",
       "18   8        SDRC          No Pathology (Controls)        n           11   \n",
       "19  10        SDRC                            Total      NaN           22   \n",
       "20  11        SDRC                  Sleep Disorders      dis           11   \n",
       "21  12        SDRC          No Pathology (Controls)        n           11   \n",
       "\n",
       "    Male_Count  Female_Count Total_AgeRange Male_AgeRange Female_AgeRange  \n",
       "0            2             0        23 - 34       23 - 34           0 - 0  \n",
       "1            4             0        65 - 78       65 - 78           0 - 0  \n",
       "2            4             5        47 - 82       54 - 82         47 - 59  \n",
       "3            2             3        18 - 44       24 - 43         18 - 44  \n",
       "4           21            19        14 - 67       14 - 44         16 - 67  \n",
       "5            7             3        40 - 62       40 - 62         50 - 52  \n",
       "6           19             3        58 - 82       58 - 82         73 - 76  \n",
       "7            7             9        23 - 42       23 - 34         24 - 42  \n",
       "8           66            42        14 - 82       14 - 82         16 - 76  \n",
       "9           59            33        14 - 82       14 - 82         16 - 76  \n",
       "10           7             9        23 - 42       23 - 34         24 - 42  \n",
       "11          66            42        14 - 82       14 - 82         16 - 76  \n",
       "12          71            82       25 - 101       26 - 97        25 - 101  \n",
       "13          30            14        18 - 79       18 - 79         20 - 60  \n",
       "14         101            96       19 - 101       19 - 97        21 - 101  \n",
       "15         101            96       18 - 101       18 - 97        20 - 101  \n",
       "16         101            96       19 - 101       19 - 97        21 - 101  \n",
       "17           2             9          28-62         41-53           29-62  \n",
       "18           6             5          18-63         18-63           19-57  \n",
       "19           8            14          18-63         18-63           19-62  \n",
       "20           2             9          28-62         41-53           29-62  \n",
       "21           6             5          18-63         18-63           19-57  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_demography_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "298a8a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Subject_Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux1</td>\n",
       "      <td>brux1</td>\n",
       "      <td>brux</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux2</td>\n",
       "      <td>brux2</td>\n",
       "      <td>brux</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb1</td>\n",
       "      <td>sdb1</td>\n",
       "      <td>sdb</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb2</td>\n",
       "      <td>sdb2</td>\n",
       "      <td>sdb</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb3</td>\n",
       "      <td>sdb3</td>\n",
       "      <td>sdb</td>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>SDRC</td>\n",
       "      <td>ins1107</td>\n",
       "      <td>ins1107</td>\n",
       "      <td>ins</td>\n",
       "      <td>7</td>\n",
       "      <td>F</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>SDRC</td>\n",
       "      <td>ins1108</td>\n",
       "      <td>ins1108</td>\n",
       "      <td>ins</td>\n",
       "      <td>8</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>SDRC</td>\n",
       "      <td>ins1109</td>\n",
       "      <td>ins1109</td>\n",
       "      <td>ins</td>\n",
       "      <td>9</td>\n",
       "      <td>F</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>SDRC</td>\n",
       "      <td>ins1110</td>\n",
       "      <td>ins1110</td>\n",
       "      <td>ins</td>\n",
       "      <td>10</td>\n",
       "      <td>M</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>SDRC</td>\n",
       "      <td>ins1111</td>\n",
       "      <td>ins1111</td>\n",
       "      <td>ins</td>\n",
       "      <td>11</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>327 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Dataset File_Name Subject_Name Category  Subject_ID Gender  Age\n",
       "0    CAP_Sleep     brux1        brux1     brux           1      M   34\n",
       "1    CAP_Sleep     brux2        brux2     brux           2      M   23\n",
       "2    CAP_Sleep      sdb1         sdb1      sdb           1      M   65\n",
       "3    CAP_Sleep      sdb2         sdb2      sdb           2      M   77\n",
       "4    CAP_Sleep      sdb3         sdb3      sdb           3      M   78\n",
       "..         ...       ...          ...      ...         ...    ...  ...\n",
       "322       SDRC   ins1107      ins1107      ins           7      F   62\n",
       "323       SDRC   ins1108      ins1108      ins           8      F   57\n",
       "324       SDRC   ins1109      ins1109      ins           9      F   32\n",
       "325       SDRC   ins1110      ins1110      ins          10      M   41\n",
       "326       SDRC   ins1111      ins1111      ins          11      F   50\n",
       "\n",
       "[327 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_demography_detail_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9de0e18b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0775eba3",
   "metadata": {},
   "source": [
    "### Get transition matrix or features information from transition probabilities and P/AUC information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9642b763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Annot_Proba_Transition3.csv\n",
      "./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Annot_Proba_Transition3_STAT_bin.csv\n"
     ]
    }
   ],
   "source": [
    "tran_mat_type = ['Proba', 'Dura', 'Count']\n",
    "mat_info_type = 0 \n",
    "print( f\"{data_directory}/{result_subdirectory}/{annot_type.capitalize()}_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv\" ) \n",
    "print( f\"{data_directory}/{result_subdirectory}/{annot_type.capitalize()}_{tran_mat_type[mat_info_type]}_Transition{tran_step}_STAT_bin.csv\" ) \n",
    "# print( f\"{data_directory}/{result_subdirectory}/Annot_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv\" ) Annot_Proba_Transition3_STAT_bin\n",
    "# data_directory, result_subdirectory, tran_mat_type, mat_info_type, tran_step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79b121c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_STP_matrix_files_for_PhDStudy01(mat_info_type, tran_step): \n",
    "    tran_mat_type = ['Proba', 'Dura', 'Count']\n",
    "    ## ['EDFXSCRem', 'CAPOnly', 'CAPandSDRC']\n",
    "    \n",
    "    annot_proba_file_list = [] \n",
    "    annot_proba_file_list.append(f'./Results//_Combined/STP_From_Same_Stages/Subject_One_Night/Annot_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv') \n",
    "    annot_proba_file_list.append(f'./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Annot_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv') \n",
    "    annot_proba_file_list.append(f'./Results//_Combined/STP_From_All_Stages/Subject_One_Night/Annot_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv') \n",
    "    annot_proba_file_list.append(f'./Results//_Combined/STP_From_All_Stages/Subject_One_Night_TrimW/Annot_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv') \n",
    "    tran_proba_file_list = [] \n",
    "    tran_proba_file_list.append(f'./Results//_Combined/STP_From_Same_Stages/Subject_One_Night/Tran_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv') \n",
    "    tran_proba_file_list.append(f'./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Tran_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv') \n",
    "    tran_proba_file_list.append(f'./Results//_Combined/STP_From_All_Stages/Subject_One_Night/Tran_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv') \n",
    "    tran_proba_file_list.append(f'./Results//_Combined/STP_From_All_Stages/Subject_One_Night_TrimW/Tran_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv') \n",
    "    return annot_proba_file_list, tran_proba_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be49c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_for_file_conditions2(mat_type, STP_From_All, TrimW, mat_info_type, tran_step):\n",
    "    print(f'Reading criteria: {mat_type}, {STP_From_All}, {TrimW}, {mat_info_type}, {tran_step}')\n",
    "    annot_proba_file_list, tran_proba_file_list = get_all_STP_matrix_files_for_PhDStudy01(mat_info_type, tran_step) \n",
    "    expected_list = annot_proba_file_list if mat_type==0 else tran_proba_file_list \n",
    "    ind = 0 \n",
    "    if STP_From_All==1 and TrimW==1:\n",
    "        ind = 3\n",
    "    elif STP_From_All==1 and TrimW==0:\n",
    "        ind = 2 \n",
    "    elif STP_From_All==0 and TrimW==1:\n",
    "        ind = 1\n",
    "    else:\n",
    "        ind = 0 \n",
    "        \n",
    "    filename = expected_list[ind] \n",
    "    print(f'Reading data from the file: {ind}, {filename}') \n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d34d925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_for_file_conditions(mat_type, STP_From_All, TrimW, mat_info_type, tran_step):\n",
    "    print(f'Reading criteria: {mat_type}, {STP_From_All}, {TrimW}, {mat_info_type}, {tran_step}')\n",
    "    \n",
    "    filename = f\"{data_directory}/{result_subdirectory}/{annot_type.capitalize()}_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv\"  \n",
    "    stat_filename = f\"{data_directory}/{result_subdirectory}/{annot_type.capitalize()}_{tran_mat_type[mat_info_type]}_Transition{tran_step}_STAT_bin.csv\" \n",
    "    \n",
    "    print(f'Reading data from the file: {filename}\\nStatistics from the file: {stat_filename}') \n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    stat_df = pd.read_csv(stat_filename, index_col=False)\n",
    "    return df, stat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "595f5db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading criteria: 0, 0, 1, 0, 3\n",
      "Reading data from the file: ./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Annot_Proba_Transition3.csv\n",
      "Statistics from the file: ./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Annot_Proba_Transition3_STAT_bin.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subject_Name</th>\n",
       "      <th>W-&gt;W-&gt;W</th>\n",
       "      <th>W-&gt;W-&gt;S1</th>\n",
       "      <th>W-&gt;W-&gt;S2</th>\n",
       "      <th>W-&gt;W-&gt;S3</th>\n",
       "      <th>W-&gt;W-&gt;S4</th>\n",
       "      <th>W-&gt;W-&gt;REM</th>\n",
       "      <th>W-&gt;S1-&gt;W</th>\n",
       "      <th>...</th>\n",
       "      <th>REM-&gt;S4-&gt;S2</th>\n",
       "      <th>REM-&gt;S4-&gt;S3</th>\n",
       "      <th>REM-&gt;S4-&gt;S4</th>\n",
       "      <th>REM-&gt;S4-&gt;REM</th>\n",
       "      <th>REM-&gt;REM-&gt;W</th>\n",
       "      <th>REM-&gt;REM-&gt;S1</th>\n",
       "      <th>REM-&gt;REM-&gt;S2</th>\n",
       "      <th>REM-&gt;REM-&gt;S3</th>\n",
       "      <th>REM-&gt;REM-&gt;S4</th>\n",
       "      <th>REM-&gt;REM-&gt;REM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux1</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.192771</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.965318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux2</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.126214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb1</td>\n",
       "      <td>0.879581</td>\n",
       "      <td>0.120419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.961165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb2</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.973684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb3</td>\n",
       "      <td>0.934673</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>0.015075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7191</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020725</td>\n",
       "      <td>0.015544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.963731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7201</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7211</td>\n",
       "      <td>0.940789</td>\n",
       "      <td>0.059211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.953020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7221</td>\n",
       "      <td>0.926230</td>\n",
       "      <td>0.073770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.935185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7241</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>0.008811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dataset Category Subject_Name   W->W->W  W->W->S1  W->W->S2  W->W->S3  \\\n",
       "0     CAP_Sleep     brux        brux1  0.795181  0.192771  0.012048       0.0   \n",
       "1     CAP_Sleep     brux        brux2  0.873786  0.126214  0.000000       0.0   \n",
       "2     CAP_Sleep      sdb         sdb1  0.879581  0.120419  0.000000       0.0   \n",
       "3     CAP_Sleep      sdb         sdb2  0.959184  0.040816  0.000000       0.0   \n",
       "4     CAP_Sleep      sdb         sdb3  0.934673  0.050251  0.015075       0.0   \n",
       "..          ...      ...          ...       ...       ...       ...       ...   \n",
       "225  Sleep_EDFX        n       ST7191  0.862745  0.137255  0.000000       0.0   \n",
       "226  Sleep_EDFX        n       ST7201  0.400000  0.600000  0.000000       0.0   \n",
       "227  Sleep_EDFX        n       ST7211  0.940789  0.059211  0.000000       0.0   \n",
       "228  Sleep_EDFX        n       ST7221  0.926230  0.073770  0.000000       0.0   \n",
       "229  Sleep_EDFX        n       ST7241  0.705882  0.294118  0.000000       0.0   \n",
       "\n",
       "     W->W->S4  W->W->REM  W->S1->W  ...  REM->S4->S2  REM->S4->S3  \\\n",
       "0         0.0        0.0  0.100000  ...          0.0          0.0   \n",
       "1         0.0        0.0  0.045455  ...          0.0          0.0   \n",
       "2         0.0        0.0  0.148148  ...          0.0          0.0   \n",
       "3         0.0        0.0  0.142857  ...          0.0          0.0   \n",
       "4         0.0        0.0  0.100000  ...          0.0          0.0   \n",
       "..        ...        ...       ...  ...          ...          ...   \n",
       "225       0.0        0.0  0.058824  ...          0.0          0.0   \n",
       "226       0.0        0.0  0.000000  ...          0.0          0.0   \n",
       "227       0.0        0.0  0.176471  ...          0.0          0.0   \n",
       "228       0.0        0.0  0.037037  ...          0.0          0.0   \n",
       "229       0.0        0.0  0.000000  ...          0.0          0.0   \n",
       "\n",
       "     REM->S4->S4  REM->S4->REM  REM->REM->W  REM->REM->S1  REM->REM->S2  \\\n",
       "0            0.0           0.0     0.034682      0.000000      0.000000   \n",
       "1            0.0           0.0     0.019802      0.004950      0.000000   \n",
       "2            0.0           0.0     0.009709      0.000000      0.029126   \n",
       "3            0.0           0.0     0.000000      0.000000      0.026316   \n",
       "4            0.0           0.0     0.066667      0.000000      0.000000   \n",
       "..           ...           ...          ...           ...           ...   \n",
       "225          0.0           0.0     0.000000      0.020725      0.015544   \n",
       "226          0.0           0.0     0.048000      0.008000      0.016000   \n",
       "227          0.0           0.0     0.026846      0.000000      0.020134   \n",
       "228          0.0           0.0     0.023148      0.041667      0.000000   \n",
       "229          0.0           0.0     0.017621      0.017621      0.008811   \n",
       "\n",
       "     REM->REM->S3  REM->REM->S4  REM->REM->REM  \n",
       "0             0.0           0.0       0.965318  \n",
       "1             0.0           0.0       0.975248  \n",
       "2             0.0           0.0       0.961165  \n",
       "3             0.0           0.0       0.973684  \n",
       "4             0.0           0.0       0.933333  \n",
       "..            ...           ...            ...  \n",
       "225           0.0           0.0       0.963731  \n",
       "226           0.0           0.0       0.928000  \n",
       "227           0.0           0.0       0.953020  \n",
       "228           0.0           0.0       0.935185  \n",
       "229           0.0           0.0       0.955947  \n",
       "\n",
       "[230 rows x 219 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name='Class' \n",
    "mat_type = 0 ## annot_type  # mat_type=annot/tran -- 0/1 \n",
    "STP_From_All = 0 ## int(tran_info_cal_from_all)  # STP_From_All=same/all -- 0/1 \n",
    "TrimW = 1 ## int(wake_state_trimmed)  # TrimW=normal/trimmed -- 0/1 \n",
    "mat_info_type = 0 ## int(tran_info_type) # Proba/Dura/Count -- 0/1/2\n",
    "tran_step = 3 ## int (tran step) # one level=2 step, two level=3 step -- 2/3\n",
    "\n",
    "dataset, stat_dataset = get_dataset_for_file_conditions(mat_type, STP_From_All, TrimW, mat_info_type, tran_step=tran_step) # mat_type=annot/tran, STP_From_All=same/all, TrimW=normal/trimmed, mat_info_type=proba/dura/count, tran_step=2/3 \n",
    "dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7aba17c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b083d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e499c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6529a7c0",
   "metadata": {},
   "source": [
    "#### Prepare final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803a7ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "id": "59029279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_age_category_to_class(dat_set, demo_det, source_cols=['Subject_Name', 'File_Name'], age_col = 'Age', age_ranges = [[19, 30], [31, 40]], class_name='Class', multi_class=True, healthy_only=True):\n",
    "#     tmp_df = dataset.merge(all_demography_detail_df, how='inner', left_on=[source_cols[0]], right_on=[source_cols[1]])\n",
    "#     tmp_df\n",
    "\n",
    "#     dat_set.insert(3, age_col, tmp_df[age_col].values) \n",
    "#     dat_set\n",
    "\n",
    "#     dat_set = dat_set[ ((dat_set[age_col]>=age_ranges[0][0]) & (dat_set[age_col]<=age_ranges[0][1])) | ((dat_set[age_col]>=age_ranges[1][0]) & (dat_set[age_col]<=age_ranges[1][1])) ]  \n",
    "#     dat_set.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     tmp_age_range = [list(range(age_ranges[0][0], age_ranges[0][1]+1)), list(range(age_ranges[1][0], age_ranges[1][1]+1))] \n",
    "#     tmp_age_range \n",
    "\n",
    "#     cls_map = {} \n",
    "#     for i, lst in enumerate(tmp_age_range): \n",
    "#         for l in lst:\n",
    "#             cls_map[l]=i\n",
    "#     cls_map\n",
    "    \n",
    "#     all_cols = dat_set.columns.values.tolist() \n",
    "#     if (age_col in all_cols):\n",
    "#         if (class_name in all_cols): \n",
    "#             dat_set = dat_set.drop([class_name], axis=1)\n",
    "#         dat_set = dat_set.rename(columns={age_col: class_name})\n",
    "    \n",
    "#     all_cols2 = dat_set.columns.values.tolist() \n",
    "#     if (class_name not in all_cols) and (class_name in all_cols2):\n",
    "#         dat_set.replace({class_name: cls_map}, inplace=True) \n",
    "        \n",
    "#     if healthy_only==True: \n",
    "#         pass\n",
    "#     return cls_map, dat_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "32623559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subject_Name</th>\n",
       "      <th>W-&gt;W-&gt;W</th>\n",
       "      <th>W-&gt;W-&gt;S1</th>\n",
       "      <th>W-&gt;W-&gt;S2</th>\n",
       "      <th>W-&gt;W-&gt;S3</th>\n",
       "      <th>W-&gt;W-&gt;S4</th>\n",
       "      <th>W-&gt;W-&gt;REM</th>\n",
       "      <th>W-&gt;S1-&gt;W</th>\n",
       "      <th>...</th>\n",
       "      <th>REM-&gt;S4-&gt;S2</th>\n",
       "      <th>REM-&gt;S4-&gt;S3</th>\n",
       "      <th>REM-&gt;S4-&gt;S4</th>\n",
       "      <th>REM-&gt;S4-&gt;REM</th>\n",
       "      <th>REM-&gt;REM-&gt;W</th>\n",
       "      <th>REM-&gt;REM-&gt;S1</th>\n",
       "      <th>REM-&gt;REM-&gt;S2</th>\n",
       "      <th>REM-&gt;REM-&gt;S3</th>\n",
       "      <th>REM-&gt;REM-&gt;S4</th>\n",
       "      <th>REM-&gt;REM-&gt;REM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>327 rows × 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dataset Category Subject_Name  W->W->W  W->W->S1  W->W->S2  W->W->S3  \\\n",
       "0     CAP_Sleep     brux        brux1      0.0       0.0       0.0       0.0   \n",
       "1     CAP_Sleep     brux        brux2      0.0       0.0       0.0       0.0   \n",
       "2     CAP_Sleep      sdb         sdb1      0.0       0.0       0.0       0.0   \n",
       "3     CAP_Sleep      sdb         sdb2      0.0       0.0       0.0       0.0   \n",
       "4     CAP_Sleep      sdb         sdb3      0.0       0.0       0.0       0.0   \n",
       "..          ...      ...          ...      ...       ...       ...       ...   \n",
       "322  Sleep_EDFX        n       ST7212      0.0       0.0       0.0       0.0   \n",
       "323  Sleep_EDFX        n       ST7221      0.0       0.0       0.0       0.0   \n",
       "324  Sleep_EDFX        n       ST7222      0.0       0.0       0.0       0.0   \n",
       "325  Sleep_EDFX        n       ST7241      0.0       0.0       0.0       0.0   \n",
       "326  Sleep_EDFX        n       ST7242      0.0       0.0       0.0       0.0   \n",
       "\n",
       "     W->W->S4  W->W->REM  W->S1->W  ...  REM->S4->S2  REM->S4->S3  \\\n",
       "0         0.0        0.0  0.200000  ...          0.0          0.0   \n",
       "1         0.0        0.0  0.380952  ...          0.0          0.0   \n",
       "2         0.0        0.0  0.500000  ...          0.0          0.0   \n",
       "3         0.0        0.0  0.333333  ...          0.0          0.0   \n",
       "4         0.0        0.0  0.444444  ...          0.0          0.0   \n",
       "..        ...        ...       ...  ...          ...          ...   \n",
       "322       0.0        0.0  0.400000  ...          0.0          0.0   \n",
       "323       0.0        0.0  0.269231  ...          0.0          0.0   \n",
       "324       0.0        0.0  0.272727  ...          0.0          0.0   \n",
       "325       0.0        0.0  0.000000  ...          0.0          0.0   \n",
       "326       0.0        0.0  0.111111  ...          0.0          0.0   \n",
       "\n",
       "     REM->S4->S4  REM->S4->REM  REM->REM->W  REM->REM->S1  REM->REM->S2  \\\n",
       "0            0.0           0.0          0.0           0.0           0.0   \n",
       "1            0.0           0.0          0.0           0.0           0.0   \n",
       "2            0.0           0.0          0.0           0.0           0.0   \n",
       "3            0.0           0.0          0.0           0.0           0.0   \n",
       "4            0.0           0.0          0.0           0.0           0.0   \n",
       "..           ...           ...          ...           ...           ...   \n",
       "322          0.0           0.0          0.0           0.0           0.0   \n",
       "323          0.0           0.0          0.0           0.0           0.0   \n",
       "324          0.0           0.0          0.0           0.0           0.0   \n",
       "325          0.0           0.0          0.0           0.0           0.0   \n",
       "326          0.0           0.0          0.0           0.0           0.0   \n",
       "\n",
       "     REM->REM->S3  REM->REM->S4  REM->REM->REM  \n",
       "0             0.0           0.0            0.0  \n",
       "1             0.0           0.0            0.0  \n",
       "2             0.0           0.0            0.0  \n",
       "3             0.0           0.0            0.0  \n",
       "4             0.0           0.0            0.0  \n",
       "..            ...           ...            ...  \n",
       "322           0.0           0.0            0.0  \n",
       "323           0.0           0.0            0.0  \n",
       "324           0.0           0.0            0.0  \n",
       "325           0.0           0.0            0.0  \n",
       "326           0.0           0.0            0.0  \n",
       "\n",
       "[327 rows x 219 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_demography_detail_df\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35d7f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_age_category_to_class(dat_set, demo_det, source_cols=['Subject_Name', 'File_Name'], age_col = 'Age', age_ranges = [[19, 30], [31, 40]], class_name='Class', multi_class=True, healthy_only=True):\n",
    "    if healthy_only==True: \n",
    "        dat_set = dat_set[(dat_set['Category']=='n')] \n",
    "    \n",
    "    tmp_df = dat_set.merge(demo_det, how='inner', left_on=[source_cols[0]], right_on=[source_cols[1]])\n",
    "    tmp_df\n",
    "#     print(tmp_df.shape[0], dat_set.shape[0])\n",
    "    assert tmp_df.shape[0]==dat_set.shape[0], \"Something hapend. No shape after dataframe joining is same...\"\n",
    "\n",
    "    dat_set.insert(3, age_col, tmp_df[age_col].values) \n",
    "    dat_set\n",
    "#     print(age_ranges, dat_set)\n",
    "\n",
    "#     dat_set = dat_set[ ((dat_set[age_col]>=age_ranges[0][0]) & (dat_set[age_col]<=age_ranges[0][1])) | ((dat_set[age_col]>=age_ranges[1][0]) & (dat_set[age_col]<=age_ranges[1][1])) ]  \n",
    "#     dat_set.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    tmp_age_range = []\n",
    "    tmp_dat_set = pd.DataFrame() \n",
    "    for i, ar in enumerate(age_ranges):\n",
    "        sel_df = dat_set[((dat_set[age_col]>=ar[0]) & (dat_set[age_col]<=ar[1]))] \n",
    "        tmp_dat_set = pd.concat([tmp_dat_set, sel_df ] )\n",
    "        tmp_age_range.append( sorted(sel_df[age_col].values.tolist()) ) \n",
    "        tmp_age_range \n",
    "#         print(tmp_age_range, dat_set)\n",
    "    dat_set = tmp_dat_set.copy()\n",
    "    dat_set.reset_index(drop=True, inplace=True)\n",
    "#     print(dat_set)\n",
    "\n",
    "    cls_map = {} \n",
    "    for i, lst in enumerate(tmp_age_range): \n",
    "        for l in lst:\n",
    "            cls_map[l]=i\n",
    "    cls_map\n",
    "    \n",
    "    all_cols = dat_set.columns.values.tolist() \n",
    "    if (age_col in all_cols):\n",
    "        if (class_name in all_cols): \n",
    "            dat_set = dat_set.drop([class_name], axis=1)\n",
    "        dat_set = dat_set.rename(columns={age_col: class_name})\n",
    "    \n",
    "    all_cols2 = dat_set.columns.values.tolist() \n",
    "    if (class_name not in all_cols) and (class_name in all_cols2):\n",
    "        dat_set.replace({class_name: cls_map}, inplace=True) \n",
    "    return cls_map, dat_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "id": "20b32d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # label_map, dataset2 = map_age_category_to_class(dataset.copy(), all_demography_detail_df.copy(), source_cols=['Subject_Name', 'File_Name'], age_col = 'Age', age_ranges = [[19, 30], [31, 200]], class_name='Class', multi_class=True, healthy_only=True) ##19-101 \n",
    "# label_map, dataset2 = map_age_category_to_class(dataset.copy(), all_demography_detail_df.copy(), source_cols=['Subject_Name', 'File_Name'], age_col = 'Age', age_ranges = [[1, 30], [40, 60], [70, 200]], class_name='Class', multi_class=True, healthy_only=True) ##19-101 \n",
    "# print(label_map)\n",
    "# dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35565dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7b888db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_category_to_class(dat_set, source_col='Category', class_name='Class', removable_cats=None, multi_class=True, age_data=None, age_col = 'Age', source_cols=['Subject_Name', 'File_Name']): \n",
    "    if class_name in dat_set.columns.tolist():\n",
    "        dat_set = dat_set.drop(columns=[class_name])\n",
    "    dat_set.insert(3, class_name, dat_set[source_col].values) \n",
    "    dat_set\n",
    "\n",
    "    cat_val = dat_set[source_col].unique().tolist() \n",
    "    cat_val.remove('n')\n",
    "    cat_val.insert(0, 'n')\n",
    "    print(cat_val) \n",
    "    \n",
    "    if removable_cats:\n",
    "        cat_val = [c for c in cat_val if c not in removable_cats]\n",
    "        dat_set = dat_set[dat_set[source_col].isin(cat_val)]\n",
    "        dat_set.reset_index(drop=True, inplace=True)\n",
    "    print(cat_val) \n",
    "        \n",
    "    cls_map = dict(zip(cat_val, list(range(len(cat_val))))) \n",
    "    cls_map\n",
    "    \n",
    "    if not multi_class:\n",
    "        for k in cls_map.keys():\n",
    "            if cls_map[k]>1:\n",
    "                cls_map[k]=1\n",
    "\n",
    "    dat_set.replace({class_name: cls_map}, inplace=True) \n",
    "    \n",
    "    if age_data is not None: \n",
    "        tmp_df = dat_set.merge(age_data, how='inner', left_on=[source_cols[0]], right_on=[source_cols[1]])\n",
    "        tmp_df\n",
    "        assert tmp_df.shape[0]==dat_set.shape[0], \"Something hapend. No shape after dataframe joining is same...\" \n",
    "        sel_ind = dat_set.columns.values.tolist().index(class_name)+1 \n",
    "        dat_set.insert(sel_ind, age_col, tmp_df[age_col].values)\n",
    "    \n",
    "    return cls_map, dat_set \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "id": "201a5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_map, dataset2 = map_category_to_class(dataset.copy(), source_col='Category', class_name='Class', removable_cats=None, multi_class=False, age_data=all_demography_detail_df.copy(), age_col = 'Age', source_cols=['Subject_Name', 'File_Name'])\n",
    "# print(label_map)\n",
    "# dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7a569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b753242e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a829fb9f",
   "metadata": {},
   "source": [
    "#### Get features with zeo transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94864e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_with_zero_values(tmp_df):\n",
    "    feat_names = tmp_df.columns.values[4:] \n",
    "    feat_names\n",
    "    zero_feats = [] \n",
    "    for f in feat_names:\n",
    "        if tmp_df[f].values.sum() == 0:\n",
    "            zero_feats.append(f)\n",
    "    return zero_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da466d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77dfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7986dcbb",
   "metadata": {},
   "source": [
    "# Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d624b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_directory(path, exp_name): \n",
    "    exp_directory = f\"{path}/{exp_name}/\"\n",
    "\n",
    "    if (not os.path.exists(exp_directory)):\n",
    "        try:\n",
    "            os.makedirs(exp_directory, exist_ok = True)\n",
    "            print(f\"Directory successfully created at path: {exp_directory}\") \n",
    "        except OSError as error:\n",
    "            print(f\"Directory cannot be created at path: {exp_directory}\") \n",
    "    else:\n",
    "        print(f\"Directory already exists at path: {exp_directory}\") \n",
    "\n",
    "    return exp_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce76f844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<ML_Classifiers.SVC: 'support_vector_classifier'>,\n",
       " <ML_Classifiers.XGBoost: 'xtreme_gradient_boosting'>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_Classifiers.SVC, ML_Classifiers.XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2d4eed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('__main__',\n",
       " 'C:\\\\Users\\\\aliem\\\\OneDrive - Deakin University\\\\_MyResearch\\\\PhD_Research\\\\HML_IHC_Sleep_Data_Analysis')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__name__, os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8acdc963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_logger(result_save_path, exp_name): \n",
    "    util = Humachlab_Utility() \n",
    "    all_log_file_name = f'{result_save_path}/all_logs_{exp_name}.txt'\n",
    "    # logger = util.get_logger(logger_name=__name__, log_file_name=all_log_file_name)\n",
    "    logger = util.get_logger(logger_name=\"Sleep ML Model Analysis\", log_file_name=all_log_file_name)\n",
    "    util, all_log_file_name, logger \n",
    "    return util, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ea1574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_logger(logger): \n",
    "    handlers = logger.handlers[:]\n",
    "    for handler in handlers:\n",
    "        logger.removeHandler(handler)\n",
    "        handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7a4a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_experiment_information_summarry(exp_dir, dict_dat=None):\n",
    "    exp_sum_dir = f'{exp_dir}/Experiment_Information.csv'\n",
    "    df = pd.DataFrame(columns=['exp_name', 'stp_from', 'exp_description', 'datasets', 'feature_selection', 'special_consideration', 'classification_type'])\n",
    "    if (os.path.exists(exp_sum_dir)):\n",
    "        df = pd.read_csv(exp_sum_dir, index_col=False) \n",
    "    if dict_dat:\n",
    "        if df.shape[0]>0:\n",
    "            print(dict_dat['exp_name'], (df['exp_name'].values.tolist()))\n",
    "            if dict_dat['exp_name'] in (df['exp_name'].values.tolist()):\n",
    "                # nn = 1 \n",
    "                # print(f'Cannot add this data. this experiment is already exited. Please recheck and do a new experiment')\n",
    "                # assert nn < 0, f'Cannot add this data. this experiment is already exited. Please recheck and do a new experiment' \n",
    "                print(f'This experiment is already exited. So this data is removed...')\n",
    "                df = df.loc[df[\"exp_name\"] != dict_dat['exp_name']] \n",
    "                df.reset_index(drop=True, inplace=True) \n",
    "                \n",
    "        df.loc[len(df)] = dict_dat\n",
    "        df.sort_values(by=['exp_name'], ascending=[True], inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True) \n",
    "        df.to_csv(exp_sum_dir, index=False) \n",
    "        print(f'Data is successfully inserted...')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde428c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a39c517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading criteria: 0, 0, 1, 0, 3\n",
      "Reading data from the file: ./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Annot_Proba_Transition3.csv\n",
      "Statistics from the file: ./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Annot_Proba_Transition3_STAT_bin.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subject_Name</th>\n",
       "      <th>W-&gt;W-&gt;W</th>\n",
       "      <th>W-&gt;W-&gt;S1</th>\n",
       "      <th>W-&gt;W-&gt;S2</th>\n",
       "      <th>W-&gt;W-&gt;S3</th>\n",
       "      <th>W-&gt;W-&gt;S4</th>\n",
       "      <th>W-&gt;W-&gt;REM</th>\n",
       "      <th>W-&gt;S1-&gt;W</th>\n",
       "      <th>...</th>\n",
       "      <th>REM-&gt;S4-&gt;S2</th>\n",
       "      <th>REM-&gt;S4-&gt;S3</th>\n",
       "      <th>REM-&gt;S4-&gt;S4</th>\n",
       "      <th>REM-&gt;S4-&gt;REM</th>\n",
       "      <th>REM-&gt;REM-&gt;W</th>\n",
       "      <th>REM-&gt;REM-&gt;S1</th>\n",
       "      <th>REM-&gt;REM-&gt;S2</th>\n",
       "      <th>REM-&gt;REM-&gt;S3</th>\n",
       "      <th>REM-&gt;REM-&gt;S4</th>\n",
       "      <th>REM-&gt;REM-&gt;REM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux1</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.192771</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.965318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux2</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.126214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb1</td>\n",
       "      <td>0.879581</td>\n",
       "      <td>0.120419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.961165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb2</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.973684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb3</td>\n",
       "      <td>0.934673</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>0.015075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7191</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020725</td>\n",
       "      <td>0.015544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.963731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7201</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7211</td>\n",
       "      <td>0.940789</td>\n",
       "      <td>0.059211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.953020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7221</td>\n",
       "      <td>0.926230</td>\n",
       "      <td>0.073770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.935185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7241</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>0.008811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dataset Category Subject_Name   W->W->W  W->W->S1  W->W->S2  W->W->S3  \\\n",
       "0     CAP_Sleep     brux        brux1  0.795181  0.192771  0.012048       0.0   \n",
       "1     CAP_Sleep     brux        brux2  0.873786  0.126214  0.000000       0.0   \n",
       "2     CAP_Sleep      sdb         sdb1  0.879581  0.120419  0.000000       0.0   \n",
       "3     CAP_Sleep      sdb         sdb2  0.959184  0.040816  0.000000       0.0   \n",
       "4     CAP_Sleep      sdb         sdb3  0.934673  0.050251  0.015075       0.0   \n",
       "..          ...      ...          ...       ...       ...       ...       ...   \n",
       "225  Sleep_EDFX        n       ST7191  0.862745  0.137255  0.000000       0.0   \n",
       "226  Sleep_EDFX        n       ST7201  0.400000  0.600000  0.000000       0.0   \n",
       "227  Sleep_EDFX        n       ST7211  0.940789  0.059211  0.000000       0.0   \n",
       "228  Sleep_EDFX        n       ST7221  0.926230  0.073770  0.000000       0.0   \n",
       "229  Sleep_EDFX        n       ST7241  0.705882  0.294118  0.000000       0.0   \n",
       "\n",
       "     W->W->S4  W->W->REM  W->S1->W  ...  REM->S4->S2  REM->S4->S3  \\\n",
       "0         0.0        0.0  0.100000  ...          0.0          0.0   \n",
       "1         0.0        0.0  0.045455  ...          0.0          0.0   \n",
       "2         0.0        0.0  0.148148  ...          0.0          0.0   \n",
       "3         0.0        0.0  0.142857  ...          0.0          0.0   \n",
       "4         0.0        0.0  0.100000  ...          0.0          0.0   \n",
       "..        ...        ...       ...  ...          ...          ...   \n",
       "225       0.0        0.0  0.058824  ...          0.0          0.0   \n",
       "226       0.0        0.0  0.000000  ...          0.0          0.0   \n",
       "227       0.0        0.0  0.176471  ...          0.0          0.0   \n",
       "228       0.0        0.0  0.037037  ...          0.0          0.0   \n",
       "229       0.0        0.0  0.000000  ...          0.0          0.0   \n",
       "\n",
       "     REM->S4->S4  REM->S4->REM  REM->REM->W  REM->REM->S1  REM->REM->S2  \\\n",
       "0            0.0           0.0     0.034682      0.000000      0.000000   \n",
       "1            0.0           0.0     0.019802      0.004950      0.000000   \n",
       "2            0.0           0.0     0.009709      0.000000      0.029126   \n",
       "3            0.0           0.0     0.000000      0.000000      0.026316   \n",
       "4            0.0           0.0     0.066667      0.000000      0.000000   \n",
       "..           ...           ...          ...           ...           ...   \n",
       "225          0.0           0.0     0.000000      0.020725      0.015544   \n",
       "226          0.0           0.0     0.048000      0.008000      0.016000   \n",
       "227          0.0           0.0     0.026846      0.000000      0.020134   \n",
       "228          0.0           0.0     0.023148      0.041667      0.000000   \n",
       "229          0.0           0.0     0.017621      0.017621      0.008811   \n",
       "\n",
       "     REM->REM->S3  REM->REM->S4  REM->REM->REM  \n",
       "0             0.0           0.0       0.965318  \n",
       "1             0.0           0.0       0.975248  \n",
       "2             0.0           0.0       0.961165  \n",
       "3             0.0           0.0       0.973684  \n",
       "4             0.0           0.0       0.933333  \n",
       "..            ...           ...            ...  \n",
       "225           0.0           0.0       0.963731  \n",
       "226           0.0           0.0       0.928000  \n",
       "227           0.0           0.0       0.953020  \n",
       "228           0.0           0.0       0.935185  \n",
       "229           0.0           0.0       0.955947  \n",
       "\n",
       "[230 rows x 219 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name='Class' \n",
    "mat_type = 0 ## annot_type  # mat_type=annot/tran -- 0/1 \n",
    "STP_From_All = 0 ## int(tran_info_cal_from_all)  # STP_From_All=same/all -- 0/1 \n",
    "TrimW = 1 ## int(wake_state_trimmed)  # TrimW=normal/trimmed -- 0/1 \n",
    "mat_info_type = 0 ## int(tran_info_type) # Proba/Dura/Count -- 0/1/2\n",
    "tran_step = 3 ## int (tran step) # one level=2 step, two level=3 step -- 2/3\n",
    "\n",
    "# ##dataset, stat_dataset = get_dataset_for_file_conditions(mat_type, STP_From_All, TrimW, mat_info_type, tran_step=tran_step) # mat_type=annot/tran, STP_From_All=same/all, TrimW=normal/trimmed, mat_info_type=proba/dura/count, tran_step=2/3 \n",
    "# ##dataset \n",
    "\n",
    "# dataset1, stat_dataset1 = get_dataset_for_file_conditions(mat_type, STP_From_All, TrimW, mat_info_type, tran_step=tran_step) # mat_type=annot/tran, STP_From_All=same/all, TrimW=normal/trimmed, mat_info_type=proba/dura/count \n",
    "# dataset1 \n",
    "\n",
    "dataset2, stat_dataset2 = get_dataset_for_file_conditions(mat_type, STP_From_All, TrimW, mat_info_type, tran_step=tran_step) # mat_type=annot/tran, STP_From_All=same/all, TrimW=normal/trimmed, mat_info_type=proba/dura/count \n",
    "dataset2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ada5f918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((230, 39), (230, 219))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset1.shape, dataset2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2590a84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in dataset2.Subject_Name.values.tolist() if i not in dataset1.Subject_Name.values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "434a66fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner_merged_total = pd.merge(\n",
    "#     dataset1, dataset2, on=['Dataset', 'Category', 'Subject_Name'], suffixes=['_proba', '_dura']\n",
    "# )\n",
    "# inner_merged_total\n",
    "\n",
    "# inner_merged_total = pd.merge(\n",
    "#     dataset1, dataset2, on=['Dataset', 'Category', 'Subject_Name'], suffixes=['_proba', '_dura']\n",
    "# )\n",
    "# print(inner_merged_total.columns.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([stat_dataset1, stat_dataset2], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3741339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Features' 'Sum' 'Mean' 'STD' 'Healthy_Sum' 'Healthy_Mean' 'Healthy_STD'\n",
      " 'Disorder_Sum' 'Disorder_Mean' 'Disorder_STD' 'P_Value' 'AUC'\n",
      " 'Wilcoxon_zscore' 'Wilcoxon_pvalue' 'MannWhitney_statistic'\n",
      " 'MannWhitney_pvalue']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subject_Name</th>\n",
       "      <th>W-&gt;W</th>\n",
       "      <th>W-&gt;S1</th>\n",
       "      <th>W-&gt;S2</th>\n",
       "      <th>W-&gt;S3</th>\n",
       "      <th>W-&gt;S4</th>\n",
       "      <th>W-&gt;REM</th>\n",
       "      <th>S1-&gt;W</th>\n",
       "      <th>...</th>\n",
       "      <th>S4-&gt;S2</th>\n",
       "      <th>S4-&gt;S3</th>\n",
       "      <th>S4-&gt;S4</th>\n",
       "      <th>S4-&gt;REM</th>\n",
       "      <th>REM-&gt;W</th>\n",
       "      <th>REM-&gt;S1</th>\n",
       "      <th>REM-&gt;S2</th>\n",
       "      <th>REM-&gt;S3</th>\n",
       "      <th>REM-&gt;S4</th>\n",
       "      <th>REM-&gt;REM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux1</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.970443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.966480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux2</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.984940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb1</td>\n",
       "      <td>0.876147</td>\n",
       "      <td>0.123853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.949239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.962617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb2</td>\n",
       "      <td>0.948387</td>\n",
       "      <td>0.045161</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.974359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb3</td>\n",
       "      <td>0.934272</td>\n",
       "      <td>0.046948</td>\n",
       "      <td>0.018779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.935065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7191</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7201</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.932836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7211</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7221</td>\n",
       "      <td>0.818792</td>\n",
       "      <td>0.181208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021645</td>\n",
       "      <td>0.043290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.935065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7241</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dataset Category Subject_Name      W->W     W->S1     W->S2  W->S3  \\\n",
       "0     CAP_Sleep     brux        brux1  0.798077  0.192308  0.009615    0.0   \n",
       "1     CAP_Sleep     brux        brux2  0.824000  0.176000  0.000000    0.0   \n",
       "2     CAP_Sleep      sdb         sdb1  0.876147  0.123853  0.000000    0.0   \n",
       "3     CAP_Sleep      sdb         sdb2  0.948387  0.045161  0.006452    0.0   \n",
       "4     CAP_Sleep      sdb         sdb3  0.934272  0.046948  0.018779    0.0   \n",
       "..          ...      ...          ...       ...       ...       ...    ...   \n",
       "225  Sleep_EDFX        n       ST7191  0.739130  0.246377  0.014493    0.0   \n",
       "226  Sleep_EDFX        n       ST7201  0.294118  0.588235  0.058824    0.0   \n",
       "227  Sleep_EDFX        n       ST7211  0.894118  0.100000  0.000000    0.0   \n",
       "228  Sleep_EDFX        n       ST7221  0.818792  0.181208  0.000000    0.0   \n",
       "229  Sleep_EDFX        n       ST7241  0.607143  0.285714  0.035714    0.0   \n",
       "\n",
       "     W->S4    W->REM     S1->W  ...    S4->S2    S4->S3    S4->S4  S4->REM  \\\n",
       "0      0.0  0.000000  0.030769  ...  0.004926  0.024631  0.970443      0.0   \n",
       "1      0.0  0.000000  0.061728  ...  0.000000  0.012048  0.984940      0.0   \n",
       "2      0.0  0.000000  0.099237  ...  0.005076  0.030457  0.949239      0.0   \n",
       "3      0.0  0.000000  0.011364  ...  0.008475  0.016949  0.966102      0.0   \n",
       "4      0.0  0.000000  0.108696  ...  0.025974  0.038961  0.935065      0.0   \n",
       "..     ...       ...       ...  ...       ...       ...       ...      ...   \n",
       "225    0.0  0.000000  0.044118  ...  0.013889  0.166667  0.805556      0.0   \n",
       "226    0.0  0.058824  0.014493  ...  0.038462  0.615385  0.346154      0.0   \n",
       "227    0.0  0.005882  0.116667  ...  0.016949  0.254237  0.728814      0.0   \n",
       "228    0.0  0.000000  0.052133  ...  0.000000  0.000000  0.000000      0.0   \n",
       "229    0.0  0.071429  0.047619  ...  0.142857  0.571429  0.142857      0.0   \n",
       "\n",
       "       REM->W   REM->S1   REM->S2  REM->S3  REM->S4  REM->REM  \n",
       "0    0.033520  0.000000  0.000000      0.0      0.0  0.966480  \n",
       "1    0.019324  0.004831  0.000000      0.0      0.0  0.975845  \n",
       "2    0.009346  0.000000  0.028037      0.0      0.0  0.962617  \n",
       "3    0.000000  0.000000  0.025641      0.0      0.0  0.974359  \n",
       "4    0.062500  0.000000  0.000000      0.0      0.0  0.937500  \n",
       "..        ...       ...       ...      ...      ...       ...  \n",
       "225  0.000000  0.020000  0.015000      0.0      0.0  0.965000  \n",
       "226  0.044776  0.007463  0.014925      0.0      0.0  0.932836  \n",
       "227  0.025641  0.000000  0.019231      0.0      0.0  0.955128  \n",
       "228  0.021645  0.043290  0.000000      0.0      0.0  0.935065  \n",
       "229  0.016878  0.016878  0.008439      0.0      0.0  0.957806  \n",
       "\n",
       "[230 rows x 39 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = pd.concat([dataset1.add_prefix('proba_'), dataset2.add_prefix('dura_')], axis=1)\n",
    "# dataset = pd.merge(dataset1, dataset2, on=['Dataset', 'Category', 'Subject_Name'], suffixes=['_proba', '_dura'])\n",
    "# dataset = pd.merge(dataset1, dataset2, on=['Dataset', 'Category', 'Subject_Name'], suffixes=['_annot', '_tran'])\n",
    "\n",
    "dataset = dataset1.copy()\n",
    "stat_dataset = stat_dataset1.copy() \n",
    "# dataset = dataset2.copy()\n",
    "# stat_dataset = stat_dataset2.copy() \n",
    "\n",
    "# dataset = pd.merge(\n",
    "#     dataset1, dataset2, on=['Dataset', 'Category', 'Subject_Name'], suffixes=['_proba', '_dura']\n",
    "# )\n",
    "# stat_dataset = pd.concat([stat_dataset1, stat_dataset2], axis=0) \n",
    "\n",
    "print(stat_dataset.columns.values) \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1328,
   "id": "297f9e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "160db897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_df = dataset[~(dataset['Dataset']=='SDRC')].copy() \n",
    "# tmp_df = tmp_df[(tmp_df['Category']=='n')]\n",
    "# tmp_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "eeb61238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap_ndf = tmp_df[(tmp_df['Dataset']=='CAP_Sleep')] \n",
    "# cap_ndf.columns = [col if i<3 else f\"CAP_Sleep_{col}\" for i,col in enumerate(cap_ndf.columns)] \n",
    "# edfx_ndf = tmp_df[(tmp_df['Dataset']=='Sleep_EDFX')] \n",
    "# edfx_ndf.columns = [col if i<3 else f\"Sleep_EDFX_{col}\" for i,col in enumerate(edfx_ndf.columns)] \n",
    "# edfx_ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fec4d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([ cap_ndf.describe().T[['mean', 'std']], edfx_ndf.describe().T[['mean', 'std']] ], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0502a75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(25, 20))\n",
    "heatmap = sns.heatmap(dataset.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG', fmt='.2g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "da1e852f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W->S1',\n",
       " 'S1->S2',\n",
       " 'S1->S4',\n",
       " 'S2->S2',\n",
       " 'S2->S4',\n",
       " 'REM->W',\n",
       " 'REM->S4',\n",
       " 'REM->REM']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset.corr()\n",
    "corr_matrix = dataset.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop_corr = [column for column in upper.columns if any(upper[column] > 0.70)]\n",
    "to_drop_corr\n",
    "# # Drop features \n",
    "# df.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d98b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efaff2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "516c341f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CAP_Sleep', 'SDRC', 'Sleep_EDFX'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Dataset'].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83fd5ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Subject_Name'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fedc035c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 []\n"
     ]
    }
   ],
   "source": [
    "zero_feats = get_features_with_zero_values(dataset)\n",
    "print(len(zero_feats), zero_feats) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096334cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e0d9d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Sum</th>\n",
       "      <th>Mean</th>\n",
       "      <th>STD</th>\n",
       "      <th>Healthy_Sum</th>\n",
       "      <th>Healthy_Mean</th>\n",
       "      <th>Healthy_STD</th>\n",
       "      <th>Disorder_Sum</th>\n",
       "      <th>Disorder_Mean</th>\n",
       "      <th>Disorder_STD</th>\n",
       "      <th>P_Value</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Wilcoxon_zscore</th>\n",
       "      <th>Wilcoxon_pvalue</th>\n",
       "      <th>MannWhitney_statistic</th>\n",
       "      <th>MannWhitney_pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W-&gt;W</td>\n",
       "      <td>200.023853</td>\n",
       "      <td>0.869669</td>\n",
       "      <td>0.111130</td>\n",
       "      <td>108.678140</td>\n",
       "      <td>0.855733</td>\n",
       "      <td>0.120193</td>\n",
       "      <td>91.345712</td>\n",
       "      <td>0.886852</td>\n",
       "      <td>0.096074</td>\n",
       "      <td>3.481301e-02</td>\n",
       "      <td>0.403218</td>\n",
       "      <td>-2.522887</td>\n",
       "      <td>1.163957e-02</td>\n",
       "      <td>5274.5</td>\n",
       "      <td>5.836077e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W-&gt;S1</td>\n",
       "      <td>24.969913</td>\n",
       "      <td>0.108565</td>\n",
       "      <td>0.094053</td>\n",
       "      <td>15.941897</td>\n",
       "      <td>0.125527</td>\n",
       "      <td>0.099497</td>\n",
       "      <td>9.028017</td>\n",
       "      <td>0.087651</td>\n",
       "      <td>0.082186</td>\n",
       "      <td>2.277298e-03</td>\n",
       "      <td>0.347374</td>\n",
       "      <td>3.978629</td>\n",
       "      <td>6.931372e-05</td>\n",
       "      <td>4544.0</td>\n",
       "      <td>3.479935e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W-&gt;S2</td>\n",
       "      <td>4.074743</td>\n",
       "      <td>0.017716</td>\n",
       "      <td>0.030857</td>\n",
       "      <td>1.629452</td>\n",
       "      <td>0.012830</td>\n",
       "      <td>0.027523</td>\n",
       "      <td>2.445291</td>\n",
       "      <td>0.023741</td>\n",
       "      <td>0.033563</td>\n",
       "      <td>7.522503e-03</td>\n",
       "      <td>0.653811</td>\n",
       "      <td>-4.009518</td>\n",
       "      <td>6.084289e-05</td>\n",
       "      <td>4528.5</td>\n",
       "      <td>1.431124e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W-&gt;S3</td>\n",
       "      <td>0.219641</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.004911</td>\n",
       "      <td>0.108293</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.004927</td>\n",
       "      <td>0.111348</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.004890</td>\n",
       "      <td>7.272645e-01</td>\n",
       "      <td>0.511199</td>\n",
       "      <td>-0.291945</td>\n",
       "      <td>7.703283e-01</td>\n",
       "      <td>6394.0</td>\n",
       "      <td>2.483123e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W-&gt;S4</td>\n",
       "      <td>0.125777</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.007757</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.010398</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>4.121993e-01</td>\n",
       "      <td>0.500879</td>\n",
       "      <td>-0.022917</td>\n",
       "      <td>9.817163e-01</td>\n",
       "      <td>6529.0</td>\n",
       "      <td>4.457874e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>W-&gt;REM</td>\n",
       "      <td>0.586073</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.009081</td>\n",
       "      <td>0.524571</td>\n",
       "      <td>0.004130</td>\n",
       "      <td>0.011837</td>\n",
       "      <td>0.061502</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>3.217895e-03</td>\n",
       "      <td>0.422139</td>\n",
       "      <td>2.029669</td>\n",
       "      <td>4.239021e-02</td>\n",
       "      <td>5522.0</td>\n",
       "      <td>1.180169e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S1-&gt;W</td>\n",
       "      <td>15.246232</td>\n",
       "      <td>0.066288</td>\n",
       "      <td>0.067213</td>\n",
       "      <td>9.969784</td>\n",
       "      <td>0.078502</td>\n",
       "      <td>0.053088</td>\n",
       "      <td>5.276448</td>\n",
       "      <td>0.051228</td>\n",
       "      <td>0.078752</td>\n",
       "      <td>2.102759e-03</td>\n",
       "      <td>0.700673</td>\n",
       "      <td>5.231105</td>\n",
       "      <td>1.684994e-07</td>\n",
       "      <td>3915.5</td>\n",
       "      <td>8.178827e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S1-&gt;S1</td>\n",
       "      <td>164.464191</td>\n",
       "      <td>0.715062</td>\n",
       "      <td>0.143797</td>\n",
       "      <td>85.942910</td>\n",
       "      <td>0.676716</td>\n",
       "      <td>0.142570</td>\n",
       "      <td>78.521282</td>\n",
       "      <td>0.762343</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>4.897664e-06</td>\n",
       "      <td>0.686301</td>\n",
       "      <td>-4.854466</td>\n",
       "      <td>1.207117e-06</td>\n",
       "      <td>4104.5</td>\n",
       "      <td>6.063551e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S1-&gt;S2</td>\n",
       "      <td>44.629370</td>\n",
       "      <td>0.194041</td>\n",
       "      <td>0.112006</td>\n",
       "      <td>27.225130</td>\n",
       "      <td>0.214371</td>\n",
       "      <td>0.116463</td>\n",
       "      <td>17.404240</td>\n",
       "      <td>0.168973</td>\n",
       "      <td>0.100755</td>\n",
       "      <td>2.128943e-03</td>\n",
       "      <td>0.381011</td>\n",
       "      <td>3.101796</td>\n",
       "      <td>1.923502e-03</td>\n",
       "      <td>4984.0</td>\n",
       "      <td>9.646592e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S1-&gt;S3</td>\n",
       "      <td>1.493990</td>\n",
       "      <td>0.006496</td>\n",
       "      <td>0.026515</td>\n",
       "      <td>0.857565</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>0.024687</td>\n",
       "      <td>0.636425</td>\n",
       "      <td>0.006179</td>\n",
       "      <td>0.028607</td>\n",
       "      <td>8.711021e-01</td>\n",
       "      <td>0.505695</td>\n",
       "      <td>0.148464</td>\n",
       "      <td>8.819768e-01</td>\n",
       "      <td>6466.0</td>\n",
       "      <td>3.975669e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S1-&gt;S4</td>\n",
       "      <td>0.065311</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.028199</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.037113</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>6.595031e-01</td>\n",
       "      <td>0.488457</td>\n",
       "      <td>-0.300913</td>\n",
       "      <td>7.634808e-01</td>\n",
       "      <td>6389.5</td>\n",
       "      <td>1.386924e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>S1-&gt;REM</td>\n",
       "      <td>4.100905</td>\n",
       "      <td>0.017830</td>\n",
       "      <td>0.026896</td>\n",
       "      <td>2.976412</td>\n",
       "      <td>0.023436</td>\n",
       "      <td>0.030251</td>\n",
       "      <td>1.124493</td>\n",
       "      <td>0.010917</td>\n",
       "      <td>0.020013</td>\n",
       "      <td>4.013109e-04</td>\n",
       "      <td>0.329600</td>\n",
       "      <td>4.445941</td>\n",
       "      <td>8.750782e-06</td>\n",
       "      <td>4309.5</td>\n",
       "      <td>1.435113e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>S2-&gt;W</td>\n",
       "      <td>4.539591</td>\n",
       "      <td>0.019737</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>1.833698</td>\n",
       "      <td>0.014439</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>2.705894</td>\n",
       "      <td>0.026271</td>\n",
       "      <td>0.080325</td>\n",
       "      <td>1.036297e-01</td>\n",
       "      <td>0.616467</td>\n",
       "      <td>3.036034</td>\n",
       "      <td>2.397124e-03</td>\n",
       "      <td>5017.0</td>\n",
       "      <td>1.201852e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>S2-&gt;S1</td>\n",
       "      <td>9.884910</td>\n",
       "      <td>0.042978</td>\n",
       "      <td>0.103662</td>\n",
       "      <td>6.406378</td>\n",
       "      <td>0.050444</td>\n",
       "      <td>0.112111</td>\n",
       "      <td>3.478532</td>\n",
       "      <td>0.033772</td>\n",
       "      <td>0.091348</td>\n",
       "      <td>2.269664e-01</td>\n",
       "      <td>0.732436</td>\n",
       "      <td>6.059115</td>\n",
       "      <td>1.368728e-09</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>6.512661e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>S2-&gt;S2</td>\n",
       "      <td>204.594912</td>\n",
       "      <td>0.889543</td>\n",
       "      <td>0.157759</td>\n",
       "      <td>111.333292</td>\n",
       "      <td>0.876640</td>\n",
       "      <td>0.138031</td>\n",
       "      <td>93.261620</td>\n",
       "      <td>0.905453</td>\n",
       "      <td>0.177832</td>\n",
       "      <td>1.698499e-01</td>\n",
       "      <td>0.240081</td>\n",
       "      <td>-6.775527</td>\n",
       "      <td>1.239536e-11</td>\n",
       "      <td>3140.5</td>\n",
       "      <td>6.240471e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>S2-&gt;S3</td>\n",
       "      <td>8.390459</td>\n",
       "      <td>0.036480</td>\n",
       "      <td>0.042502</td>\n",
       "      <td>5.940230</td>\n",
       "      <td>0.046773</td>\n",
       "      <td>0.044281</td>\n",
       "      <td>2.450230</td>\n",
       "      <td>0.023789</td>\n",
       "      <td>0.036392</td>\n",
       "      <td>3.588028e-05</td>\n",
       "      <td>0.689626</td>\n",
       "      <td>4.943145</td>\n",
       "      <td>7.687208e-07</td>\n",
       "      <td>4060.0</td>\n",
       "      <td>3.857922e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>S2-&gt;S4</td>\n",
       "      <td>0.092039</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.076477</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.015561</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>6.547331e-02</td>\n",
       "      <td>0.464185</td>\n",
       "      <td>0.933628</td>\n",
       "      <td>3.504959e-01</td>\n",
       "      <td>6072.0</td>\n",
       "      <td>2.817489e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>S2-&gt;REM</td>\n",
       "      <td>2.498089</td>\n",
       "      <td>0.010861</td>\n",
       "      <td>0.011107</td>\n",
       "      <td>1.409925</td>\n",
       "      <td>0.011102</td>\n",
       "      <td>0.007044</td>\n",
       "      <td>1.088163</td>\n",
       "      <td>0.010565</td>\n",
       "      <td>0.014633</td>\n",
       "      <td>7.167950e-01</td>\n",
       "      <td>0.639248</td>\n",
       "      <td>3.625903</td>\n",
       "      <td>2.879531e-04</td>\n",
       "      <td>4721.0</td>\n",
       "      <td>1.445188e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>S3-&gt;W</td>\n",
       "      <td>2.630026</td>\n",
       "      <td>0.011435</td>\n",
       "      <td>0.043096</td>\n",
       "      <td>1.622712</td>\n",
       "      <td>0.012777</td>\n",
       "      <td>0.031093</td>\n",
       "      <td>1.007314</td>\n",
       "      <td>0.009780</td>\n",
       "      <td>0.054316</td>\n",
       "      <td>6.017798e-01</td>\n",
       "      <td>0.442665</td>\n",
       "      <td>1.494602</td>\n",
       "      <td>1.350185e-01</td>\n",
       "      <td>5790.5</td>\n",
       "      <td>4.815483e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>S3-&gt;S1</td>\n",
       "      <td>4.076654</td>\n",
       "      <td>0.017725</td>\n",
       "      <td>0.078417</td>\n",
       "      <td>2.016563</td>\n",
       "      <td>0.015878</td>\n",
       "      <td>0.046974</td>\n",
       "      <td>2.060091</td>\n",
       "      <td>0.020001</td>\n",
       "      <td>0.104887</td>\n",
       "      <td>6.933089e-01</td>\n",
       "      <td>0.559399</td>\n",
       "      <td>1.548407</td>\n",
       "      <td>1.215243e-01</td>\n",
       "      <td>5763.5</td>\n",
       "      <td>2.506618e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>S3-&gt;S2</td>\n",
       "      <td>42.811394</td>\n",
       "      <td>0.186136</td>\n",
       "      <td>0.250092</td>\n",
       "      <td>38.829390</td>\n",
       "      <td>0.305743</td>\n",
       "      <td>0.282573</td>\n",
       "      <td>3.982004</td>\n",
       "      <td>0.038660</td>\n",
       "      <td>0.042719</td>\n",
       "      <td>3.887191e-18</td>\n",
       "      <td>0.831206</td>\n",
       "      <td>8.633815</td>\n",
       "      <td>5.933901e-18</td>\n",
       "      <td>2208.0</td>\n",
       "      <td>2.969660e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>S3-&gt;S3</td>\n",
       "      <td>157.753562</td>\n",
       "      <td>0.685885</td>\n",
       "      <td>0.304326</td>\n",
       "      <td>66.038094</td>\n",
       "      <td>0.519985</td>\n",
       "      <td>0.288398</td>\n",
       "      <td>91.715468</td>\n",
       "      <td>0.890441</td>\n",
       "      <td>0.168750</td>\n",
       "      <td>2.200777e-24</td>\n",
       "      <td>0.886821</td>\n",
       "      <td>-10.083578</td>\n",
       "      <td>6.530362e-24</td>\n",
       "      <td>1480.5</td>\n",
       "      <td>3.203071e-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>S3-&gt;S4</td>\n",
       "      <td>12.123690</td>\n",
       "      <td>0.052712</td>\n",
       "      <td>0.079878</td>\n",
       "      <td>9.205868</td>\n",
       "      <td>0.072487</td>\n",
       "      <td>0.101889</td>\n",
       "      <td>2.917822</td>\n",
       "      <td>0.028328</td>\n",
       "      <td>0.019256</td>\n",
       "      <td>2.356107e-05</td>\n",
       "      <td>0.484023</td>\n",
       "      <td>0.416496</td>\n",
       "      <td>6.770474e-01</td>\n",
       "      <td>6331.5</td>\n",
       "      <td>3.366042e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>S3-&gt;REM</td>\n",
       "      <td>0.604675</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.013940</td>\n",
       "      <td>0.287374</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>0.013182</td>\n",
       "      <td>0.317301</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>0.014809</td>\n",
       "      <td>6.598637e-01</td>\n",
       "      <td>0.501567</td>\n",
       "      <td>0.040852</td>\n",
       "      <td>9.674135e-01</td>\n",
       "      <td>6520.0</td>\n",
       "      <td>4.705984e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>S4-&gt;W</td>\n",
       "      <td>0.893508</td>\n",
       "      <td>0.003885</td>\n",
       "      <td>0.009809</td>\n",
       "      <td>0.453102</td>\n",
       "      <td>0.003568</td>\n",
       "      <td>0.011202</td>\n",
       "      <td>0.440406</td>\n",
       "      <td>0.004276</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>5.881061e-01</td>\n",
       "      <td>0.391981</td>\n",
       "      <td>-2.815829</td>\n",
       "      <td>4.865151e-03</td>\n",
       "      <td>5127.5</td>\n",
       "      <td>2.576184e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>S4-&gt;S1</td>\n",
       "      <td>3.261657</td>\n",
       "      <td>0.014181</td>\n",
       "      <td>0.099853</td>\n",
       "      <td>0.453834</td>\n",
       "      <td>0.003573</td>\n",
       "      <td>0.019148</td>\n",
       "      <td>2.807823</td>\n",
       "      <td>0.027260</td>\n",
       "      <td>0.146637</td>\n",
       "      <td>7.418514e-02</td>\n",
       "      <td>0.519838</td>\n",
       "      <td>-0.517132</td>\n",
       "      <td>6.050639e-01</td>\n",
       "      <td>6281.0</td>\n",
       "      <td>2.044395e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>S4-&gt;S2</td>\n",
       "      <td>3.678762</td>\n",
       "      <td>0.015995</td>\n",
       "      <td>0.079995</td>\n",
       "      <td>2.514545</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.097977</td>\n",
       "      <td>1.164217</td>\n",
       "      <td>0.011303</td>\n",
       "      <td>0.049125</td>\n",
       "      <td>4.253375e-01</td>\n",
       "      <td>0.602248</td>\n",
       "      <td>-2.665373</td>\n",
       "      <td>7.690303e-03</td>\n",
       "      <td>5203.0</td>\n",
       "      <td>1.544964e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>S4-&gt;S3</td>\n",
       "      <td>22.586227</td>\n",
       "      <td>0.098201</td>\n",
       "      <td>0.215609</td>\n",
       "      <td>21.093086</td>\n",
       "      <td>0.166087</td>\n",
       "      <td>0.271468</td>\n",
       "      <td>1.493141</td>\n",
       "      <td>0.014497</td>\n",
       "      <td>0.015860</td>\n",
       "      <td>5.144747e-08</td>\n",
       "      <td>0.528362</td>\n",
       "      <td>0.739330</td>\n",
       "      <td>4.597069e-01</td>\n",
       "      <td>6169.5</td>\n",
       "      <td>2.253108e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>S4-&gt;S4</td>\n",
       "      <td>134.511382</td>\n",
       "      <td>0.584832</td>\n",
       "      <td>0.438940</td>\n",
       "      <td>48.446972</td>\n",
       "      <td>0.381472</td>\n",
       "      <td>0.408706</td>\n",
       "      <td>86.064410</td>\n",
       "      <td>0.835577</td>\n",
       "      <td>0.332269</td>\n",
       "      <td>6.105710e-17</td>\n",
       "      <td>0.813890</td>\n",
       "      <td>-8.182445</td>\n",
       "      <td>2.781413e-16</td>\n",
       "      <td>2434.5</td>\n",
       "      <td>4.354174e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>S4-&gt;REM</td>\n",
       "      <td>1.068465</td>\n",
       "      <td>0.004645</td>\n",
       "      <td>0.065837</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>1.030003</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.098045</td>\n",
       "      <td>2.686266e-01</td>\n",
       "      <td>0.484596</td>\n",
       "      <td>-0.401550</td>\n",
       "      <td>6.880155e-01</td>\n",
       "      <td>6339.0</td>\n",
       "      <td>5.640681e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>REM-&gt;W</td>\n",
       "      <td>4.255135</td>\n",
       "      <td>0.018501</td>\n",
       "      <td>0.033948</td>\n",
       "      <td>2.317329</td>\n",
       "      <td>0.018247</td>\n",
       "      <td>0.021502</td>\n",
       "      <td>1.937806</td>\n",
       "      <td>0.018814</td>\n",
       "      <td>0.044757</td>\n",
       "      <td>9.003139e-01</td>\n",
       "      <td>0.585506</td>\n",
       "      <td>1.447771</td>\n",
       "      <td>1.476812e-01</td>\n",
       "      <td>5814.0</td>\n",
       "      <td>7.188721e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>REM-&gt;S1</td>\n",
       "      <td>5.264456</td>\n",
       "      <td>0.022889</td>\n",
       "      <td>0.039833</td>\n",
       "      <td>3.933551</td>\n",
       "      <td>0.030973</td>\n",
       "      <td>0.048043</td>\n",
       "      <td>1.330904</td>\n",
       "      <td>0.012921</td>\n",
       "      <td>0.022741</td>\n",
       "      <td>5.743439e-04</td>\n",
       "      <td>0.335869</td>\n",
       "      <td>4.278546</td>\n",
       "      <td>1.881181e-05</td>\n",
       "      <td>4393.5</td>\n",
       "      <td>5.780181e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>REM-&gt;S2</td>\n",
       "      <td>3.695046</td>\n",
       "      <td>0.016065</td>\n",
       "      <td>0.017879</td>\n",
       "      <td>1.976139</td>\n",
       "      <td>0.015560</td>\n",
       "      <td>0.020125</td>\n",
       "      <td>1.718908</td>\n",
       "      <td>0.016688</td>\n",
       "      <td>0.014620</td>\n",
       "      <td>6.359048e-01</td>\n",
       "      <td>0.445532</td>\n",
       "      <td>-1.419871</td>\n",
       "      <td>1.556451e-01</td>\n",
       "      <td>5828.0</td>\n",
       "      <td>7.683629e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>REM-&gt;S3</td>\n",
       "      <td>0.276820</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.006982</td>\n",
       "      <td>0.169087</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.007632</td>\n",
       "      <td>0.107733</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>7.591080e-01</td>\n",
       "      <td>0.495184</td>\n",
       "      <td>-0.125547</td>\n",
       "      <td>9.000909e-01</td>\n",
       "      <td>6477.5</td>\n",
       "      <td>3.732689e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>REM-&gt;S4</td>\n",
       "      <td>0.015619</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>6.503802e-01</td>\n",
       "      <td>0.500879</td>\n",
       "      <td>-0.022917</td>\n",
       "      <td>9.817163e-01</td>\n",
       "      <td>6529.0</td>\n",
       "      <td>4.457874e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>REM-&gt;REM</td>\n",
       "      <td>215.492923</td>\n",
       "      <td>0.936926</td>\n",
       "      <td>0.087101</td>\n",
       "      <td>118.592530</td>\n",
       "      <td>0.933799</td>\n",
       "      <td>0.062999</td>\n",
       "      <td>96.900394</td>\n",
       "      <td>0.940781</td>\n",
       "      <td>0.109638</td>\n",
       "      <td>5.475819e-01</td>\n",
       "      <td>0.361517</td>\n",
       "      <td>-3.609961</td>\n",
       "      <td>3.062432e-04</td>\n",
       "      <td>4729.0</td>\n",
       "      <td>1.536876e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Features         Sum      Mean       STD  Healthy_Sum  Healthy_Mean  \\\n",
       "0       W->W  200.023853  0.869669  0.111130   108.678140      0.855733   \n",
       "1      W->S1   24.969913  0.108565  0.094053    15.941897      0.125527   \n",
       "2      W->S2    4.074743  0.017716  0.030857     1.629452      0.012830   \n",
       "3      W->S3    0.219641  0.000955  0.004911     0.108293      0.000853   \n",
       "4      W->S4    0.125777  0.000547  0.007757     0.117647      0.000926   \n",
       "5     W->REM    0.586073  0.002548  0.009081     0.524571      0.004130   \n",
       "6      S1->W   15.246232  0.066288  0.067213     9.969784      0.078502   \n",
       "7     S1->S1  164.464191  0.715062  0.143797    85.942910      0.676716   \n",
       "8     S1->S2   44.629370  0.194041  0.112006    27.225130      0.214371   \n",
       "9     S1->S3    1.493990  0.006496  0.026515     0.857565      0.006752   \n",
       "10    S1->S4    0.065311  0.000284  0.002354     0.028199      0.000222   \n",
       "11   S1->REM    4.100905  0.017830  0.026896     2.976412      0.023436   \n",
       "12     S2->W    4.539591  0.019737  0.054688     1.833698      0.014439   \n",
       "13    S2->S1    9.884910  0.042978  0.103662     6.406378      0.050444   \n",
       "14    S2->S2  204.594912  0.889543  0.157759   111.333292      0.876640   \n",
       "15    S2->S3    8.390459  0.036480  0.042502     5.940230      0.046773   \n",
       "16    S2->S4    0.092039  0.000400  0.001844     0.076477      0.000602   \n",
       "17   S2->REM    2.498089  0.010861  0.011107     1.409925      0.011102   \n",
       "18     S3->W    2.630026  0.011435  0.043096     1.622712      0.012777   \n",
       "19    S3->S1    4.076654  0.017725  0.078417     2.016563      0.015878   \n",
       "20    S3->S2   42.811394  0.186136  0.250092    38.829390      0.305743   \n",
       "21    S3->S3  157.753562  0.685885  0.304326    66.038094      0.519985   \n",
       "22    S3->S4   12.123690  0.052712  0.079878     9.205868      0.072487   \n",
       "23   S3->REM    0.604675  0.002629  0.013940     0.287374      0.002263   \n",
       "24     S4->W    0.893508  0.003885  0.009809     0.453102      0.003568   \n",
       "25    S4->S1    3.261657  0.014181  0.099853     0.453834      0.003573   \n",
       "26    S4->S2    3.678762  0.015995  0.079995     2.514545      0.019800   \n",
       "27    S4->S3   22.586227  0.098201  0.215609    21.093086      0.166087   \n",
       "28    S4->S4  134.511382  0.584832  0.438940    48.446972      0.381472   \n",
       "29   S4->REM    1.068465  0.004645  0.065837     0.038462      0.000303   \n",
       "30    REM->W    4.255135  0.018501  0.033948     2.317329      0.018247   \n",
       "31   REM->S1    5.264456  0.022889  0.039833     3.933551      0.030973   \n",
       "32   REM->S2    3.695046  0.016065  0.017879     1.976139      0.015560   \n",
       "33   REM->S3    0.276820  0.001204  0.006982     0.169087      0.001331   \n",
       "34   REM->S4    0.015619  0.000068  0.000797     0.011364      0.000089   \n",
       "35  REM->REM  215.492923  0.936926  0.087101   118.592530      0.933799   \n",
       "\n",
       "    Healthy_STD  Disorder_Sum  Disorder_Mean  Disorder_STD       P_Value  \\\n",
       "0      0.120193     91.345712       0.886852      0.096074  3.481301e-02   \n",
       "1      0.099497      9.028017       0.087651      0.082186  2.277298e-03   \n",
       "2      0.027523      2.445291       0.023741      0.033563  7.522503e-03   \n",
       "3      0.004927      0.111348       0.001081      0.004890  7.272645e-01   \n",
       "4      0.010398      0.008130       0.000079      0.000797  4.121993e-01   \n",
       "5      0.011837      0.061502       0.000597      0.002123  3.217895e-03   \n",
       "6      0.053088      5.276448       0.051228      0.078752  2.102759e-03   \n",
       "7      0.142570     78.521282       0.762343      0.130624  4.897664e-06   \n",
       "8      0.116463     17.404240       0.168973      0.100755  2.128943e-03   \n",
       "9      0.024687      0.636425       0.006179      0.028607  8.711021e-01   \n",
       "10     0.002267      0.037113       0.000360      0.002456  6.595031e-01   \n",
       "11     0.030251      1.124493       0.010917      0.020013  4.013109e-04   \n",
       "12     0.010992      2.705894       0.026271      0.080325  1.036297e-01   \n",
       "13     0.112111      3.478532       0.033772      0.091348  2.269664e-01   \n",
       "14     0.138031     93.261620       0.905453      0.177832  1.698499e-01   \n",
       "15     0.044281      2.450230       0.023789      0.036392  3.588028e-05   \n",
       "16     0.002351      0.015561       0.000151      0.000815  6.547331e-02   \n",
       "17     0.007044      1.088163       0.010565      0.014633  7.167950e-01   \n",
       "18     0.031093      1.007314       0.009780      0.054316  6.017798e-01   \n",
       "19     0.046974      2.060091       0.020001      0.104887  6.933089e-01   \n",
       "20     0.282573      3.982004       0.038660      0.042719  3.887191e-18   \n",
       "21     0.288398     91.715468       0.890441      0.168750  2.200777e-24   \n",
       "22     0.101889      2.917822       0.028328      0.019256  2.356107e-05   \n",
       "23     0.013182      0.317301       0.003081      0.014809  6.598637e-01   \n",
       "24     0.011202      0.440406       0.004276      0.007737  5.881061e-01   \n",
       "25     0.019148      2.807823       0.027260      0.146637  7.418514e-02   \n",
       "26     0.097977      1.164217       0.011303      0.049125  4.253375e-01   \n",
       "27     0.271468      1.493141       0.014497      0.015860  5.144747e-08   \n",
       "28     0.408706     86.064410       0.835577      0.332269  6.105710e-17   \n",
       "29     0.003399      1.030003       0.010000      0.098045  2.686266e-01   \n",
       "30     0.021502      1.937806       0.018814      0.044757  9.003139e-01   \n",
       "31     0.048043      1.330904       0.012921      0.022741  5.743439e-04   \n",
       "32     0.020125      1.718908       0.016688      0.014620  6.359048e-01   \n",
       "33     0.007632      0.107733       0.001046      0.006083  7.591080e-01   \n",
       "34     0.001004      0.004255       0.000041      0.000417  6.503802e-01   \n",
       "35     0.062999     96.900394       0.940781      0.109638  5.475819e-01   \n",
       "\n",
       "         AUC  Wilcoxon_zscore  Wilcoxon_pvalue  MannWhitney_statistic  \\\n",
       "0   0.403218        -2.522887     1.163957e-02                 5274.5   \n",
       "1   0.347374         3.978629     6.931372e-05                 4544.0   \n",
       "2   0.653811        -4.009518     6.084289e-05                 4528.5   \n",
       "3   0.511199        -0.291945     7.703283e-01                 6394.0   \n",
       "4   0.500879        -0.022917     9.817163e-01                 6529.0   \n",
       "5   0.422139         2.029669     4.239021e-02                 5522.0   \n",
       "6   0.700673         5.231105     1.684994e-07                 3915.5   \n",
       "7   0.686301        -4.854466     1.207117e-06                 4104.5   \n",
       "8   0.381011         3.101796     1.923502e-03                 4984.0   \n",
       "9   0.505695         0.148464     8.819768e-01                 6466.0   \n",
       "10  0.488457        -0.300913     7.634808e-01                 6389.5   \n",
       "11  0.329600         4.445941     8.750782e-06                 4309.5   \n",
       "12  0.616467         3.036034     2.397124e-03                 5017.0   \n",
       "13  0.732436         6.059115     1.368728e-09                 3500.0   \n",
       "14  0.240081        -6.775527     1.239536e-11                 3140.5   \n",
       "15  0.689626         4.943145     7.687208e-07                 4060.0   \n",
       "16  0.464185         0.933628     3.504959e-01                 6072.0   \n",
       "17  0.639248         3.625903     2.879531e-04                 4721.0   \n",
       "18  0.442665         1.494602     1.350185e-01                 5790.5   \n",
       "19  0.559399         1.548407     1.215243e-01                 5763.5   \n",
       "20  0.831206         8.633815     5.933901e-18                 2208.0   \n",
       "21  0.886821       -10.083578     6.530362e-24                 1480.5   \n",
       "22  0.484023         0.416496     6.770474e-01                 6331.5   \n",
       "23  0.501567         0.040852     9.674135e-01                 6520.0   \n",
       "24  0.391981        -2.815829     4.865151e-03                 5127.5   \n",
       "25  0.519838        -0.517132     6.050639e-01                 6281.0   \n",
       "26  0.602248        -2.665373     7.690303e-03                 5203.0   \n",
       "27  0.528362         0.739330     4.597069e-01                 6169.5   \n",
       "28  0.813890        -8.182445     2.781413e-16                 2434.5   \n",
       "29  0.484596        -0.401550     6.880155e-01                 6339.0   \n",
       "30  0.585506         1.447771     1.476812e-01                 5814.0   \n",
       "31  0.335869         4.278546     1.881181e-05                 4393.5   \n",
       "32  0.445532        -1.419871     1.556451e-01                 5828.0   \n",
       "33  0.495184        -0.125547     9.000909e-01                 6477.5   \n",
       "34  0.500879        -0.022917     9.817163e-01                 6529.0   \n",
       "35  0.361517        -3.609961     3.062432e-04                 4729.0   \n",
       "\n",
       "    MannWhitney_pvalue  \n",
       "0         5.836077e-03  \n",
       "1         3.479935e-05  \n",
       "2         1.431124e-05  \n",
       "3         2.483123e-01  \n",
       "4         4.457874e-01  \n",
       "5         1.180169e-03  \n",
       "6         8.178827e-08  \n",
       "7         6.063551e-07  \n",
       "8         9.646592e-04  \n",
       "9         3.975669e-01  \n",
       "10        1.386924e-01  \n",
       "11        1.435113e-06  \n",
       "12        1.201852e-03  \n",
       "13        6.512661e-10  \n",
       "14        6.240471e-12  \n",
       "15        3.857922e-07  \n",
       "16        2.817489e-02  \n",
       "17        1.445188e-04  \n",
       "18        4.815483e-02  \n",
       "19        2.506618e-02  \n",
       "20        2.969660e-18  \n",
       "21        3.203071e-24  \n",
       "22        3.366042e-01  \n",
       "23        4.705984e-01  \n",
       "24        2.576184e-04  \n",
       "25        2.044395e-01  \n",
       "26        1.544964e-03  \n",
       "27        2.253108e-01  \n",
       "28        4.354174e-17  \n",
       "29        5.640681e-02  \n",
       "30        7.188721e-02  \n",
       "31        5.780181e-06  \n",
       "32        7.683629e-02  \n",
       "33        3.732689e-01  \n",
       "34        4.457874e-01  \n",
       "35        1.536876e-04  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f9a640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features_baseOn_pValue(tstat_dataset, pval=0.05): \n",
    "    tsdf = tstat_dataset[(tstat_dataset['P_Value']<pval)] \n",
    "    return tsdf \n",
    "\n",
    "def get_top_features_baseOn_AUC(tstat_dataset, auc=0.5): \n",
    "    tsdf = tstat_dataset[(tstat_dataset['AUC']>auc)] \n",
    "    return tsdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "012c9d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_featute_names_baseOn_pValue(tstat_dataset, pval=0.05): \n",
    "    top_feat_df = get_top_features_baseOn_pValue(tstat_dataset, pval=pval)\n",
    "    top_feat_df\n",
    "    return top_feat_df['Features'].values.tolist() \n",
    "\n",
    "def get_top_featute_names_baseOn_AUC(tstat_dataset, auc=0.5): \n",
    "    top_feat_df = get_top_features_baseOn_AUC(tstat_dataset, auc=auc)\n",
    "    top_feat_df\n",
    "    return top_feat_df['Features'].values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12b1d87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,\n",
       " ['W->W',\n",
       "  'W->S1',\n",
       "  'W->S2',\n",
       "  'W->REM',\n",
       "  'S1->W',\n",
       "  'S1->S1',\n",
       "  'S1->S2',\n",
       "  'S1->REM',\n",
       "  'S2->S3',\n",
       "  'S3->S2',\n",
       "  'S3->S3',\n",
       "  'S3->S4',\n",
       "  'S4->S3',\n",
       "  'S4->S4',\n",
       "  'REM->S1'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feats_p = get_top_featute_names_baseOn_pValue(stat_dataset, pval=0.05)\n",
    "len(top_feats_p), top_feats_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba31c642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " ['W->S2',\n",
       "  'W->S3',\n",
       "  'W->S4',\n",
       "  'S1->W',\n",
       "  'S1->S1',\n",
       "  'S1->S3',\n",
       "  'S2->W',\n",
       "  'S2->S1',\n",
       "  'S2->S3',\n",
       "  'S2->REM',\n",
       "  'S3->S1',\n",
       "  'S3->S2',\n",
       "  'S3->S3',\n",
       "  'S3->REM',\n",
       "  'S4->S1',\n",
       "  'S4->S2',\n",
       "  'S4->S3',\n",
       "  'S4->S4',\n",
       "  'REM->W',\n",
       "  'REM->S4'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feats_a = get_top_featute_names_baseOn_AUC(stat_dataset, auc=0.5)\n",
    "len(top_feats_a), top_feats_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3fc5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['Dataset'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c7583d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subject_Name</th>\n",
       "      <th>W-&gt;W-&gt;W</th>\n",
       "      <th>W-&gt;W-&gt;S1</th>\n",
       "      <th>W-&gt;W-&gt;S2</th>\n",
       "      <th>W-&gt;W-&gt;S3</th>\n",
       "      <th>W-&gt;W-&gt;S4</th>\n",
       "      <th>W-&gt;W-&gt;REM</th>\n",
       "      <th>W-&gt;S1-&gt;W</th>\n",
       "      <th>...</th>\n",
       "      <th>REM-&gt;S4-&gt;S2</th>\n",
       "      <th>REM-&gt;S4-&gt;S3</th>\n",
       "      <th>REM-&gt;S4-&gt;S4</th>\n",
       "      <th>REM-&gt;S4-&gt;REM</th>\n",
       "      <th>REM-&gt;REM-&gt;W</th>\n",
       "      <th>REM-&gt;REM-&gt;S1</th>\n",
       "      <th>REM-&gt;REM-&gt;S2</th>\n",
       "      <th>REM-&gt;REM-&gt;S3</th>\n",
       "      <th>REM-&gt;REM-&gt;S4</th>\n",
       "      <th>REM-&gt;REM-&gt;REM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux1</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.192771</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.965318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux2</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.126214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb1</td>\n",
       "      <td>0.879581</td>\n",
       "      <td>0.120419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.961165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb2</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.973684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb3</td>\n",
       "      <td>0.934673</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>0.015075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7191</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020725</td>\n",
       "      <td>0.015544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.963731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7201</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7211</td>\n",
       "      <td>0.940789</td>\n",
       "      <td>0.059211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.953020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7221</td>\n",
       "      <td>0.926230</td>\n",
       "      <td>0.073770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.935185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7241</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>0.008811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dataset Category Subject_Name   W->W->W  W->W->S1  W->W->S2  W->W->S3  \\\n",
       "0     CAP_Sleep     brux        brux1  0.795181  0.192771  0.012048       0.0   \n",
       "1     CAP_Sleep     brux        brux2  0.873786  0.126214  0.000000       0.0   \n",
       "2     CAP_Sleep      sdb         sdb1  0.879581  0.120419  0.000000       0.0   \n",
       "3     CAP_Sleep      sdb         sdb2  0.959184  0.040816  0.000000       0.0   \n",
       "4     CAP_Sleep      sdb         sdb3  0.934673  0.050251  0.015075       0.0   \n",
       "..          ...      ...          ...       ...       ...       ...       ...   \n",
       "225  Sleep_EDFX        n       ST7191  0.862745  0.137255  0.000000       0.0   \n",
       "226  Sleep_EDFX        n       ST7201  0.400000  0.600000  0.000000       0.0   \n",
       "227  Sleep_EDFX        n       ST7211  0.940789  0.059211  0.000000       0.0   \n",
       "228  Sleep_EDFX        n       ST7221  0.926230  0.073770  0.000000       0.0   \n",
       "229  Sleep_EDFX        n       ST7241  0.705882  0.294118  0.000000       0.0   \n",
       "\n",
       "     W->W->S4  W->W->REM  W->S1->W  ...  REM->S4->S2  REM->S4->S3  \\\n",
       "0         0.0        0.0  0.100000  ...          0.0          0.0   \n",
       "1         0.0        0.0  0.045455  ...          0.0          0.0   \n",
       "2         0.0        0.0  0.148148  ...          0.0          0.0   \n",
       "3         0.0        0.0  0.142857  ...          0.0          0.0   \n",
       "4         0.0        0.0  0.100000  ...          0.0          0.0   \n",
       "..        ...        ...       ...  ...          ...          ...   \n",
       "225       0.0        0.0  0.058824  ...          0.0          0.0   \n",
       "226       0.0        0.0  0.000000  ...          0.0          0.0   \n",
       "227       0.0        0.0  0.176471  ...          0.0          0.0   \n",
       "228       0.0        0.0  0.037037  ...          0.0          0.0   \n",
       "229       0.0        0.0  0.000000  ...          0.0          0.0   \n",
       "\n",
       "     REM->S4->S4  REM->S4->REM  REM->REM->W  REM->REM->S1  REM->REM->S2  \\\n",
       "0            0.0           0.0     0.034682      0.000000      0.000000   \n",
       "1            0.0           0.0     0.019802      0.004950      0.000000   \n",
       "2            0.0           0.0     0.009709      0.000000      0.029126   \n",
       "3            0.0           0.0     0.000000      0.000000      0.026316   \n",
       "4            0.0           0.0     0.066667      0.000000      0.000000   \n",
       "..           ...           ...          ...           ...           ...   \n",
       "225          0.0           0.0     0.000000      0.020725      0.015544   \n",
       "226          0.0           0.0     0.048000      0.008000      0.016000   \n",
       "227          0.0           0.0     0.026846      0.000000      0.020134   \n",
       "228          0.0           0.0     0.023148      0.041667      0.000000   \n",
       "229          0.0           0.0     0.017621      0.017621      0.008811   \n",
       "\n",
       "     REM->REM->S3  REM->REM->S4  REM->REM->REM  \n",
       "0             0.0           0.0       0.965318  \n",
       "1             0.0           0.0       0.975248  \n",
       "2             0.0           0.0       0.961165  \n",
       "3             0.0           0.0       0.973684  \n",
       "4             0.0           0.0       0.933333  \n",
       "..            ...           ...            ...  \n",
       "225           0.0           0.0       0.963731  \n",
       "226           0.0           0.0       0.928000  \n",
       "227           0.0           0.0       0.953020  \n",
       "228           0.0           0.0       0.935185  \n",
       "229           0.0           0.0       0.955947  \n",
       "\n",
       "[230 rows x 219 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset[~(dataset['Subject_Name'].str.startswith('SC'))]\n",
    "# dataset[~(dataset['Dataset']=='Sleep_EDFX')]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d63805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "458f8949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Annot_Proba_Transition3.csv\n",
      "./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Annot_Proba_Transition3_STAT_bin.csv\n"
     ]
    }
   ],
   "source": [
    "print( f\"{data_directory}/{result_subdirectory}/{annot_type.capitalize()}_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv\" ) \n",
    "print( f\"{data_directory}/{result_subdirectory}/{annot_type.capitalize()}_{tran_mat_type[mat_info_type]}_Transition{tran_step}_STAT_bin.csv\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c511aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9ea25d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_name</th>\n",
       "      <th>stp_from</th>\n",
       "      <th>exp_description</th>\n",
       "      <th>datasets</th>\n",
       "      <th>feature_selection</th>\n",
       "      <th>special_consideration</th>\n",
       "      <th>classification_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ML0011</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>2 datasets- CAP_Sleep, Sleep_EDFX</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, subject balanced over the fol...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ML0012</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>2 datasets- CAP_Sleep, Sleep_EDFX</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, subject balanced over the fol...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ML0013</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>2 datasets- CAP_Sleep, Sleep_EDFX</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, subject balanced over the fol...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ML10001</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>2 datasets- All</td>\n",
       "      <td>AUC&gt;0.5</td>\n",
       "      <td>Remove all zero transition and W-&gt;W stage, sub...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ML10002</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>2 datasets- All</td>\n",
       "      <td>AUC&gt;0.5</td>\n",
       "      <td>Remove all zero, subject balanced over the fol...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>ML6005</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>3 datasets- All</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, AUC&gt;0.5, subject balanced ove...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>ML6006</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>3 datasets- All</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, AUC&gt;0.6, subject balanced ove...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>ML6007</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>3 datasets- All</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, pval&lt;=0.05, subject balanced ...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>ML6008</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>3 datasets- All</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, AUC&gt;0.5, subject balanced ove...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>ML6009</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>3 datasets- All</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, AUC&gt;0.6, subject balanced ove...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    exp_name stp_from                                    exp_description  \\\n",
       "0     ML0011     Same  Data classification using sleep transition mat...   \n",
       "1     ML0012     Same  Data classification using sleep transition mat...   \n",
       "2     ML0013     Same  Data classification using sleep transition mat...   \n",
       "3    ML10001     Same  Data classification using sleep transition mat...   \n",
       "4    ML10002     Same  Data classification using sleep transition mat...   \n",
       "..       ...      ...                                                ...   \n",
       "104   ML6005     Same  Data classification using sleep transition mat...   \n",
       "105   ML6006     Same  Data classification using sleep transition mat...   \n",
       "106   ML6007     Same  Data classification using sleep transition mat...   \n",
       "107   ML6008     Same  Data classification using sleep transition mat...   \n",
       "108   ML6009     Same  Data classification using sleep transition mat...   \n",
       "\n",
       "                              datasets     feature_selection  \\\n",
       "0    2 datasets- CAP_Sleep, Sleep_EDFX  No feature selection   \n",
       "1    2 datasets- CAP_Sleep, Sleep_EDFX  No feature selection   \n",
       "2    2 datasets- CAP_Sleep, Sleep_EDFX  No feature selection   \n",
       "3                      2 datasets- All               AUC>0.5   \n",
       "4                      2 datasets- All               AUC>0.5   \n",
       "..                                 ...                   ...   \n",
       "104                    3 datasets- All  No feature selection   \n",
       "105                    3 datasets- All  No feature selection   \n",
       "106                    3 datasets- All  No feature selection   \n",
       "107                    3 datasets- All  No feature selection   \n",
       "108                    3 datasets- All  No feature selection   \n",
       "\n",
       "                                 special_consideration  \\\n",
       "0    Remove all zero, subject balanced over the fol...   \n",
       "1    Remove all zero, subject balanced over the fol...   \n",
       "2    Remove all zero, subject balanced over the fol...   \n",
       "3    Remove all zero transition and W->W stage, sub...   \n",
       "4    Remove all zero, subject balanced over the fol...   \n",
       "..                                                 ...   \n",
       "104  Remove all zero, AUC>0.5, subject balanced ove...   \n",
       "105  Remove all zero, AUC>0.6, subject balanced ove...   \n",
       "106  Remove all zero, pval<=0.05, subject balanced ...   \n",
       "107  Remove all zero, AUC>0.5, subject balanced ove...   \n",
       "108  Remove all zero, AUC>0.6, subject balanced ove...   \n",
       "\n",
       "                              classification_type  \n",
       "0    Binary classification- Healthy vs disordered  \n",
       "1    Binary classification- Healthy vs disordered  \n",
       "2    Binary classification- Healthy vs disordered  \n",
       "3    Binary classification- Healthy vs disordered  \n",
       "4    Binary classification- Healthy vs disordered  \n",
       "..                                            ...  \n",
       "104  Binary classification- Healthy vs disordered  \n",
       "105  Binary classification- Healthy vs disordered  \n",
       "106  Binary classification- Healthy vs disordered  \n",
       "107  Binary classification- Healthy vs disordered  \n",
       "108  Binary classification- Healthy vs disordered  \n",
       "\n",
       "[109 rows x 7 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_sum_df = modify_experiment_information_summarry(result_directory) \n",
    "exp_sum_df \n",
    "\n",
    "# exp_sum_df = modify_experiment_information_summarry(result_directory, dict_dat=exp_detail) \n",
    "# exp_sum_df \n",
    "\n",
    "# exp_sum_df = modify_experiment_information_summarry(result_directory) \n",
    "# exp_sum_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "295af0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'logger' is not defined\n"
     ]
    }
   ],
   "source": [
    "# pd.read_csv?\n",
    "# exp_detail\n",
    "# exp_name\n",
    "try:\n",
    "    stop_logger(logger)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    logger = None\n",
    "    \n",
    "# logger = None\n",
    "\n",
    "# del logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7977a7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tran_type\n",
    "tran_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "73a8ba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists at path: ./Results//_Classification/ML11002/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./Results//_Classification/ML11002/'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'ML1301'\n",
    "# exp_name = 'ML11002'\n",
    "#>1000 & <1500-Annot,Same stage, no feat sels \n",
    "# >1500 & <2000-Annot,Same stage, feat sels \n",
    "# >2000 & <2500-Annot,All stage, no feat sels \n",
    "# >2500 & <3000-Annot,All stage, feat sels \n",
    "# >3000 & <3500-Annot,All stage, no feat sels, TrimW \n",
    "# >3500 & <4000-Annot,All stage, feat sels, TrimW \n",
    "tran_type = 'All' if prob_cal_from_all==1 else 'Same' #\"combined\" # 'All' if prob_cal_from_all==1 else 'Same'\n",
    "exp_detail = {'exp_name':exp_name, 'stp_from':tran_type.capitalize(), \n",
    "              'exp_description':f\"Data classification using sleep transition matrix for {tran_type} sleep stages for {tran_step} stage trans, with TrimW and transition same transition\", \n",
    "              'datasets':'3 datasets- CAP, EDFX, SDRC', 'feature_selection':'pre selection P-value<0.05', \n",
    "              'special_consideration':'Remove all zero transition and W->W stage, subject balanced over the folds, validated with training data, All data n vs dis', \n",
    "              'classification_type':'Binary classification- Healthy vs disordered'}\n",
    "#               'exp_description':f\"Data classification using sleep transition matrix for {tran_type} sleep stages, with TrimW and transition same transition\", \n",
    "#               'datasets':'2 datasets- CAP_Sleep, Sleep_EDFX', 'feature_selection':'No feature selection', \n",
    "#               'special_consideration':'Remove all zero, feature values with <5% transition, healthy-healthy correlated features and W->W, subject balanced over the folds, validated with training data, TESING', \n",
    "#               'classification_type':'Binary classification- Healthy vs disordered'}\n",
    "# exp_detail = {'exp_name':exp_name, 'stp_from':'All' if prob_cal_from_all==1 else 'Same', \n",
    "#               'exp_description':f\"Data classification using sleep transition matrix for {'all' if prob_cal_from_all==1 else 'same'} sleep stages, with TrimW\", \n",
    "#               'datasets':'2 datasets- CAP_Sleep, Sleep_EDFX, Only healthy subjects', 'feature_selection':'No feature selection', \n",
    "#               'special_consideration':'W->W and theoritically infeasible transitions, Remove all zero, subject balanced over the folds, validated with training data', \n",
    "#               'classification_type':'Binary classification- Age group <=35 and >=55'}\n",
    "# exp_detail = {'exp_name':exp_name, 'stp_from':'All' if prob_cal_from_all==1 else 'Same', \n",
    "#               'exp_description':f\"Data classification using sleep transition matrix for {'all' if prob_cal_from_all==1 else 'same'} sleep stages, with TrimW\", \n",
    "#               'datasets':'2 datasets- CAP_Sleep, Sleep_EDFX', 'feature_selection':'No feature selection', \n",
    "#               'special_consideration':'Remove all zero, subject balanced over the folds, validated with training data, TESING', \n",
    "#               'classification_type':'Binary classification- Healthy vs disordered'}\n",
    "\n",
    "# exp_detail = {'exp_name':exp_name, 'stp_from':'All' if prob_cal_from_all==1 else 'Same', \n",
    "#               'exp_description':f\"Data classification using sleep transition matrix for {'all' if prob_cal_from_all==1 else 'same'} sleep stages, with TrimW\", \n",
    "#               'datasets':'2 datasets- CAP_Sleep, Sleep_EDFX', 'feature_selection':'No feature selection', \n",
    "#               'special_consideration':'Remove all zero, W->W and theoritically infeasible transitions, subject balanced over the folds, validated with training data', \n",
    "#               'classification_type':'Binary classification- Healthy vs disordered'}\n",
    "# exp_detail = {'exp_name':exp_name, 'exp_description':'Data classification using sleep transition matrix for all sleep stages', 'datasets':'2 datasets- CAP_Sleep, Sleep_EDFX', \n",
    "#               'feature_selection':'No feature selection', 'special_consideration':'No special consideration', \n",
    "#               'classification_type':\"Multi-class classification: Healthy and 7 different disordered in one vs all fashion | removed: 'brux', 'sdb', 'narco', 'ins', 'plm'\"}\n",
    "# exp_detail = {'exp_name':exp_name, 'exp_description':'Wake vs sleep binary classification using sleep transition matrix', 'datasets':'2 datasets- CAP_Sleep, Sleep_EDFX', \n",
    "#               'feature_selection':'AUC based training set only feature selection with AUC>=0.7', 'special_consideration':'W->W stage transition removed, subject balanced over the folds', \n",
    "#               'classification_type':'Binary classification: Healthy vs disordered'} \n",
    "# exp_detail = {'exp_name':exp_name, 'exp_description':'Wake vs sleep binary classification using sleep transition matrix, with TrimW', 'datasets':'1 datasets- CAP_Sleep', \n",
    "#               'feature_selection':'No feature selection | RF based predetermined features after experiment with RF importence>0.02 | AUC based predetermined features after experiment with AUC>0.7', 'special_consideration':'No special consideration, nxx balanced over the folds', 'classification_type':'Binary classification: Healthy vs disordered'} \n",
    "# exp_detail = {'exp_name':exp_name, 'exp_description':'Data classification using sleep transition matrix for all sleep stages', 'datasets':'2 datasets- CAP_Sleep, Sleep_EDFX (Excluding SC records)', \n",
    "#               'feature_selection':'AUC based >0.7', 'special_consideration':'Remove W->W and all zero transitions, nxx balanced over the folds', 'classification_type':'Binary classification: Healthy vs disordered'} \n",
    "result_save_path = create_experiment_directory(result_directory, exp_name) \n",
    "result_save_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "108f6d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<ML_Classifiers.SVC: 'support_vector_classifier'>,\n",
       " <ML_Classifiers.RF: 'random_forest'>,\n",
       " <ML_Classifiers.XGBoost: 'xtreme_gradient_boosting'>)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logger.info(\"Hello\")\n",
    "ML_Classifiers.SVC, ML_Classifiers.RF, ML_Classifiers.XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "dd2ba58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Class',\n",
       " ['Dataset', 'Category', 'Subject_Name'],\n",
       " 'Subject_Name',\n",
       " ['Dataset', 'Category', 'Subject_Name', 'Class'],\n",
       " './Results//_Classification/ML11002/')"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state_value = 312\n",
    "class_name = \"Class\" \n",
    "metadata_column = [\"Dataset\", \"Category\", \"Subject_Name\"] \n",
    "all_metadata_columns = metadata_column+[class_name]\n",
    "# ### #Binary/Multi-class healthy vs disorders \n",
    "split_column = \"Subject_Name\"  #\"Subject_Name\" for binary or multi-class \n",
    "split_balance_pattern = [['n'], ['SC', 'ST'], ['brux'], ['sdb'], ['ins'], ['narco'], ['nfle'], ['plm'], ['rbd']] #[['n'], ['SC', 'ST']]  for binary || [['n'], ['SC', 'ST'], ['brux'], ['sdb'], ['ins'], ['narco'], ['nfle'], ['plm'], ['rbd']] for multi-class \n",
    "# split_balance_pattern = [['n'], ['ST'], ['brux'], ['sdb'], ['ins'], ['narco'], ['nfle'], ['plm'], ['rbd']] #[['n'], ['SC', 'ST']]  for binary || [['n'], ['SC', 'ST'], ['brux'], ['sdb'], ['ins'], ['narco'], ['nfle'], ['plm'], ['rbd']] for multi-class \n",
    "# split_balance_pattern = [['n'], ['brux'], ['sdb'], ['ins'], ['narco'], ['nfle'], ['plm'], ['rbd']] #[['n'], ['SC', 'ST']]  for binary || [['n'], ['SC', 'ST'], ['brux'], ['sdb'], ['ins'], ['narco'], ['nfle'], ['plm'], ['rbd']] for multi-class \n",
    "### #Binary/Multi-class age-group detection  \n",
    "# split_column = class_name\n",
    "# split_balance_pattern = [[1]]\n",
    "class_name, metadata_column, split_column, all_metadata_columns, result_save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7d0edd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 ['W->W', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n"
     ]
    }
   ],
   "source": [
    "#### 2 step transition feature selection \n",
    "# dataset\n",
    "# OLD ['S3->S3','S4->S4','S3->S2','W->S1','W->S2','S2->S2','REM->REM','S4->S3','S3->S4','S2->S1','S2->S3','S2->W']\n",
    "\n",
    "### From 1031 \n",
    "# AUC_0_7 = ['S3->S3','W->S1','S3->S4','S3->S2','S4->S4','S2->S2','S4->S3','W->S2','S2->S3','S1->W','S2->S1'] \n",
    "# AUC_0_5 = ['S3->S3','W->S1','S3->S4','S3->S2','S4->S4','S2->S2','S4->S3','W->S2','S2->S3','S1->W','S2->S1','S3->W','S1->REM','S1->S1','REM->REM','S2->REM','S4->S2','REM->W',\n",
    "#            'REM->S1','S2->W','S1->S2','S4->W','S3->S1','W->S3','W->REM','S2->S4','S3->REM']\n",
    "# RF_FI_0_2 = ['S3->S3','S4->S4','S3->S2','W->S1','S2->S2','W->S2','S4->S3','S2->S1','S3->S4','S1->S1','S2->S3','REM->REM','S2->W','S2->REM','REM->W'] \n",
    "\n",
    "# ### Overall AUC\n",
    "# AUC_0_7 = ['W->W', 'S1->W', 'S1->REM', 'S2->S1', 'S2->S2', 'S3->S3', 'S4->S4']\n",
    "# AUC_0_5 = ['W->W', 'W->S2', 'S1->W', 'S1->REM', 'S2->S1', 'S2->S2', 'S2->S4', 'S2->REM', 'S3->S2', 'S3->S3', 'S4->W', 'S4->S2', 'S4->S3', 'S4->S4', 'REM->S2']\n",
    "\n",
    "### From 1135  \n",
    "AUC_0_7 = ['S3->S3', 'S4->S4', 'S2->S2', 'S4->S3', 'REM->REM', 'S3->S4', 'S2->REM', 'S3->S2', 'S2->S1', 'W->S2', 'S1->REM'] \n",
    "AUC_0_5 = ['S3->S3', 'S4->S4', 'S2->S2', 'S4->S3', 'REM->REM', 'S3->S4', 'S2->REM', 'S3->S2', 'S2->S1', 'W->S2', 'S1->REM', 'S4->S2', 'S1->W', 'S4->W', 'REM->S2', \n",
    "           'S2->W', 'S2->S3', 'REM->S1', 'S3->W', 'S2->S4', 'S1->S1', 'W->S3'] \n",
    "RF_FI_0_2 = ['S3->S3','S4->S4','S2->S2','REM->REM','S3->S2','S3->S4','S2->S1','S1->REM','S4->S3','S1->S2','S2->S3','W->S2','W->S1','S4->S2','S2->REM','REM->S2','S1->W','S1->S1','S4->W'] \n",
    "\n",
    "theoritically_feasible_transitions = ['W->S1', 'W->S2', 'S1->W', 'S1->S1', 'S1->S2', 'S2->W', 'S2->S1', 'S2->S2', 'S2->S3', 'S2->S4', 'S2->REM', 'S3->W', 'S3->S2', 'S3->S3', 'S3->S4',\n",
    "                                     'S4->W', 'S4->S2', 'S4->S3', 'S4->S4', 'REM->W', 'REM->S2', 'REM->REM'] \n",
    "feats_cant_seperate_both_healthy = ['W->W', 'W->S1', 'W->REM', 'S1->W', 'S1->S1', 'S1->REM', 'S2->W', 'S2->S1', 'S2->S2', 'S2->S3', 'S2->REM', 'S3->S2', 'S3->S3', 'S4->S2', 'S4->S4', \n",
    "                                    'REM->W', 'REM->S1', 'REM->S2', 'REM->REM']\n",
    "# feats_cant_seperate_both_healthy2 = ['W->REM', 'S1->W', 'S1->S2', 'S2->S1', 'S2->REM', 'S3->S2', 'S3->S4', 'S4->S1', 'S4->S2', 'REM->W', 'REM->S1', 'REM->S2']\n",
    "\n",
    "theoritically_infeasible_transitions = ['W->S3', 'W->S4', 'W->REM', 'S1->S3', 'S1->S4', 'S1->REM', 'S3->S1', 'S3->REM', 'S4->S1', 'S4->REM', 'REM->S1', 'REM->S3', 'REM->S4'] \n",
    "\n",
    "len(AUC_0_7),len(AUC_0_5),len(RF_FI_0_2) \n",
    "\n",
    "\n",
    "## For PhS Study01: P-value<0.05 based feature #15 ['W->W', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
    "## For PhS Study01: AUC>0.5 based feature #20 ['W->S2', 'W->S3', 'W->S4', 'S1->W', 'S1->S1', 'S1->S3', 'S2->W', 'S2->S1', 'S2->S3', 'S2->REM', 'S3->S1', 'S3->S2', 'S3->S3', 'S3->REM', 'S4->S1', 'S4->S2', 'S4->S3', 'S4->S4', 'REM->W', 'REM->S4']\n",
    "## For PhS Study01: AUC>0.6 based feature #11 ['W->S2', 'S1->W', 'S1->S1', 'S2->W', 'S2->S1', 'S2->S3', 'S2->REM', 'S3->S2', 'S3->S3', 'S4->S2', 'S4->S4']\n",
    "## Features showing common characteristics among the healthy subjects from CAP and EDFX \n",
    "\n",
    "top_feats = get_top_featute_names_baseOn_pValue(stat_dataset, pval=0.05)\n",
    "# top_feats = get_top_featute_names_baseOn_AUC(stat_dataset, auc=0.6)\n",
    "print(len(top_feats), top_feats) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f687ef5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W->W',\n",
       " 'W->S1',\n",
       " 'S1->W',\n",
       " 'S1->S1',\n",
       " 'S1->S2',\n",
       " 'S2->S2',\n",
       " 'S3->S2',\n",
       " 'S3->S3',\n",
       " 'S4->S3',\n",
       " 'S4->S4',\n",
       " 'REM->REM']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature values greater than or equal to 1% of the transitions\n",
    "processed_dataset = dataset.copy()\n",
    "dd = ((processed_dataset.mean()/processed_dataset.mean().sum())>=0.01) \n",
    "dd = dd[dd==True]\n",
    "type(dd), dd.index.tolist(), dd\n",
    "take_feats = dd.index.tolist() \n",
    "take_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c1794700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: ['W->W']\n",
      "['n', 'brux', 'sdb', 'ins', 'narco', 'nfle', 'plm', 'rbd']\n",
      "['n', 'brux', 'sdb', 'ins', 'narco', 'nfle', 'plm', 'rbd']\n",
      "{'n': 0, 'brux': 1, 'sdb': 1, 'ins': 1, 'narco': 1, 'nfle': 1, 'plm': 1, 'rbd': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subject_Name</th>\n",
       "      <th>Class</th>\n",
       "      <th>W-&gt;S1</th>\n",
       "      <th>W-&gt;S2</th>\n",
       "      <th>W-&gt;REM</th>\n",
       "      <th>S1-&gt;W</th>\n",
       "      <th>S1-&gt;S1</th>\n",
       "      <th>S1-&gt;S2</th>\n",
       "      <th>S1-&gt;REM</th>\n",
       "      <th>S2-&gt;S3</th>\n",
       "      <th>S3-&gt;S2</th>\n",
       "      <th>S3-&gt;S3</th>\n",
       "      <th>S3-&gt;S4</th>\n",
       "      <th>S4-&gt;S3</th>\n",
       "      <th>S4-&gt;S4</th>\n",
       "      <th>REM-&gt;S1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014472</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>0.851485</td>\n",
       "      <td>0.059406</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.970443</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022822</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.924242</td>\n",
       "      <td>0.025253</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.984940</td>\n",
       "      <td>0.004831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.123853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099237</td>\n",
       "      <td>0.763359</td>\n",
       "      <td>0.137405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.055944</td>\n",
       "      <td>0.874126</td>\n",
       "      <td>0.069930</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.949239</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045161</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.926136</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004128</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.046948</td>\n",
       "      <td>0.018779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023656</td>\n",
       "      <td>0.024590</td>\n",
       "      <td>0.961749</td>\n",
       "      <td>0.013661</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.935065</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7191</td>\n",
       "      <td>0</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.014184</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7201</td>\n",
       "      <td>0</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.024590</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.007463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7211</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.053903</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7221</td>\n",
       "      <td>0</td>\n",
       "      <td>0.181208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052133</td>\n",
       "      <td>0.734597</td>\n",
       "      <td>0.161137</td>\n",
       "      <td>0.052133</td>\n",
       "      <td>0.004566</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7241</td>\n",
       "      <td>0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.023881</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.016878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dataset Category Subject_Name  Class     W->S1     W->S2    W->REM  \\\n",
       "0     CAP_Sleep     brux        brux1      1  0.192308  0.009615  0.000000   \n",
       "1     CAP_Sleep     brux        brux2      1  0.176000  0.000000  0.000000   \n",
       "2     CAP_Sleep      sdb         sdb1      1  0.123853  0.000000  0.000000   \n",
       "3     CAP_Sleep      sdb         sdb2      1  0.045161  0.006452  0.000000   \n",
       "4     CAP_Sleep      sdb         sdb3      1  0.046948  0.018779  0.000000   \n",
       "..          ...      ...          ...    ...       ...       ...       ...   \n",
       "225  Sleep_EDFX        n       ST7191      0  0.246377  0.014493  0.000000   \n",
       "226  Sleep_EDFX        n       ST7201      0  0.588235  0.058824  0.058824   \n",
       "227  Sleep_EDFX        n       ST7211      0  0.100000  0.000000  0.005882   \n",
       "228  Sleep_EDFX        n       ST7221      0  0.181208  0.000000  0.000000   \n",
       "229  Sleep_EDFX        n       ST7241      0  0.285714  0.035714  0.071429   \n",
       "\n",
       "        S1->W    S1->S1    S1->S2   S1->REM    S2->S3    S3->S2    S3->S3  \\\n",
       "0    0.030769  0.807692  0.161538  0.000000  0.014472  0.079208  0.851485   \n",
       "1    0.061728  0.851852  0.086420  0.000000  0.022822  0.045455  0.924242   \n",
       "2    0.099237  0.763359  0.137405  0.000000  0.050000  0.055944  0.874126   \n",
       "3    0.011364  0.926136  0.062500  0.000000  0.004128  0.026316  0.921053   \n",
       "4    0.108696  0.782609  0.108696  0.000000  0.023656  0.024590  0.961749   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "225  0.044118  0.558824  0.382353  0.014706  0.014184  0.040816  0.387755   \n",
       "226  0.014493  0.623188  0.333333  0.028986  0.024590  0.212121  0.530303   \n",
       "227  0.116667  0.483333  0.383333  0.016667  0.053903  0.239130  0.521739   \n",
       "228  0.052133  0.734597  0.161137  0.052133  0.004566  1.000000  0.000000   \n",
       "229  0.047619  0.523810  0.380952  0.047619  0.023881  0.368421  0.473684   \n",
       "\n",
       "       S3->S4    S4->S3    S4->S4   REM->S1  \n",
       "0    0.059406  0.024631  0.970443  0.000000  \n",
       "1    0.025253  0.012048  0.984940  0.004831  \n",
       "2    0.069930  0.030457  0.949239  0.000000  \n",
       "3    0.052632  0.016949  0.966102  0.000000  \n",
       "4    0.013661  0.038961  0.935065  0.000000  \n",
       "..        ...       ...       ...       ...  \n",
       "225  0.571429  0.166667  0.805556  0.020000  \n",
       "226  0.227273  0.615385  0.346154  0.007463  \n",
       "227  0.173913  0.254237  0.728814  0.000000  \n",
       "228  0.000000  0.000000  0.000000  0.043290  \n",
       "229  0.157895  0.571429  0.142857  0.016878  \n",
       "\n",
       "[230 rows x 18 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processed_dataset = dataset.copy() if not random_state_value else dataset.copy().sample(frac=1, random_state=random_state_value).reset_index(drop=True) \n",
    "processed_dataset = dataset.copy()\n",
    "all_cols = processed_dataset.columns.values.tolist() \n",
    "last_metadata_col_indx = all_cols.index('Subject_Name')+1 \n",
    "\n",
    "# ### Select some features based on dataset \n",
    "# processed_dataset = processed_dataset[~(processed_dataset['Subject_Name'].str.startswith('SC'))] #dataset[~(dataset['Subject_Name'].str.startswith('SC'))]\n",
    "# processed_dataset = processed_dataset[~(processed_dataset['Dataset']=='Sleep_EDFX')] #dataset[~(dataset['Subject_Name'].str.startswith('SC'))]\n",
    "\n",
    "### Select specific group of features from % of transitions\n",
    "# selected_feats = all_cols[:last_metadata_col_indx]+take_feats\n",
    "selected_feats = all_cols[:last_metadata_col_indx]+top_feats\n",
    "selected_feats \n",
    "processed_dataset = processed_dataset.loc[:, selected_feats]\n",
    "\n",
    "### Remove zero and wake-to-wake and specific group \n",
    "removable_feats = []\n",
    "zero_feats = get_features_with_zero_values(processed_dataset)\n",
    "zero_feats\n",
    "removable_feats.extend(zero_feats)\n",
    "removable_feats.extend(['W->W'])\n",
    "# removable_feats.extend(to_drop_corr)\n",
    "# removable_feats.extend(feats_cant_seperate_both_healthy) \n",
    "# removable_feats.extend(theoritically_infeasible_transitions) \n",
    "print(f'Removed: {removable_feats}')\n",
    "removable_feats \n",
    "processed_dataset = processed_dataset.drop(removable_feats, axis=1, errors='ignore')\n",
    "\n",
    "### Remove features with <5% transitions\n",
    "\n",
    "\n",
    "### Select specific group of features \n",
    "# # # processed_dataset = processed_dataset[all_metadata_columns+sorted_PAUC_df]\n",
    "# # processed_dataset = processed_dataset.drop(removable_feats, axis=1)\n",
    "# # # select features from manual feature selection list: from previous AUC or RF \n",
    "# selected_feats = all_cols[:last_metadata_col_indx]+theoritically_feasible_transitions\n",
    "# selected_feats = all_cols[:last_metadata_col_indx]+feats_cant_seperate_both_healthy\n",
    "# selected_feats \n",
    "# processed_dataset = processed_dataset.loc[:, selected_feats]\n",
    "\n",
    "### #Binary/Multi-class healthy vs disorders \n",
    "# processed_dataset = processed_dataset[~processed_dataset['Category'].isin(['brux', 'sdb'])]### Brux and sdb is cancelled coz of low number to fit in 5 fold\n",
    "label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=None, multi_class=False)\n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=None, multi_class=True)\n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=['brux', 'sdb', 'narco', 'ins', 'plm'], multi_class=True) \n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=['brux', 'sdb', 'narco', 'ins', 'plm'], multi_class=True) \n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=['brux', 'sdb', 'narco', 'ins', 'plm'], multi_class=True) \n",
    "\n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=None, multi_class=False, age_data=all_demography_detail_df.copy(), age_col = 'Age', source_cols=['Subject_Name', 'File_Name'])\n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=None, multi_class=False, age_data=None, age_col = 'Age', source_cols=['Subject_Name', 'File_Name'])\n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=['brux', 'sdb', 'narco', 'ins', 'plm', 'nfle'], multi_class=False, age_data=None, age_col = 'Age', source_cols=['Subject_Name', 'File_Name'])\n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=['brux', 'sdb', 'narco', 'ins', 'rbd', 'nfle'], multi_class=False, age_data=None, age_col = 'Age', source_cols=['Subject_Name', 'File_Name'])\n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=['brux', 'sdb', 'narco', 'ins', 'plm', 'rbd'], multi_class=False, age_data=None, age_col = 'Age', source_cols=['Subject_Name', 'File_Name'])\n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=['brux', 'sdb', 'narco', 'plm', 'rbd', 'nfle'], multi_class=False, age_data=None, age_col = 'Age', source_cols=['Subject_Name', 'File_Name'])\n",
    "\n",
    "\n",
    "print(label_map)\n",
    "# processed_dataset = processed_dataset[~processed_dataset['Subject_Name'].str.startswith('SC')] \n",
    "# processed_dataset = processed_dataset[~processed_dataset['Subject_Name'].str.startswith('ST')] \n",
    "# processed_dataset = processed_dataset[~(processed_dataset['Dataset']=='Sleep_EDFX')]\n",
    "# processed_dataset = processed_dataset[(processed_dataset['Dataset']=='SDRC')]\n",
    "# processed_dataset = processed_dataset[(processed_dataset['Dataset']=='CAP_Sleep')]\n",
    "\n",
    "# ### #Binary/Multi-class age-group detection  \n",
    "# label_map, processed_dataset = map_age_category_to_class(processed_dataset.copy(), all_demography_detail_df.copy(), source_cols=['Subject_Name', 'File_Name'], age_col ='Age', age_ranges = [[0, 30], [40, 200]], class_name='Class', multi_class=True, healthy_only=True) ##19-101 \n",
    "# label_map, processed_dataset = map_age_category_to_class(processed_dataset.copy(), all_demography_detail_df.copy(), source_cols=['Subject_Name', 'File_Name'], age_col ='Age', age_ranges = [[0, 30], [40, 60], [70, 200]], class_name='Class', multi_class=True, healthy_only=True) ##19-101 \n",
    "# label_map, processed_dataset = map_age_category_to_class(processed_dataset.copy(), all_demography_detail_df.copy(), source_cols=['Subject_Name', 'File_Name'], age_col ='Age', age_ranges = [[0, 35], [35, 55], [55, 200]], class_name='Class', multi_class=True, healthy_only=True) ##19-101  \n",
    "# label_map, processed_dataset = map_age_category_to_class(processed_dataset.copy(), all_demography_detail_df.copy(), source_cols=['Subject_Name', 'File_Name'], age_col ='Age', age_ranges = [[0, 35], [55, 200]], class_name='Class', multi_class=True, healthy_only=True) ##19-101 \n",
    "# print(label_map)\n",
    "# processed_dataset\n",
    "\n",
    "processed_dataset = processed_dataset.reset_index(drop=True) \n",
    "# processed_dataset = processed_dataset.reset_index() \n",
    "processed_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "089ad0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class      0.447826\n",
       "W->S1      0.108565\n",
       "W->S2      0.017716\n",
       "W->REM     0.002548\n",
       "S1->W      0.066288\n",
       "S1->S1     0.715062\n",
       "S1->S2     0.194041\n",
       "S1->REM    0.017830\n",
       "S2->S3     0.036480\n",
       "S3->S2     0.186136\n",
       "S3->S3     0.685885\n",
       "S3->S4     0.052712\n",
       "S4->S3     0.098201\n",
       "S4->S4     0.584832\n",
       "REM->S1    0.022889\n",
       "dtype: float64"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processed_dataset[~processed_dataset['Subject_Name'].str.startswith('SC')].groupby('Category')['Class'].value_counts() \n",
    "processed_dataset.mean()\n",
    "# processed_dataset.isna().sum()\n",
    "# label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "99100403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAP_Sleep' 'SDRC' 'Sleep_EDFX'] [1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Category  Class\n",
       "brux      1          2\n",
       "ins       1         20\n",
       "n         0        127\n",
       "narco     1          5\n",
       "nfle      1         40\n",
       "plm       1         10\n",
       "rbd       1         22\n",
       "sdb       1          4\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(processed_dataset['Dataset'].unique(), processed_dataset['Class'].unique())\n",
    "processed_dataset.groupby('Category')['Class'].value_counts()\n",
    "# processed_dataset['Category'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2f44e93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Object is initialised with the following properties: \n",
      "        ###################################################################################################\n",
      "        Dataset size: (230, 18), Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        Target class column name: Class\n",
      "        Metadata column names: ['Dataset', 'Category', 'Subject_Name']\n",
      "        Dataset split column on which the training and test sets will be devided: Subject_Name\n",
      "        Is multi-class classification: False\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.HumachLab_ML_CLassifiers at 0x1548f3e4048>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if logger:\n",
    "    stop_logger(logger) \n",
    "util, logger = start_logger(result_save_path, exp_name)\n",
    "\n",
    "# classifier_obj = HumachLab_ML_CLassifiers(logger=logger, directory=result_save_path, dataset=dataset.copy(), class_name=class_name, metadata_column=metadata_column, split_column=split_column) \n",
    "classifier_obj = HumachLab_ML_CLassifiers(logger=logger, directory=result_save_path, dataset=processed_dataset.copy(), class_name=class_name, label_map=label_map, metadata_column=metadata_column, split_column=split_column, random_state_value=random_state_value, split_balance_pattern=split_balance_pattern) \n",
    "\n",
    "classifier_obj \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6e0a8ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from HumachLab_ML_CLassifiers class\n"
     ]
    }
   ],
   "source": [
    "classifier_obj.print_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "1df9319e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(5, 0), (2, 5)],\n",
       " [<ML_Classifiers.LogReg: 'logistic_regression'>,\n",
       "  <ML_Classifiers.SVC: 'support_vector_classifier'>,\n",
       "  <ML_Classifiers.NB: 'naive_bayes'>,\n",
       "  <ML_Classifiers.kNN: 'k_nearest_neighbors'>,\n",
       "  <ML_Classifiers.DT: 'decision_tree'>,\n",
       "  <ML_Classifiers.RF: 'random_forest'>,\n",
       "  <ML_Classifiers.GBoost: 'gradient_boosting'>,\n",
       "  <ML_Classifiers.XGBoost: 'xtreme_gradient_boosting'>],\n",
       " './Results//_Classification/ML11002/',\n",
       " True)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting_crieteria = [(10, 0), (5, 20)]     ### for test & training (validation) splitting_crieteria (m, n)-m folds, n%:  m=0: loso, m>0: m-fold, (n>0 given m>0) -shuffled random splitting with m-fold, n% testing \n",
    "# splitting_crieteria = [(5, 0), (5, 20)]     ### for test & training (validation) splitting_crieteria (m, n)-m folds, n%:  m=0: loso, m>0: m-fold, (n>0 given m>0) -shuffled random splitting with m-fold, n% testing \n",
    "# splitting_crieteria = [(1, 0), (1, 20)]   ## for small data  ### for test & training (validation) splitting_crieteria (m, n)-m folds, n%:  m=0: loso, m>0: m-fold, (n>0 given m>0) -shuffled random splitting with m-fold, n% testing \n",
    "# splitting_crieteria = [(5, 0), (2, 10)]   ## for small data  ### for test & training (validation) splitting_crieteria (m, n)-m folds, n%:  m=0: loso, m>0: m-fold, (n>0 given m>0) -shuffled random splitting with m-fold, n% testing \n",
    "splitting_crieteria = [(5, 0), (2, 5)]   ## for small data  ### for test & training (validation) splitting_crieteria (m, n)-m folds, n%:  m=0: loso, m>0: m-fold, (n>0 given m>0) -shuffled random splitting with m-fold, n% testing \n",
    "model_list = [ML_Classifiers.LogReg, ML_Classifiers.SVC, ML_Classifiers.NB, ML_Classifiers.kNN, ML_Classifiers.DT, ML_Classifiers.RF, ML_Classifiers.GBoost, ML_Classifiers.XGBoost] # [ML_Classifiers.LogReg, ML_Classifiers.SVC, ML_Classifiers.NB, ML_Classifiers.kNN, ML_Classifiers.DT, ML_Classifiers.RF, ML_Classifiers.GBoost] \n",
    "should_use_params = True \n",
    "is_validate_models = True\n",
    "# is_binary_classification = False \n",
    "apply_feature_selection = False\n",
    "custom_splitter = True\n",
    "exp_name = exp_name\n",
    "\n",
    "splitting_crieteria, model_list, result_save_path, should_use_params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "6e678c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Class', 'Subject_Name', './Results//_Classification')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_obj.class_name, classifier_obj.split_column, result_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0c3ba474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Classification is set with the following parameters: \n",
      "        ###################################################################################################\n",
      "        Splitting crieteria: [(5, 0), (2, 5)]\n",
      "        Test split: 5-fold cross validation\n",
      "        Training split: 2-fold 5% random test splitting\n",
      "        List of ML models that will be applied: ['logistic_regression', 'support_vector_classifier', 'naive_bayes', 'k_nearest_neighbors', 'decision_tree', 'random_forest', 'gradient_boosting', 'xtreme_gradient_boosting']\n",
      "        Use parameters for model: True\n",
      "        Is validate the model (or only train): True \n",
      "        Classification results will be saved in the directory: ./Results//_Classification/ML11002/\n",
      "        \n",
      "Custom splitter testing...\n",
      "test- tsi, train_all_ids, test_ids, tr_all_dat, ts_dat 0 [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [128, 0, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 26, 6, 31, 32, 33, 34, 7, 35, 36, 37, 38, 8, 9, 2, 71, 72, 81, 82, 83, 84, 101, 103, 104, 105, 106, 107] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['n1110', 'brux1', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'narco1', 'ins1', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'ins2', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'ins3', 'ins4', 'sdb1', 'plm1', 'plm2', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5']\n",
      "\n",
      "            ### MODEL TEST PHASE \n",
      "            TEST 1 START... XXXXX \n",
      "            ===================================================================================================\n",
      "            Test=> 48 ['n1110', 'brux1', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'narco1', 'ins1', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'ins2', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'ins3', 'ins4', 'sdb1', 'plm1', 'plm2', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5'] \n",
      "            Training (Including Validation)=> 182 ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            \n",
      "Random 5 percentage splitting testing...\n",
      "train- tri, tmp_train_ids, tmp_val_ids, tr_dat, val_dat 0 [117 178   4  54  67  92 159  55  16 107  59 170 125 168  27  20 141  22\n",
      "   9 180  91  28  31   3 152   5  87 121 112 135 116 145  96  71 177 104\n",
      "  75  50  36  84  88  58 106  47  72 127  45  35  93  77  13 114 105 103\n",
      "  81 147 157 176 137 179 143  37  74  62  89 108  99 181  57 166  25 140\n",
      " 139 122 129 134  33  90 158 142  17 169  46 120   2  40  98 173  15  14\n",
      " 102  85  56  48 110  79 109   1 128 153 124 172 160  78 149  43   8 133\n",
      "  76 111 136  30 163 146  34 148 123 162  66 118 115  86  80 150 154  19\n",
      " 100 167  38 165 174 175 155  41  51  44  69  82  42 132 119   7  52  29\n",
      "  83  18 151 130  61 171  95  12 126  11  68  10  23 161 156 131  60  94\n",
      "  39  21   0  65  49  73  26 138   6  24  97 144  64  53  70 101 164  32\n",
      " 113  63] [117 178   4  54  67  92 159  55  16 107  59 170 125 168  27  20 141  22\n",
      "   9 180  91  28  31   3 152   5  87 121 112 135 116 145  96  71 177 104\n",
      "  75  50  36  84  88  58 106  47  72 127  45  35  93  77  13 114 105 103\n",
      "  81 147 157 176 137 179 143  37  74  62  89 108  99 181  57 166  25 140\n",
      " 139 122 129 134  33  90 158 142  17 169  46 120   2  40  98 173  15  14\n",
      " 102  85  56  48 110  79 109   1 128 153 124 172 160  78 149  43   8 133\n",
      "  76 111 136  30 163 146  34 148 123 162  66 118 115  86  80 150 154  19\n",
      " 100 167  38 165 174 175 155  41  51  44  69  82  42 132 119   7  52  29\n",
      "  83  18 151 130  61 171  95  12 126  11  68  10  23 161 156 131  60  94\n",
      "  39  21   0  65  49  73  26 138   6  24  97 144  64  53  70 101 164  32\n",
      " 113  63] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "            *** MODEL TRAINING AND VALIDATION PHASE FOR TEST 1 \n",
      "            TRAINING 1 START... XXXXX \n",
      "            ***************************************************************************************************\n",
      "            Validation=> 182 ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            Training=> 182 ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            \n",
      "\n",
      "            *** ML MODEL FOR TEST:1, TRAINING:1 AND MODEL: LR \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: logistic_regression_parameters\n",
      "All parameters: ['penalty', 'solver', 'max_iter', 'C'], [['l2', 'elasticnet'], [50, 100, 130, 150, 170, 200], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]], [0, 2, 3]\n",
      "Parameters:  {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\"\n",
      "            Selected features: ['W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "            \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: LR - {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 84 candidates, totalling 420 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[87 14]\n",
      " [10 71]]\n",
      "[[71 10 14 87]\n",
      " [87 14 10 71]]\n",
      "[[158  24]\n",
      " [ 24 158]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 86.81318681318682\n",
      " Precision = [0.83529412 0.89690722]\n",
      " Recall = [0.87654321 0.86138614]\n",
      " F1 score = [0.85542169 0.87878788]\n",
      " AUC score = 86.89646742452022\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 86.81318681318682\n",
      " Precision = [0.89690722 0.83529412]\n",
      " Recall = [0.86138614 0.87654321]\n",
      " F1 score = [0.87878788 0.85542169]\n",
      " AUC score = 86.89646742452022\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[87 14]\n",
      " [10 71]]\n",
      " Accuracy (acc): 86.813\n",
      " Precision (prc): 83.529\n",
      " Recall (rec): 87.654\n",
      " Sensitivity (sns): 87.654\n",
      " Specificity (spc): 86.139\n",
      " F1 Score (f1s): 85.542\n",
      " ROC AUC (AUC): 0.869\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: LogisticRegression(max_iter=50) \n",
      "        Best parameters of the model: {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
      "\n",
      "                             model  \\\n",
      "0  LogisticRegression(max_iter=50)   \n",
      "\n",
      "                              model_parameters  model_scores  \\\n",
      "0  {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'}         89.19   \n",
      "\n",
      "       confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
      "0  [[87, 14], [10, 71]]    86.813     83.529  87.654       87.654   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       86.139    85.542    0.869   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:1, TRAINING:1 AND MODEL: SVC \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: support_vector_classifier_parameters\n",
      "All parameters: ['probability', 'C', 'kernel', 'gamma', 'degree', 'class_weightdict'], [[True], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], ['linear', 'rbf', 'poly']], [0, 1, 2]\n",
      "Parameters:  {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']}\n",
      "run_model_gridSearch- val_ids, train_ids [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "210 fits failed out of a total of 420.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.84411765        nan 0.84411765        nan 0.84411765        nan\n",
      " 0.84411765        nan 0.84411765        nan 0.84411765        nan\n",
      " 0.85588235        nan 0.85588235        nan 0.85588235        nan\n",
      " 0.85588235        nan 0.85588235        nan 0.85588235        nan\n",
      " 0.86764706        nan 0.86764706        nan 0.86764706        nan\n",
      " 0.86764706        nan 0.86764706        nan 0.86764706        nan\n",
      " 0.89191176        nan 0.89191176        nan 0.89191176        nan\n",
      " 0.89191176        nan 0.89191176        nan 0.89191176        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.83980769        nan 0.83980769        nan 0.83980769        nan\n",
      " 0.83980769        nan 0.83980769        nan 0.83980769        nan\n",
      " 0.84288462        nan 0.84288462        nan 0.84288462        nan\n",
      " 0.84288462        nan 0.84288462        nan 0.84288462        nan\n",
      " 0.85831731        nan 0.85831731        nan 0.85831731        nan\n",
      " 0.85831731        nan 0.85831731        nan 0.85831731        nan\n",
      " 0.87370192        nan 0.87370192        nan 0.87370192        nan\n",
      " 0.87370192        nan 0.87370192        nan 0.87370192        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.84460476        nan 0.84460476        nan 0.84460476        nan\n",
      " 0.84460476        nan 0.84460476        nan 0.84460476        nan\n",
      " 0.843129          nan 0.843129          nan 0.843129          nan\n",
      " 0.843129          nan 0.843129          nan 0.843129          nan\n",
      " 0.84839581        nan 0.84839581        nan 0.84839581        nan\n",
      " 0.84839581        nan 0.84839581        nan 0.84839581        nan\n",
      " 0.8644052         nan 0.8644052         nan 0.8644052         nan\n",
      " 0.8644052         nan 0.8644052         nan 0.8644052         nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.84625639        nan 0.84625639        nan 0.84625639        nan\n",
      " 0.84625639        nan 0.84625639        nan 0.84625639        nan\n",
      " 0.83228376        nan 0.83228376        nan 0.83228376        nan\n",
      " 0.83228376        nan 0.83228376        nan 0.83228376        nan\n",
      " 0.84295991        nan 0.84295991        nan 0.84295991        nan\n",
      " 0.84295991        nan 0.84295991        nan 0.84295991        nan\n",
      " 0.85320781        nan 0.85320781        nan 0.85320781        nan\n",
      " 0.85320781        nan 0.85320781        nan 0.85320781        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: SVC - {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[87 14]\n",
      " [10 71]]\n",
      "[[71 10 14 87]\n",
      " [87 14 10 71]]\n",
      "[[158  24]\n",
      " [ 24 158]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 86.81318681318682\n",
      " Precision = [0.83529412 0.89690722]\n",
      " Recall = [0.87654321 0.86138614]\n",
      " F1 score = [0.85542169 0.87878788]\n",
      " AUC score = 86.89646742452022\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 86.81318681318682\n",
      " Precision = [0.89690722 0.83529412]\n",
      " Recall = [0.86138614 0.87654321]\n",
      " F1 score = [0.87878788 0.85542169]\n",
      " AUC score = 86.89646742452022\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[87 14]\n",
      " [10 71]]\n",
      " Accuracy (acc): 86.813\n",
      " Precision (prc): 83.529\n",
      " Recall (rec): 87.654\n",
      " Sensitivity (sns): 87.654\n",
      " Specificity (spc): 86.139\n",
      " F1 Score (f1s): 85.542\n",
      " ROC AUC (AUC): 0.869\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: SVC(C=0.5, kernel='linear', probability=True) \n",
      "        Best parameters of the model: {'C': 0.5, 'kernel': 'linear', 'probability': True} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
      "\n",
      "                                           model  \\\n",
      "0  SVC(C=0.5, kernel='linear', probability=True)   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'C': 0.5, 'kernel': 'linear', 'probability': ...         88.01   \n",
      "\n",
      "       confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
      "0  [[87, 14], [10, 71]]    86.813     83.529  87.654       87.654   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       86.139    85.542    0.869   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:1, TRAINING:1 AND MODEL: NB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: naive_bayes_parameters\n",
      "All parameters: ['var_smoothing'], [[1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]], [0]\n",
      "Parameters:  {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]}\n",
      "run_model_gridSearch- val_ids, train_ids [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: NB - {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[84 17]\n",
      " [ 5 76]]\n",
      "[[76  5 17 84]\n",
      " [84 17  5 76]]\n",
      "[[160  22]\n",
      " [ 22 160]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.91208791208791\n",
      " Precision = [0.8172043  0.94382022]\n",
      " Recall = [0.9382716  0.83168317]\n",
      " F1 score = [0.87356322 0.88421053]\n",
      " AUC score = 88.49773866275515\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.91208791208791\n",
      " Precision = [0.94382022 0.8172043 ]\n",
      " Recall = [0.83168317 0.9382716 ]\n",
      " F1 score = [0.88421053 0.87356322]\n",
      " AUC score = 88.49773866275515\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[84 17]\n",
      " [ 5 76]]\n",
      " Accuracy (acc): 87.912\n",
      " Precision (prc): 81.72\n",
      " Recall (rec): 93.827\n",
      " Sensitivity (sns): 93.827\n",
      " Specificity (spc): 83.168\n",
      " F1 Score (f1s): 87.356\n",
      " ROC AUC (AUC): 0.885\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GaussianNB(var_smoothing=0.005623413251903491) \n",
      "        Best parameters of the model: {'var_smoothing': 0.005623413251903491} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
      "\n",
      "                                            model  \\\n",
      "0  GaussianNB(var_smoothing=0.005623413251903491)   \n",
      "\n",
      "                          model_parameters  model_scores     confusion_matrix  \\\n",
      "0  {'var_smoothing': 0.005623413251903491}         88.09  [[84, 17], [5, 76]]   \n",
      "\n",
      "   accuracy  precision  recall  sensitivity  specificity  f1_score  roc_auc  \n",
      "0    87.912      81.72  93.827       93.827       83.168    87.356    0.885   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:1, TRAINING:1 AND MODEL: KNN \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: k_nearest_neighbors_parameters\n",
      "All parameters: ['n_neighbors', 'p', 'metric', 'n_splits'], [[2, 3, 5, 10, 15, 25, 35], ['manhattan', 'minkowski', 'euclidean']], [0, 2]\n",
      "Parameters:  {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']}\n",
      "run_model_gridSearch- val_ids, train_ids [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: KNN - {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[85 16]\n",
      " [11 70]]\n",
      "[[70 11 16 85]\n",
      " [85 16 11 70]]\n",
      "[[155  27]\n",
      " [ 27 155]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 85.16483516483517\n",
      " Precision = [0.81395349 0.88541667]\n",
      " Recall = [0.86419753 0.84158416]\n",
      " F1 score = [0.83832335 0.86294416]\n",
      " AUC score = 85.28908446400195\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 85.16483516483517\n",
      " Precision = [0.88541667 0.81395349]\n",
      " Recall = [0.84158416 0.86419753]\n",
      " F1 score = [0.86294416 0.83832335]\n",
      " AUC score = 85.28908446400195\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[85 16]\n",
      " [11 70]]\n",
      " Accuracy (acc): 85.165\n",
      " Precision (prc): 81.395\n",
      " Recall (rec): 86.42\n",
      " Sensitivity (sns): 86.42\n",
      " Specificity (spc): 84.158\n",
      " F1 Score (f1s): 83.832\n",
      " ROC AUC (AUC): 0.853\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: KNeighborsClassifier(metric='manhattan', n_neighbors=25) \n",
      "        Best parameters of the model: {'metric': 'manhattan', 'n_neighbors': 25} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
      "\n",
      "                                               model  \\\n",
      "0  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
      "\n",
      "                             model_parameters  model_scores  \\\n",
      "0  {'metric': 'manhattan', 'n_neighbors': 25}         88.01   \n",
      "\n",
      "       confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
      "0  [[85, 16], [11, 70]]    85.165     81.395   86.42        86.42   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       84.158    83.832    0.853   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:1, TRAINING:1 AND MODEL: DT \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: decision_tree_parameters\n",
      "All parameters: ['max_depth', 'criterion', 'splitter', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: DT - {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[88 13]\n",
      " [ 0 81]]\n",
      "[[81  0 13 88]\n",
      " [88 13  0 81]]\n",
      "[[169  13]\n",
      " [ 13 169]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 92.85714285714286\n",
      " Precision = [0.86170213 1.        ]\n",
      " Recall = [1.         0.87128713]\n",
      " F1 score = [0.92571429 0.93121693]\n",
      " AUC score = 93.56435643564356\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 92.85714285714286\n",
      " Precision = [1.         0.86170213]\n",
      " Recall = [0.87128713 1.        ]\n",
      " F1 score = [0.93121693 0.92571429]\n",
      " AUC score = 93.56435643564356\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[88 13]\n",
      " [ 0 81]]\n",
      " Accuracy (acc): 92.857\n",
      " Precision (prc): 86.17\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 87.129\n",
      " F1 Score (f1s): 92.571\n",
      " ROC AUC (AUC): 0.936\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: DecisionTreeClassifier(criterion='entropy', max_depth=3) \n",
      "        Best parameters of the model: {'criterion': 'entropy', 'max_depth': 3} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
      "\n",
      "                                               model  \\\n",
      "0  DecisionTreeClassifier(criterion='entropy', ma...   \n",
      "\n",
      "                           model_parameters  model_scores  \\\n",
      "0  {'criterion': 'entropy', 'max_depth': 3}         84.19   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[88, 13], [0, 81]]    92.857      86.17   100.0        100.0       87.129   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    92.571    0.936   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:1, TRAINING:1 AND MODEL: RF \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: random_forest_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'criterion', 'splitter', 'max_features', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.80514706 0.82941176 0.80441176 0.69191176 0.71691176 0.69191176\n",
      " 0.67941176 0.71691176 0.71691176 0.81764706 0.84191176 0.70441176\n",
      " 0.70441176 0.77941176 0.69191176 0.66691176 0.69191176 0.67941176\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.89524038 0.98153846 0.97538462 0.99692308 1.         1.\n",
      " 1.         1.         1.         0.87677885 1.         0.98153846\n",
      " 0.99384615 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.83503647 0.82879979 0.81461995 0.73897436 0.75998126 0.73570854\n",
      " 0.72062045 0.75765568 0.75291809 0.84147586 0.8158658  0.72759083\n",
      " 0.7526072  0.79214286 0.72846515 0.71396825 0.73904516 0.72535058\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.88743926 0.93749025 0.96482846 0.99692289 1.         1.\n",
      " 1.         1.         1.         0.87734437 0.93502822 0.96848785\n",
      " 0.9849347  1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: RF - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[100   1]\n",
      " [  0  81]]\n",
      "[[ 81   0   1 100]\n",
      " [100   1   0  81]]\n",
      "[[181   1]\n",
      " [  1 181]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 99.45054945054946\n",
      " Precision = [0.98780488 1.        ]\n",
      " Recall = [1.         0.99009901]\n",
      " F1 score = [0.99386503 0.99502488]\n",
      " AUC score = 99.5049504950495\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 99.45054945054946\n",
      " Precision = [1.         0.98780488]\n",
      " Recall = [0.99009901 1.        ]\n",
      " F1 score = [0.99502488 0.99386503]\n",
      " AUC score = 99.5049504950495\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[100   1]\n",
      " [  0  81]]\n",
      " Accuracy (acc): 99.451\n",
      " Precision (prc): 98.78\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 99.01\n",
      " F1 Score (f1s): 99.387\n",
      " ROC AUC (AUC): 0.995\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: RandomForestClassifier(max_depth=25, n_estimators=15) \n",
      "        Best parameters of the model: {'criterion': 'gini', 'max_depth': 25, 'n_estimators': 15} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
      "\n",
      "                                               model  \\\n",
      "0  (DecisionTreeClassifier(max_depth=25, max_feat...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'criterion': 'gini', 'max_depth': 25, 'n_esti...         90.44   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[100, 1], [0, 81]]    99.451      98.78   100.0        100.0        99.01   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    99.387    0.995   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:1, TRAINING:1 AND MODEL: GB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: gradient_boosting_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'learning_rate', 'max_features', 'loss', 'min_samples_split', 'min_samples_leaf'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.5, 1.0]], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "405 fits failed out of a total of 1215.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 467, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.80588235 0.88014706 0.85661765 0.84264706 0.86838235 0.86764706\n",
      " 0.88014706 0.88014706 0.86764706 0.80514706 0.84338235 0.86764706\n",
      " 0.85514706 0.88014706 0.86764706 0.85514706 0.86764706 0.86764706\n",
      " 0.84117647 0.83014706 0.88014706 0.85441176 0.86691176 0.88014706\n",
      " 0.88014706 0.88014706 0.88014706 0.76911765 0.83014706 0.79338235\n",
      " 0.85514706 0.85514706 0.86764706 0.89191176 0.88014706 0.88014706\n",
      " 0.81691176 0.84191176 0.83014706 0.83014706 0.89191176 0.87941176\n",
      " 0.87941176 0.88014706 0.86764706 0.83014706 0.84264706 0.84191176\n",
      " 0.84191176 0.86691176 0.86764706 0.88014706 0.89191176 0.87941176\n",
      " 0.73014706 0.83014706 0.83088235 0.86691176 0.87941176 0.89191176\n",
      " 0.88014706 0.86764706 0.87941176 0.78014706 0.85441176 0.80588235\n",
      " 0.90441176 0.89191176 0.85514706 0.88014706 0.89191176 0.89191176\n",
      " 0.76764706 0.73014706 0.85514706 0.88014706 0.84264706 0.86764706\n",
      " 0.89191176 0.87941176 0.86764706 0.82941176 0.85661765 0.84411765\n",
      " 0.86764706 0.88014706 0.85588235 0.85514706 0.86764706 0.86764706\n",
      " 0.84338235 0.86691176 0.87941176 0.86764706 0.88014706 0.86764706\n",
      " 0.86764706 0.87941176 0.86764706 0.85588235 0.85514706 0.86691176\n",
      " 0.86691176 0.90441176 0.89191176 0.89191176 0.89191176 0.88014706\n",
      " 0.76691176 0.84191176 0.85514706 0.85441176 0.87941176 0.86764706\n",
      " 0.89191176 0.89191176 0.86764706 0.77941176 0.79264706 0.83014706\n",
      " 0.86691176 0.87941176 0.86764706 0.87941176 0.89191176 0.89191176\n",
      " 0.84264706 0.84264706 0.81691176 0.84264706 0.90441176 0.88014706\n",
      " 0.86691176 0.90441176 0.89191176 0.79411765 0.79191176 0.87941176\n",
      " 0.85441176 0.90441176 0.85514706 0.88014706 0.90441176 0.90441176\n",
      " 0.80441176 0.85441176 0.81838235 0.89191176 0.85441176 0.82941176\n",
      " 0.89191176 0.90441176 0.89191176 0.84191176 0.80514706 0.82941176\n",
      " 0.88014706 0.89191176 0.83088235 0.86764706 0.89264706 0.87941176\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.870625   0.87990385 0.86442308 0.86144231 0.86451923 0.87682692\n",
      " 0.87682692 0.87677885 0.87985577 0.910625   0.90144231 0.93225962\n",
      " 0.91072115 0.92615385 0.94456731 0.92302885 0.91995192 0.91692308\n",
      " 0.94769231 0.96615385 0.97533654 0.97225962 0.98153846 0.99692308\n",
      " 0.99692308 1.         1.         0.95995192 0.97230769 0.98769231\n",
      " 0.99692308 0.99692308 1.         1.         1.         1.\n",
      " 0.97230769 0.98153846 0.99076923 1.         1.         0.99692308\n",
      " 1.         1.         1.         0.93538462 0.97538462 0.99692308\n",
      " 0.996875   1.         1.         1.         1.         1.\n",
      " 0.93841346 0.98149038 0.99384615 0.99692308 1.         1.\n",
      " 1.         1.         1.         0.96307692 0.97846154 0.98769231\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.96610577 0.98461538 0.98461538 1.         1.         1.\n",
      " 1.         1.         1.         0.836875   0.85201923 0.87057692\n",
      " 0.87375    0.87375    0.87677885 0.88298077 0.87677885 0.87990385\n",
      " 0.91379808 0.92918269 0.92610577 0.92       0.93225962 0.91692308\n",
      " 0.93230769 0.92615385 0.93846154 0.96       0.97538462 0.98153846\n",
      " 0.99384615 0.99076923 0.99692308 0.99692308 0.99692308 1.\n",
      " 0.95076923 0.98153846 0.99692308 1.         0.99384615 1.\n",
      " 1.         1.         1.         0.956875   0.98456731 0.99072115\n",
      " 0.99384615 1.         1.         1.         1.         1.\n",
      " 0.96302885 0.98461538 0.99076923 0.99384615 1.         1.\n",
      " 1.         1.         1.         0.96302885 0.99076923 0.98461538\n",
      " 0.99384615 1.         0.99692308 1.         1.         1.\n",
      " 0.97846154 0.99692308 0.98456731 1.         0.99384615 1.\n",
      " 1.         1.         1.         0.96610577 0.97528846 0.98461538\n",
      " 0.99692308 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.79706811 0.87581384 0.86819923 0.85708637 0.87660764 0.87542283\n",
      " 0.87931034 0.87931034 0.87285873 0.80770017 0.85765603 0.87542283\n",
      " 0.86121511 0.87931034 0.86679813 0.86597701 0.87285873 0.87542283\n",
      " 0.84508342 0.84662217 0.88277916 0.8642873  0.86467831 0.88021505\n",
      " 0.88187445 0.88021505 0.88021505 0.8063975  0.83842432 0.83055854\n",
      " 0.85521505 0.86121511 0.87333333 0.88497696 0.87777603 0.88021505\n",
      " 0.82108405 0.83220998 0.85307379 0.83141839 0.88147152 0.87762064\n",
      " 0.88333333 0.88277916 0.86727273 0.83172764 0.84098025 0.85449466\n",
      " 0.85228879 0.85943882 0.8648337  0.88021505 0.88497696 0.87809524\n",
      " 0.77308458 0.83988681 0.84309706 0.8647619  0.87002842 0.88497696\n",
      " 0.88021505 0.86679813 0.87809524 0.78844073 0.84654559 0.8400242\n",
      " 0.88103617 0.86938941 0.85842119 0.88021505 0.87891635 0.88497696\n",
      " 0.78227331 0.7639646  0.85300502 0.87171542 0.85604111 0.86727273\n",
      " 0.88497696 0.88017581 0.87333333 0.80369939 0.86213863 0.86174762\n",
      " 0.87542283 0.88187445 0.86759557 0.86854111 0.87285873 0.87285873\n",
      " 0.85076784 0.8531099  0.87157225 0.87041971 0.87081071 0.86679813\n",
      " 0.87285873 0.87762064 0.87285873 0.83367205 0.8523728  0.86224199\n",
      " 0.85685031 0.89142857 0.88497696 0.88497696 0.88753213 0.88021505\n",
      " 0.77291607 0.81629227 0.8523337  0.84981336 0.88108051 0.87333333\n",
      " 0.87648003 0.88497696 0.86727273 0.8013295  0.80347701 0.85216908\n",
      " 0.87229403 0.8745898  0.87333333 0.87203463 0.88497696 0.88147152\n",
      " 0.83422764 0.82496334 0.84116132 0.8560289  0.88899225 0.87589744\n",
      " 0.86597701 0.89398374 0.88753213 0.82341429 0.81293103 0.87565891\n",
      " 0.86185098 0.88792313 0.86597701 0.87171542 0.88536797 0.89142857\n",
      " 0.78678121 0.85436021 0.83940447 0.87419355 0.85484395 0.84493246\n",
      " 0.88147152 0.89142857 0.87891635 0.8097491  0.83405836 0.83732558\n",
      " 0.87171542 0.87648003 0.84820163 0.87089431 0.88666667 0.8745898\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.86082585 0.86959842 0.86644065 0.86319819 0.86624014 0.87458503\n",
      " 0.87066445 0.87603656 0.87393866 0.87936674 0.89916428 0.91588663\n",
      " 0.89479861 0.9043052  0.91702063 0.90285293 0.89983693 0.89939153\n",
      " 0.93205175 0.94752234 0.96059111 0.95635973 0.96253816 0.96603309\n",
      " 0.96884369 0.97043426 0.97180763 0.96443037 0.96485918 0.99224769\n",
      " 0.9924118  0.98790034 0.99238954 0.99391626 0.99694656 0.99541985\n",
      " 0.9722809  0.9845181  0.99229431 0.99541985 0.99694656 0.99844961\n",
      " 1.         1.         1.         0.94972681 0.97841292 0.99539618\n",
      " 0.99689848 0.99544298 0.99847328 1.         1.         1.\n",
      " 0.9572706  0.97702731 0.99534828 0.99236606 1.         0.99847328\n",
      " 1.         1.         1.         0.96297525 0.97843585 0.99377422\n",
      " 1.         1.         0.99847328 1.         1.         0.99847328\n",
      " 0.97186896 0.98158454 0.98914673 0.99847328 0.99544298 0.99694656\n",
      " 1.         1.         1.         0.83576209 0.85414574 0.86872192\n",
      " 0.86892442 0.87027829 0.87347246 0.87545701 0.87347246 0.87377652\n",
      " 0.89521855 0.91272666 0.90713103 0.89954317 0.90787566 0.90075775\n",
      " 0.9063874  0.90178785 0.90841954 0.95406793 0.95503385 0.96248874\n",
      " 0.96594624 0.96287238 0.96609554 0.96457419 0.96457516 0.96895425\n",
      " 0.949417   0.9741129  0.98646223 0.99402985 0.99384615 0.99391626\n",
      " 0.99541985 0.99090857 0.99391626 0.96146383 0.97862342 0.9937977\n",
      " 0.99384615 0.9969697  1.         1.         1.         1.\n",
      " 0.96313836 0.98313333 0.99532461 0.99384597 0.99694656 1.\n",
      " 1.         1.         1.         0.96173522 0.98623605 0.99222384\n",
      " 0.99534884 1.         0.99692289 1.         1.         1.\n",
      " 0.96974676 0.98928895 0.98914635 0.99389313 0.99689922 1.\n",
      " 0.99847328 0.99847328 1.         0.96617531 0.97251165 0.99067345\n",
      " 0.99539618 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: GB - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 405 candidates, totalling 2025 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[101   0]\n",
      " [  0  81]]\n",
      "[[ 81   0   0 101]\n",
      " [101   0   0  81]]\n",
      "[[182   0]\n",
      " [  0 182]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 100.0\n",
      " Precision = [1. 1.]\n",
      " Recall = [1. 1.]\n",
      " F1 score = [1. 1.]\n",
      " AUC score = 100.0\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 100.0\n",
      " Precision = [1. 1.]\n",
      " Recall = [1. 1.]\n",
      " F1 score = [1. 1.]\n",
      " AUC score = 100.0\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[101   0]\n",
      " [  0  81]]\n",
      " Accuracy (acc): 100.0\n",
      " Precision (prc): 100.0\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 100.0\n",
      " F1 Score (f1s): 100.0\n",
      " ROC AUC (AUC): 1.0\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GradientBoostingClassifier(learning_rate=0.5, n_estimators=30) \n",
      "        Best parameters of the model: {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 30} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
      "\n",
      "                                               model  \\\n",
      "0  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'learning_rate': 0.5, 'max_depth': 3, 'n_esti...         89.19   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[101, 0], [0, 81]]     100.0      100.0   100.0        100.0        100.0   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0     100.0      1.0   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:1, TRAINING:1 AND MODEL: XGB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: xtreme_gradient_boosting_parameters\n",
      "All parameters: ['max_depth', 'eta', 'max_leaves'], [[2, 3, 6, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: XGB - {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[94  7]\n",
      " [ 1 80]]\n",
      "[[80  1  7 94]\n",
      " [94  7  1 80]]\n",
      "[[174   8]\n",
      " [  8 174]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 95.6043956043956\n",
      " Precision = [0.91954023 0.98947368]\n",
      " Recall = [0.98765432 0.93069307]\n",
      " F1 score = [0.95238095 0.95918367]\n",
      " AUC score = 95.91736951472924\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 95.6043956043956\n",
      " Precision = [0.98947368 0.91954023]\n",
      " Recall = [0.93069307 0.98765432]\n",
      " F1 score = [0.95918367 0.95238095]\n",
      " AUC score = 95.91736951472924\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[94  7]\n",
      " [ 1 80]]\n",
      " Accuracy (acc): 95.604\n",
      " Precision (prc): 91.954\n",
      " Recall (rec): 98.765\n",
      " Sensitivity (sns): 98.765\n",
      " Specificity (spc): 93.069\n",
      " F1 Score (f1s): 95.238\n",
      " ROC AUC (AUC): 0.959\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False, eta=0.01,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.00999999978, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, ...) \n",
      "        Best parameters of the model: {'eta': 0.01, 'max_depth': 3} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
      "\n",
      "                                               model  \\\n",
      "0  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
      "\n",
      "                model_parameters  model_scores    confusion_matrix  accuracy  \\\n",
      "0  {'eta': 0.01, 'max_depth': 3}         89.26  [[94, 7], [1, 80]]    95.604   \n",
      "\n",
      "   precision  recall  sensitivity  specificity  f1_score  roc_auc  \n",
      "0     91.954  98.765       98.765       93.069    95.238    0.959   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            Best model index calculation  \n",
      "            \n",
      "WWWWWWWWWWWWWWWWWWW recall    Training_No  Model_No Model_Name  \\\n",
      "0            1         1         LR   \n",
      "1            1         2        SVC   \n",
      "2            1         3         NB   \n",
      "3            1         4        KNN   \n",
      "4            1         5         DT   \n",
      "5            1         6         RF   \n",
      "6            1         7         GB   \n",
      "7            1         8        XGB   \n",
      "\n",
      "                                              method  \\\n",
      "0  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
      "1  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
      "2  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
      "3  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
      "4  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
      "5  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
      "6  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
      "7  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
      "\n",
      "                                               model  \\\n",
      "0                    LogisticRegression(max_iter=50)   \n",
      "1      SVC(C=0.5, kernel='linear', probability=True)   \n",
      "2     GaussianNB(var_smoothing=0.005623413251903491)   \n",
      "3  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
      "4  DecisionTreeClassifier(criterion='entropy', ma...   \n",
      "5  (DecisionTreeClassifier(max_depth=25, max_feat...   \n",
      "6  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
      "7  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0        {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'}         89.19   \n",
      "1  {'C': 0.5, 'kernel': 'linear', 'probability': ...         88.01   \n",
      "2            {'var_smoothing': 0.005623413251903491}         88.09   \n",
      "3         {'metric': 'manhattan', 'n_neighbors': 25}         88.01   \n",
      "4           {'criterion': 'entropy', 'max_depth': 3}         84.19   \n",
      "5  {'criterion': 'gini', 'max_depth': 25, 'n_esti...         90.44   \n",
      "6  {'learning_rate': 0.5, 'max_depth': 3, 'n_esti...         89.19   \n",
      "7                      {'eta': 0.01, 'max_depth': 3}         89.26   \n",
      "\n",
      "       confusion_matrix  accuracy  precision   recall  sensitivity  \\\n",
      "0  [[87, 14], [10, 71]]    86.813     83.529   87.654       87.654   \n",
      "1  [[87, 14], [10, 71]]    86.813     83.529   87.654       87.654   \n",
      "2   [[84, 17], [5, 76]]    87.912     81.720   93.827       93.827   \n",
      "3  [[85, 16], [11, 70]]    85.165     81.395   86.420       86.420   \n",
      "4   [[88, 13], [0, 81]]    92.857     86.170  100.000      100.000   \n",
      "5   [[100, 1], [0, 81]]    99.451     98.780  100.000      100.000   \n",
      "6   [[101, 0], [0, 81]]   100.000    100.000  100.000      100.000   \n",
      "7    [[94, 7], [1, 80]]    95.604     91.954   98.765       98.765   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       86.139    85.542    0.869  \n",
      "1       86.139    85.542    0.869  \n",
      "2       83.168    87.356    0.885  \n",
      "3       84.158    83.832    0.853  \n",
      "4       87.129    92.571    0.936  \n",
      "5       99.010    99.387    0.995  \n",
      "6      100.000   100.000    1.000  \n",
      "7       93.069    95.238    0.959  \n",
      "\n",
      "            ***************************************************************************************************\n",
      "            TRAINING 1 END... \n",
      "            \n",
      "train- tri, tmp_train_ids, tmp_val_ids, tr_dat, val_dat 1 [ 31 153 137  51  83   9  99  58  77  15 177  70 180 167  86 174 114 136\n",
      "  52 101  78 162 126  48  62  47   8  64   4  89 173  10  88 139  19 179\n",
      "  33  93  74  73 170  75 175  23 171 100   5   7 144  53  68  24 141  91\n",
      "  84 131 155  29  26 122 118  41 166 145 104 127   3  38  39 148  81 154\n",
      " 146 152 172  57 160 140 134  87 176   0 105   1  82  66  25  49  34  32\n",
      "  69 117  17 168   6 103 125  59 109  18 133  43  98 178  37 132 156 151\n",
      " 119  79 130  60 150 115 124 161 113 169 121  21  92 106 181  65  76 135\n",
      " 164 147  27  30  55  54 123  11 110 159 107  80 129  50  90 165  71  56\n",
      "  45 116 138 111  16  96 120 163  40 102  95  28  46  67  20  72  97  94\n",
      "  35  12  36  85  13 157  42 142 149 108  61  14   2 143  63  22 128 112\n",
      " 158  44] [ 31 153 137  51  83   9  99  58  77  15 177  70 180 167  86 174 114 136\n",
      "  52 101  78 162 126  48  62  47   8  64   4  89 173  10  88 139  19 179\n",
      "  33  93  74  73 170  75 175  23 171 100   5   7 144  53  68  24 141  91\n",
      "  84 131 155  29  26 122 118  41 166 145 104 127   3  38  39 148  81 154\n",
      " 146 152 172  57 160 140 134  87 176   0 105   1  82  66  25  49  34  32\n",
      "  69 117  17 168   6 103 125  59 109  18 133  43  98 178  37 132 156 151\n",
      " 119  79 130  60 150 115 124 161 113 169 121  21  92 106 181  65  76 135\n",
      " 164 147  27  30  55  54 123  11 110 159 107  80 129  50  90 165  71  56\n",
      "  45 116 138 111  16  96 120 163  40 102  95  28  46  67  20  72  97  94\n",
      "  35  12  36  85  13 157  42 142 149 108  61  14   2 143  63  22 128 112\n",
      " 158  44] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "            *** MODEL TRAINING AND VALIDATION PHASE FOR TEST 1 \n",
      "            TRAINING 2 START... XXXXX \n",
      "            ***************************************************************************************************\n",
      "            Validation=> 182 ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            Training=> 182 ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            \n",
      "\n",
      "            *** ML MODEL FOR TEST:1, TRAINING:2 AND MODEL: LR \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: logistic_regression_parameters\n",
      "All parameters: ['penalty', 'solver', 'max_iter', 'C'], [['l2', 'elasticnet'], [50, 100, 130, 150, 170, 200], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]], [0, 2, 3]\n",
      "Parameters:  {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: LR - {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 84 candidates, totalling 420 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[87 14]\n",
      " [10 71]]\n",
      "[[71 10 14 87]\n",
      " [87 14 10 71]]\n",
      "[[158  24]\n",
      " [ 24 158]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 86.81318681318682\n",
      " Precision = [0.83529412 0.89690722]\n",
      " Recall = [0.87654321 0.86138614]\n",
      " F1 score = [0.85542169 0.87878788]\n",
      " AUC score = 86.89646742452022\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 86.81318681318682\n",
      " Precision = [0.89690722 0.83529412]\n",
      " Recall = [0.86138614 0.87654321]\n",
      " F1 score = [0.87878788 0.85542169]\n",
      " AUC score = 86.89646742452022\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[87 14]\n",
      " [10 71]]\n",
      " Accuracy (acc): 86.813\n",
      " Precision (prc): 83.529\n",
      " Recall (rec): 87.654\n",
      " Sensitivity (sns): 87.654\n",
      " Specificity (spc): 86.139\n",
      " F1 Score (f1s): 85.542\n",
      " ROC AUC (AUC): 0.869\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: LogisticRegression(max_iter=50) \n",
      "        Best parameters of the model: {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
      "\n",
      "                             model  \\\n",
      "0  LogisticRegression(max_iter=50)   \n",
      "\n",
      "                              model_parameters  model_scores  \\\n",
      "0  {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'}         89.19   \n",
      "\n",
      "       confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
      "0  [[87, 14], [10, 71]]    86.813     83.529  87.654       87.654   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       86.139    85.542    0.869   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:1, TRAINING:2 AND MODEL: SVC \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: support_vector_classifier_parameters\n",
      "All parameters: ['probability', 'C', 'kernel', 'gamma', 'degree', 'class_weightdict'], [[True], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], ['linear', 'rbf', 'poly']], [0, 1, 2]\n",
      "Parameters:  {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']}\n",
      "run_model_gridSearch- val_ids, train_ids [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "210 fits failed out of a total of 420.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.84411765        nan 0.84411765        nan 0.84411765        nan\n",
      " 0.84411765        nan 0.84411765        nan 0.84411765        nan\n",
      " 0.85588235        nan 0.85588235        nan 0.85588235        nan\n",
      " 0.85588235        nan 0.85588235        nan 0.85588235        nan\n",
      " 0.86764706        nan 0.86764706        nan 0.86764706        nan\n",
      " 0.86764706        nan 0.86764706        nan 0.86764706        nan\n",
      " 0.89191176        nan 0.89191176        nan 0.89191176        nan\n",
      " 0.89191176        nan 0.89191176        nan 0.89191176        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.83980769        nan 0.83980769        nan 0.83980769        nan\n",
      " 0.83980769        nan 0.83980769        nan 0.83980769        nan\n",
      " 0.84288462        nan 0.84288462        nan 0.84288462        nan\n",
      " 0.84288462        nan 0.84288462        nan 0.84288462        nan\n",
      " 0.85831731        nan 0.85831731        nan 0.85831731        nan\n",
      " 0.85831731        nan 0.85831731        nan 0.85831731        nan\n",
      " 0.87370192        nan 0.87370192        nan 0.87370192        nan\n",
      " 0.87370192        nan 0.87370192        nan 0.87370192        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.84460476        nan 0.84460476        nan 0.84460476        nan\n",
      " 0.84460476        nan 0.84460476        nan 0.84460476        nan\n",
      " 0.843129          nan 0.843129          nan 0.843129          nan\n",
      " 0.843129          nan 0.843129          nan 0.843129          nan\n",
      " 0.84839581        nan 0.84839581        nan 0.84839581        nan\n",
      " 0.84839581        nan 0.84839581        nan 0.84839581        nan\n",
      " 0.8644052         nan 0.8644052         nan 0.8644052         nan\n",
      " 0.8644052         nan 0.8644052         nan 0.8644052         nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.84625639        nan 0.84625639        nan 0.84625639        nan\n",
      " 0.84625639        nan 0.84625639        nan 0.84625639        nan\n",
      " 0.83228376        nan 0.83228376        nan 0.83228376        nan\n",
      " 0.83228376        nan 0.83228376        nan 0.83228376        nan\n",
      " 0.84295991        nan 0.84295991        nan 0.84295991        nan\n",
      " 0.84295991        nan 0.84295991        nan 0.84295991        nan\n",
      " 0.85320781        nan 0.85320781        nan 0.85320781        nan\n",
      " 0.85320781        nan 0.85320781        nan 0.85320781        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: SVC - {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[87 14]\n",
      " [10 71]]\n",
      "[[71 10 14 87]\n",
      " [87 14 10 71]]\n",
      "[[158  24]\n",
      " [ 24 158]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 86.81318681318682\n",
      " Precision = [0.83529412 0.89690722]\n",
      " Recall = [0.87654321 0.86138614]\n",
      " F1 score = [0.85542169 0.87878788]\n",
      " AUC score = 86.89646742452022\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 86.81318681318682\n",
      " Precision = [0.89690722 0.83529412]\n",
      " Recall = [0.86138614 0.87654321]\n",
      " F1 score = [0.87878788 0.85542169]\n",
      " AUC score = 86.89646742452022\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[87 14]\n",
      " [10 71]]\n",
      " Accuracy (acc): 86.813\n",
      " Precision (prc): 83.529\n",
      " Recall (rec): 87.654\n",
      " Sensitivity (sns): 87.654\n",
      " Specificity (spc): 86.139\n",
      " F1 Score (f1s): 85.542\n",
      " ROC AUC (AUC): 0.869\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: SVC(C=0.5, kernel='linear', probability=True) \n",
      "        Best parameters of the model: {'C': 0.5, 'kernel': 'linear', 'probability': True} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
      "\n",
      "                                           model  \\\n",
      "0  SVC(C=0.5, kernel='linear', probability=True)   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'C': 0.5, 'kernel': 'linear', 'probability': ...         88.01   \n",
      "\n",
      "       confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
      "0  [[87, 14], [10, 71]]    86.813     83.529  87.654       87.654   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       86.139    85.542    0.869   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:1, TRAINING:2 AND MODEL: NB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: naive_bayes_parameters\n",
      "All parameters: ['var_smoothing'], [[1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]], [0]\n",
      "Parameters:  {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]}\n",
      "run_model_gridSearch- val_ids, train_ids [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: NB - {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[84 17]\n",
      " [ 5 76]]\n",
      "[[76  5 17 84]\n",
      " [84 17  5 76]]\n",
      "[[160  22]\n",
      " [ 22 160]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.91208791208791\n",
      " Precision = [0.8172043  0.94382022]\n",
      " Recall = [0.9382716  0.83168317]\n",
      " F1 score = [0.87356322 0.88421053]\n",
      " AUC score = 88.49773866275515\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.91208791208791\n",
      " Precision = [0.94382022 0.8172043 ]\n",
      " Recall = [0.83168317 0.9382716 ]\n",
      " F1 score = [0.88421053 0.87356322]\n",
      " AUC score = 88.49773866275515\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[84 17]\n",
      " [ 5 76]]\n",
      " Accuracy (acc): 87.912\n",
      " Precision (prc): 81.72\n",
      " Recall (rec): 93.827\n",
      " Sensitivity (sns): 93.827\n",
      " Specificity (spc): 83.168\n",
      " F1 Score (f1s): 87.356\n",
      " ROC AUC (AUC): 0.885\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GaussianNB(var_smoothing=0.005623413251903491) \n",
      "        Best parameters of the model: {'var_smoothing': 0.005623413251903491} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
      "\n",
      "                                            model  \\\n",
      "0  GaussianNB(var_smoothing=0.005623413251903491)   \n",
      "\n",
      "                          model_parameters  model_scores     confusion_matrix  \\\n",
      "0  {'var_smoothing': 0.005623413251903491}         88.09  [[84, 17], [5, 76]]   \n",
      "\n",
      "   accuracy  precision  recall  sensitivity  specificity  f1_score  roc_auc  \n",
      "0    87.912      81.72  93.827       93.827       83.168    87.356    0.885   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:1, TRAINING:2 AND MODEL: KNN \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: k_nearest_neighbors_parameters\n",
      "All parameters: ['n_neighbors', 'p', 'metric', 'n_splits'], [[2, 3, 5, 10, 15, 25, 35], ['manhattan', 'minkowski', 'euclidean']], [0, 2]\n",
      "Parameters:  {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']}\n",
      "run_model_gridSearch- val_ids, train_ids [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: KNN - {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[85 16]\n",
      " [11 70]]\n",
      "[[70 11 16 85]\n",
      " [85 16 11 70]]\n",
      "[[155  27]\n",
      " [ 27 155]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 85.16483516483517\n",
      " Precision = [0.81395349 0.88541667]\n",
      " Recall = [0.86419753 0.84158416]\n",
      " F1 score = [0.83832335 0.86294416]\n",
      " AUC score = 85.28908446400195\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 85.16483516483517\n",
      " Precision = [0.88541667 0.81395349]\n",
      " Recall = [0.84158416 0.86419753]\n",
      " F1 score = [0.86294416 0.83832335]\n",
      " AUC score = 85.28908446400195\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[85 16]\n",
      " [11 70]]\n",
      " Accuracy (acc): 85.165\n",
      " Precision (prc): 81.395\n",
      " Recall (rec): 86.42\n",
      " Sensitivity (sns): 86.42\n",
      " Specificity (spc): 84.158\n",
      " F1 Score (f1s): 83.832\n",
      " ROC AUC (AUC): 0.853\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: KNeighborsClassifier(metric='manhattan', n_neighbors=25) \n",
      "        Best parameters of the model: {'metric': 'manhattan', 'n_neighbors': 25} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
      "\n",
      "                                               model  \\\n",
      "0  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
      "\n",
      "                             model_parameters  model_scores  \\\n",
      "0  {'metric': 'manhattan', 'n_neighbors': 25}         88.01   \n",
      "\n",
      "       confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
      "0  [[85, 16], [11, 70]]    85.165     81.395   86.42        86.42   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       84.158    83.832    0.853   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:1, TRAINING:2 AND MODEL: DT \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: decision_tree_parameters\n",
      "All parameters: ['max_depth', 'criterion', 'splitter', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: DT - {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[88 13]\n",
      " [ 0 81]]\n",
      "[[81  0 13 88]\n",
      " [88 13  0 81]]\n",
      "[[169  13]\n",
      " [ 13 169]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 92.85714285714286\n",
      " Precision = [0.86170213 1.        ]\n",
      " Recall = [1.         0.87128713]\n",
      " F1 score = [0.92571429 0.93121693]\n",
      " AUC score = 93.56435643564356\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 92.85714285714286\n",
      " Precision = [1.         0.86170213]\n",
      " Recall = [0.87128713 1.        ]\n",
      " F1 score = [0.93121693 0.92571429]\n",
      " AUC score = 93.56435643564356\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[88 13]\n",
      " [ 0 81]]\n",
      " Accuracy (acc): 92.857\n",
      " Precision (prc): 86.17\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 87.129\n",
      " F1 Score (f1s): 92.571\n",
      " ROC AUC (AUC): 0.936\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: DecisionTreeClassifier(criterion='entropy', max_depth=3) \n",
      "        Best parameters of the model: {'criterion': 'entropy', 'max_depth': 3} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
      "\n",
      "                                               model  \\\n",
      "0  DecisionTreeClassifier(criterion='entropy', ma...   \n",
      "\n",
      "                           model_parameters  model_scores  \\\n",
      "0  {'criterion': 'entropy', 'max_depth': 3}         85.44   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[88, 13], [0, 81]]    92.857      86.17   100.0        100.0       87.129   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    92.571    0.936   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:1, TRAINING:2 AND MODEL: RF \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: random_forest_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'criterion', 'splitter', 'max_features', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.81764706 0.80441176 0.76691176 0.69191176 0.75441176 0.74191176\n",
      " 0.74191176 0.66691176 0.71691176 0.79264706 0.85441176 0.79191176\n",
      " 0.72941176 0.75441176 0.70441176 0.67941176 0.72941176 0.75441176\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.89524038 0.98153846 0.97538462 0.99384615 1.         1.\n",
      " 1.         1.         1.         0.87677885 1.         0.99692308\n",
      " 0.99384615 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.84147586 0.80185003 0.78045713 0.7444916  0.78496504 0.77756368\n",
      " 0.76614053 0.71750507 0.75329096 0.82818163 0.82125173 0.7976376\n",
      " 0.75544462 0.77172414 0.74571429 0.7277869  0.75863895 0.76966813\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.88743926 0.93749025 0.96482846 0.99689922 1.         1.\n",
      " 1.         1.         1.         0.87734437 0.93502822 0.96779268\n",
      " 0.9849347  1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: RF - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[100   1]\n",
      " [  0  81]]\n",
      "[[ 81   0   1 100]\n",
      " [100   1   0  81]]\n",
      "[[181   1]\n",
      " [  1 181]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 99.45054945054946\n",
      " Precision = [0.98780488 1.        ]\n",
      " Recall = [1.         0.99009901]\n",
      " F1 score = [0.99386503 0.99502488]\n",
      " AUC score = 99.5049504950495\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 99.45054945054946\n",
      " Precision = [1.         0.98780488]\n",
      " Recall = [0.99009901 1.        ]\n",
      " F1 score = [0.99502488 0.99386503]\n",
      " AUC score = 99.5049504950495\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[100   1]\n",
      " [  0  81]]\n",
      " Accuracy (acc): 99.451\n",
      " Precision (prc): 98.78\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 99.01\n",
      " F1 Score (f1s): 99.387\n",
      " ROC AUC (AUC): 0.995\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: RandomForestClassifier(max_depth=20, n_estimators=15) \n",
      "        Best parameters of the model: {'criterion': 'gini', 'max_depth': 20, 'n_estimators': 15} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
      "\n",
      "                                               model  \\\n",
      "0  (DecisionTreeClassifier(max_depth=20, max_feat...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'criterion': 'gini', 'max_depth': 20, 'n_esti...         90.44   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[100, 1], [0, 81]]    99.451      98.78   100.0        100.0        99.01   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    99.387    0.995   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:1, TRAINING:2 AND MODEL: GB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: gradient_boosting_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'learning_rate', 'max_features', 'loss', 'min_samples_split', 'min_samples_leaf'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.5, 1.0]], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "405 fits failed out of a total of 1215.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 467, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.84411765 0.87941176 0.86764706 0.87941176 0.86764706 0.88014706\n",
      " 0.85588235 0.86764706 0.88014706 0.83235294 0.86691176 0.85588235\n",
      " 0.87941176 0.87941176 0.86764706 0.86764706 0.86764706 0.86764706\n",
      " 0.84191176 0.75661765 0.87941176 0.84191176 0.85514706 0.84264706\n",
      " 0.88014706 0.88014706 0.89191176 0.84191176 0.82941176 0.86764706\n",
      " 0.88014706 0.87941176 0.86691176 0.88014706 0.88014706 0.87941176\n",
      " 0.79264706 0.81691176 0.80514706 0.87941176 0.85514706 0.84264706\n",
      " 0.86691176 0.89191176 0.88014706 0.74264706 0.79338235 0.80441176\n",
      " 0.87941176 0.87941176 0.86764706 0.85514706 0.89191176 0.89191176\n",
      " 0.83014706 0.81764706 0.84264706 0.90441176 0.85441176 0.88014706\n",
      " 0.90441176 0.88014706 0.88014706 0.79264706 0.85514706 0.83014706\n",
      " 0.84191176 0.86764706 0.90441176 0.89191176 0.89191176 0.88014706\n",
      " 0.77941176 0.84191176 0.82941176 0.86691176 0.85514706 0.86764706\n",
      " 0.87941176 0.89191176 0.88014706 0.86764706 0.86691176 0.86764706\n",
      " 0.88014706 0.85588235 0.86764706 0.86764706 0.86764706 0.86764706\n",
      " 0.84264706 0.87941176 0.86764706 0.85514706 0.86764706 0.86764706\n",
      " 0.88014706 0.88014706 0.86764706 0.80514706 0.82941176 0.89191176\n",
      " 0.87941176 0.88014706 0.88014706 0.87941176 0.88014706 0.86764706\n",
      " 0.80514706 0.80514706 0.84264706 0.89264706 0.86764706 0.86691176\n",
      " 0.87941176 0.89191176 0.86764706 0.85441176 0.86764706 0.81691176\n",
      " 0.89191176 0.88014706 0.86691176 0.86691176 0.87941176 0.88014706\n",
      " 0.82941176 0.74191176 0.78014706 0.86691176 0.88014706 0.88014706\n",
      " 0.89191176 0.89191176 0.86764706 0.77941176 0.77941176 0.79264706\n",
      " 0.86691176 0.84191176 0.87941176 0.85441176 0.90441176 0.87941176\n",
      " 0.70514706 0.85441176 0.81764706 0.88014706 0.89191176 0.87941176\n",
      " 0.87941176 0.87941176 0.89191176 0.83014706 0.80514706 0.84191176\n",
      " 0.87941176 0.89191176 0.86764706 0.89191176 0.88014706 0.89191176\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.88908654 0.87067308 0.89528846 0.87375    0.87990385 0.87677885\n",
      " 0.8675     0.87990385 0.87677885 0.92298077 0.90769231 0.92\n",
      " 0.92       0.92302885 0.92307692 0.92       0.92307692 0.92923077\n",
      " 0.93225962 0.97538462 0.98153846 0.99384615 0.98764423 0.99076923\n",
      " 0.99384615 1.         1.         0.97538462 0.97538462 0.99692308\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.96605769 0.97230769 0.99076923 1.         1.         1.\n",
      " 1.         1.         1.         0.956875   0.97538462 0.98769231\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.96615385 0.98149038 0.98461538 0.99692308 1.         1.\n",
      " 1.         1.         1.         0.97538462 0.98461538 1.\n",
      " 0.99692308 0.99384615 1.         0.99692308 1.         1.\n",
      " 0.95692308 0.98769231 0.98456731 1.         1.         0.99379808\n",
      " 1.         1.         1.         0.88302885 0.87384615 0.86139423\n",
      " 0.88610577 0.87673077 0.86754808 0.87677885 0.87370192 0.87677885\n",
      " 0.90456731 0.94149038 0.92307692 0.91375    0.90769231 0.91692308\n",
      " 0.92605769 0.91379808 0.90764423 0.95384615 0.96307692 0.99384615\n",
      " 0.99076923 0.99384615 0.99692308 0.99384615 1.         1.\n",
      " 0.95076923 0.96918269 0.98769231 0.99384615 1.         1.\n",
      " 1.         1.         1.         0.96923077 0.96610577 0.99692308\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.97230769 0.98461538 0.98764423 0.99692308 1.         1.\n",
      " 1.         0.99692308 1.         0.97538462 0.98153846 0.98461538\n",
      " 0.99692308 1.         1.         1.         1.         1.\n",
      " 0.97225962 0.98769231 0.99384615 1.         1.         1.\n",
      " 0.99692308 1.         1.         0.96302885 0.99384615 0.99692308\n",
      " 1.         1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.85568701 0.88285873 0.86936223 0.88017581 0.87285873 0.88187445\n",
      " 0.86759557 0.87285873 0.88187445 0.81533207 0.86830259 0.86759557\n",
      " 0.87285873 0.87156003 0.87041971 0.87285873 0.86679813 0.87041971\n",
      " 0.83589234 0.79092794 0.87681034 0.85539937 0.85991641 0.86118479\n",
      " 0.87415445 0.87777603 0.88254064 0.85228879 0.82521808 0.86165445\n",
      " 0.86814516 0.87852535 0.87419879 0.88021505 0.88277916 0.88065041\n",
      " 0.80828884 0.82162919 0.82890595 0.87852535 0.86640712 0.8519427\n",
      " 0.86125647 0.87891635 0.88021505 0.76945315 0.80286797 0.83668716\n",
      " 0.8630631  0.87619048 0.87589744 0.85991641 0.88753213 0.87891635\n",
      " 0.83532208 0.82707373 0.85604111 0.89064516 0.86471741 0.86337104\n",
      " 0.89142857 0.87777603 0.88277916 0.79745551 0.85736371 0.8470254\n",
      " 0.85709677 0.86679813 0.89142857 0.88497696 0.89021505 0.88021505\n",
      " 0.78674117 0.84664363 0.84851765 0.86510842 0.86353799 0.87333333\n",
      " 0.88017581 0.88497696 0.88021505 0.87285873 0.8513329  0.87285873\n",
      " 0.87931034 0.86759557 0.87285873 0.87542283 0.87542283 0.87542283\n",
      " 0.82639943 0.87012987 0.86809683 0.86035873 0.87285873 0.87285873\n",
      " 0.87931034 0.87687132 0.87285873 0.82211982 0.83744685 0.87203463\n",
      " 0.87762064 0.88277916 0.87671855 0.87809524 0.87171542 0.87333333\n",
      " 0.81901822 0.81949138 0.8620895  0.88422764 0.86900154 0.86559524\n",
      " 0.87207373 0.88753213 0.86727273 0.84409927 0.85838209 0.82775923\n",
      " 0.88497696 0.88021505 0.86813819 0.87597701 0.8746289  0.88021505\n",
      " 0.8146595  0.78429413 0.8210582  0.8631858  0.88021505 0.87675765\n",
      " 0.88753213 0.87891635 0.87089431 0.80698508 0.80264687 0.83208791\n",
      " 0.86215578 0.85740558 0.88065041 0.86078187 0.88293164 0.87809524\n",
      " 0.76729643 0.8419086  0.83730143 0.87171542 0.88021505 0.8635377\n",
      " 0.87809524 0.87565891 0.87891635 0.83069508 0.82967874 0.85390015\n",
      " 0.86039485 0.87891635 0.87333333 0.87891635 0.87671855 0.88497696\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.87590143 0.86976935 0.87852681 0.8688962  0.87637012 0.87095392\n",
      " 0.86947742 0.87509785 0.87345288 0.90260236 0.89298731 0.90371757\n",
      " 0.89967614 0.90958449 0.90409085 0.89968862 0.90264409 0.90477709\n",
      " 0.91807595 0.95504297 0.96249239 0.97160661 0.96700151 0.96716121\n",
      " 0.96585959 0.97037037 0.966122   0.967961   0.97532996 0.99541931\n",
      " 0.99544298 0.99393939 0.99541985 0.99847328 0.99847328 1.\n",
      " 0.96308248 0.97373329 0.9907197  1.         0.9969697  1.\n",
      " 1.         1.         1.         0.95835015 0.97365842 0.99377422\n",
      " 0.99541985 0.9969697  1.         1.         1.         1.\n",
      " 0.95609117 0.98301655 0.98609422 0.99692289 0.99694656 1.\n",
      " 1.         1.         1.         0.97691024 0.98454346 0.993962\n",
      " 0.99541985 0.99537269 1.         0.99844961 1.         1.\n",
      " 0.9572186  0.97725333 0.98611732 0.993962   0.99847328 0.99687481\n",
      " 1.         1.         1.         0.85343435 0.87004895 0.86443003\n",
      " 0.87720272 0.87101478 0.86816579 0.8721707  0.87161308 0.8747348\n",
      " 0.89378212 0.90594535 0.89978764 0.90189243 0.89525687 0.89927631\n",
      " 0.90181197 0.89779675 0.89306331 0.93525632 0.94166224 0.96729504\n",
      " 0.96299749 0.97304387 0.96182419 0.96731535 0.97180763 0.97180763\n",
      " 0.95348278 0.97077766 0.98167903 0.98935818 0.99393939 0.9969697\n",
      " 1.         0.99694656 0.99847328 0.96189265 0.96463802 0.99692289\n",
      " 0.99391626 1.         1.         0.99847328 1.         1.\n",
      " 0.96929889 0.98167354 0.98916984 0.99236587 1.         0.9969697\n",
      " 1.         0.99844961 1.         0.96510135 0.98749549 0.9876446\n",
      " 0.99539636 1.         0.99847328 1.         1.         1.\n",
      " 0.96774686 0.98318262 0.99537269 0.99541985 1.         1.\n",
      " 0.99692308 1.         1.         0.96459799 0.98477909 0.99539618\n",
      " 0.9969697  0.99544298 1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: GB - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 405 candidates, totalling 2025 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[101   0]\n",
      " [  0  81]]\n",
      "[[ 81   0   0 101]\n",
      " [101   0   0  81]]\n",
      "[[182   0]\n",
      " [  0 182]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 100.0\n",
      " Precision = [1. 1.]\n",
      " Recall = [1. 1.]\n",
      " F1 score = [1. 1.]\n",
      " AUC score = 100.0\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 100.0\n",
      " Precision = [1. 1.]\n",
      " Recall = [1. 1.]\n",
      " F1 score = [1. 1.]\n",
      " AUC score = 100.0\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[101   0]\n",
      " [  0  81]]\n",
      " Accuracy (acc): 100.0\n",
      " Precision (prc): 100.0\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 100.0\n",
      " F1 Score (f1s): 100.0\n",
      " ROC AUC (AUC): 1.0\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GradientBoostingClassifier(learning_rate=0.5, n_estimators=30) \n",
      "        Best parameters of the model: {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 30} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
      "\n",
      "                                               model  \\\n",
      "0  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'learning_rate': 0.5, 'max_depth': 3, 'n_esti...         89.19   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[101, 0], [0, 81]]     100.0      100.0   100.0        100.0        100.0   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0     100.0      1.0   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:1, TRAINING:2 AND MODEL: XGB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: xtreme_gradient_boosting_parameters\n",
      "All parameters: ['max_depth', 'eta', 'max_leaves'], [[2, 3, 6, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco2', 'narco3', 'narco4', 'narco5', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1111', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: XGB - {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[94  7]\n",
      " [ 1 80]]\n",
      "[[80  1  7 94]\n",
      " [94  7  1 80]]\n",
      "[[174   8]\n",
      " [  8 174]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 95.6043956043956\n",
      " Precision = [0.91954023 0.98947368]\n",
      " Recall = [0.98765432 0.93069307]\n",
      " F1 score = [0.95238095 0.95918367]\n",
      " AUC score = 95.91736951472924\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 95.6043956043956\n",
      " Precision = [0.98947368 0.91954023]\n",
      " Recall = [0.93069307 0.98765432]\n",
      " F1 score = [0.95918367 0.95238095]\n",
      " AUC score = 95.91736951472924\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[94  7]\n",
      " [ 1 80]]\n",
      " Accuracy (acc): 95.604\n",
      " Precision (prc): 91.954\n",
      " Recall (rec): 98.765\n",
      " Sensitivity (sns): 98.765\n",
      " Specificity (spc): 93.069\n",
      " F1 Score (f1s): 95.238\n",
      " ROC AUC (AUC): 0.959\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False, eta=0.01,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.00999999978, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, ...) \n",
      "        Best parameters of the model: {'eta': 0.01, 'max_depth': 3} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
      "\n",
      "                                               model  \\\n",
      "0  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
      "\n",
      "                model_parameters  model_scores    confusion_matrix  accuracy  \\\n",
      "0  {'eta': 0.01, 'max_depth': 3}         89.26  [[94, 7], [1, 80]]    95.604   \n",
      "\n",
      "   precision  recall  sensitivity  specificity  f1_score  roc_auc  \n",
      "0     91.954  98.765       98.765       93.069    95.238    0.959   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            Best model index calculation  \n",
      "            \n",
      "\n",
      "            ***************************************************************************************************\n",
      "            TRAINING 2 END... \n",
      "            \n",
      "\n",
      "        ### MODEL EVALUATION PHASE \n",
      "        EVALUATION 1 START... XXXXX \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "        From training? False, Data shape: (48, 18), Indices: [128, 0, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 26, 6, 31, 32, 33, 34, 7, 35, 36, 37, 38, 8, 9, 2, 71, 72, 81, 82, 83, 84, 101, 103, 104, 105, 106, 107]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (48, 14), Target shape: (48,), Metadata: (48, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[20  6]\n",
      " [ 1 21]]\n",
      "[[21  1  6 20]\n",
      " [20  6  1 21]]\n",
      "[[41  7]\n",
      " [ 7 41]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 85.41666666666666\n",
      " Precision = [0.77777778 0.95238095]\n",
      " Recall = [0.95454545 0.76923077]\n",
      " F1 score = [0.85714286 0.85106383]\n",
      " AUC score = 86.18881118881119\n",
      " Support = [22 26]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 85.41666666666666\n",
      " Precision = [0.95238095 0.77777778]\n",
      " Recall = [0.76923077 0.95454545]\n",
      " F1 score = [0.85106383 0.85714286]\n",
      " AUC score = 86.18881118881119\n",
      " Support = [26 22]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[20  6]\n",
      " [ 1 21]]\n",
      " Accuracy (acc): 85.417\n",
      " Precision (prc): 77.778\n",
      " Recall (rec): 95.455\n",
      " Sensitivity (sns): 95.455\n",
      " Specificity (spc): 76.923\n",
      " F1 Score (f1s): 85.714\n",
      " ROC AUC (AUC): 0.862\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[19  7]\n",
      " [ 1 21]]\n",
      "[[21  1  7 19]\n",
      " [19  7  1 21]]\n",
      "[[40  8]\n",
      " [ 8 40]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 83.33333333333334\n",
      " Precision = [0.75 0.95]\n",
      " Recall = [0.95454545 0.73076923]\n",
      " F1 score = [0.84       0.82608696]\n",
      " AUC score = 84.26573426573427\n",
      " Support = [22 26]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 83.33333333333334\n",
      " Precision = [0.95 0.75]\n",
      " Recall = [0.73076923 0.95454545]\n",
      " F1 score = [0.82608696 0.84      ]\n",
      " AUC score = 84.26573426573427\n",
      " Support = [26 22]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[19  7]\n",
      " [ 1 21]]\n",
      " Accuracy (acc): 83.333\n",
      " Precision (prc): 75.0\n",
      " Recall (rec): 95.455\n",
      " Sensitivity (sns): 95.455\n",
      " Specificity (spc): 73.077\n",
      " F1 Score (f1s): 84.0\n",
      " ROC AUC (AUC): 0.843\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[20  6]\n",
      " [ 0 22]]\n",
      "[[22  0  6 20]\n",
      " [20  6  0 22]]\n",
      "[[42  6]\n",
      " [ 6 42]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.5\n",
      " Precision = [0.78571429 1.        ]\n",
      " Recall = [1.         0.76923077]\n",
      " F1 score = [0.88       0.86956522]\n",
      " AUC score = 88.46153846153845\n",
      " Support = [22 26]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.5\n",
      " Precision = [1.         0.78571429]\n",
      " Recall = [0.76923077 1.        ]\n",
      " F1 score = [0.86956522 0.88      ]\n",
      " AUC score = 88.46153846153845\n",
      " Support = [26 22]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[20  6]\n",
      " [ 0 22]]\n",
      " Accuracy (acc): 87.5\n",
      " Precision (prc): 78.571\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 76.923\n",
      " F1 Score (f1s): 88.0\n",
      " ROC AUC (AUC): 0.885\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[17  9]\n",
      " [ 1 21]]\n",
      "[[21  1  9 17]\n",
      " [17  9  1 21]]\n",
      "[[38 10]\n",
      " [10 38]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 79.16666666666666\n",
      " Precision = [0.7        0.94444444]\n",
      " Recall = [0.95454545 0.65384615]\n",
      " F1 score = [0.80769231 0.77272727]\n",
      " AUC score = 80.41958041958041\n",
      " Support = [22 26]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 79.16666666666666\n",
      " Precision = [0.94444444 0.7       ]\n",
      " Recall = [0.65384615 0.95454545]\n",
      " F1 score = [0.77272727 0.80769231]\n",
      " AUC score = 80.41958041958041\n",
      " Support = [26 22]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[17  9]\n",
      " [ 1 21]]\n",
      " Accuracy (acc): 79.167\n",
      " Precision (prc): 70.0\n",
      " Recall (rec): 95.455\n",
      " Sensitivity (sns): 95.455\n",
      " Specificity (spc): 65.385\n",
      " F1 Score (f1s): 80.769\n",
      " ROC AUC (AUC): 0.804\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[18  8]\n",
      " [ 1 21]]\n",
      "[[21  1  8 18]\n",
      " [18  8  1 21]]\n",
      "[[39  9]\n",
      " [ 9 39]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 81.25\n",
      " Precision = [0.72413793 0.94736842]\n",
      " Recall = [0.95454545 0.69230769]\n",
      " F1 score = [0.82352941 0.8       ]\n",
      " AUC score = 82.34265734265735\n",
      " Support = [22 26]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 81.25\n",
      " Precision = [0.94736842 0.72413793]\n",
      " Recall = [0.69230769 0.95454545]\n",
      " F1 score = [0.8        0.82352941]\n",
      " AUC score = 82.34265734265735\n",
      " Support = [26 22]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[18  8]\n",
      " [ 1 21]]\n",
      " Accuracy (acc): 81.25\n",
      " Precision (prc): 72.414\n",
      " Recall (rec): 95.455\n",
      " Sensitivity (sns): 95.455\n",
      " Specificity (spc): 69.231\n",
      " F1 Score (f1s): 82.353\n",
      " ROC AUC (AUC): 0.823\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[20  6]\n",
      " [ 0 22]]\n",
      "[[22  0  6 20]\n",
      " [20  6  0 22]]\n",
      "[[42  6]\n",
      " [ 6 42]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.5\n",
      " Precision = [0.78571429 1.        ]\n",
      " Recall = [1.         0.76923077]\n",
      " F1 score = [0.88       0.86956522]\n",
      " AUC score = 88.46153846153845\n",
      " Support = [22 26]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.5\n",
      " Precision = [1.         0.78571429]\n",
      " Recall = [0.76923077 1.        ]\n",
      " F1 score = [0.86956522 0.88      ]\n",
      " AUC score = 88.46153846153845\n",
      " Support = [26 22]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[20  6]\n",
      " [ 0 22]]\n",
      " Accuracy (acc): 87.5\n",
      " Precision (prc): 78.571\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 76.923\n",
      " F1 Score (f1s): 88.0\n",
      " ROC AUC (AUC): 0.885\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[20  6]\n",
      " [ 1 21]]\n",
      "[[21  1  6 20]\n",
      " [20  6  1 21]]\n",
      "[[41  7]\n",
      " [ 7 41]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 85.41666666666666\n",
      " Precision = [0.77777778 0.95238095]\n",
      " Recall = [0.95454545 0.76923077]\n",
      " F1 score = [0.85714286 0.85106383]\n",
      " AUC score = 86.18881118881119\n",
      " Support = [22 26]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 85.41666666666666\n",
      " Precision = [0.95238095 0.77777778]\n",
      " Recall = [0.76923077 0.95454545]\n",
      " F1 score = [0.85106383 0.85714286]\n",
      " AUC score = 86.18881118881119\n",
      " Support = [26 22]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[20  6]\n",
      " [ 1 21]]\n",
      " Accuracy (acc): 85.417\n",
      " Precision (prc): 77.778\n",
      " Recall (rec): 95.455\n",
      " Sensitivity (sns): 95.455\n",
      " Specificity (spc): 76.923\n",
      " F1 Score (f1s): 85.714\n",
      " ROC AUC (AUC): 0.862\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[20  6]\n",
      " [ 2 20]]\n",
      "[[20  2  6 20]\n",
      " [20  6  2 20]]\n",
      "[[40  8]\n",
      " [ 8 40]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 83.33333333333334\n",
      " Precision = [0.76923077 0.90909091]\n",
      " Recall = [0.90909091 0.76923077]\n",
      " F1 score = [0.83333333 0.83333333]\n",
      " AUC score = 83.91608391608392\n",
      " Support = [22 26]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 83.33333333333334\n",
      " Precision = [0.90909091 0.76923077]\n",
      " Recall = [0.76923077 0.90909091]\n",
      " F1 score = [0.83333333 0.83333333]\n",
      " AUC score = 83.91608391608392\n",
      " Support = [26 22]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[20  6]\n",
      " [ 2 20]]\n",
      " Accuracy (acc): 83.333\n",
      " Precision (prc): 76.923\n",
      " Recall (rec): 90.909\n",
      " Sensitivity (sns): 90.909\n",
      " Specificity (spc): 76.923\n",
      " F1 Score (f1s): 83.333\n",
      " ROC AUC (AUC): 0.839\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "            ===================================================================================================\n",
      "            TEST 1 END...\n",
      "            \n",
      "test- tsi, train_all_ids, test_ids, tr_all_dat, ts_dat 1 [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [129, 1, 3, 27, 10, 11, 12, 13, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 39, 40, 41, 42, 43, 44, 45, 46, 73, 74, 85, 86, 87, 88, 102, 108, 109, 110, 111, 112] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['n1111', 'brux2', 'sdb2', 'narco2', 'ins5', 'ins6', 'ins7', 'ins8', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'plm3', 'plm4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10']\n",
      "\n",
      "            ### MODEL TEST PHASE \n",
      "            TEST 2 START... XXXXX \n",
      "            ===================================================================================================\n",
      "            Test=> 48 ['n1111', 'brux2', 'sdb2', 'narco2', 'ins5', 'ins6', 'ins7', 'ins8', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'plm3', 'plm4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd22', 'n6', 'n7', 'n8', 'n9', 'n10'] \n",
      "            Training (Including Validation)=> 182 ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            \n",
      "Random 5 percentage splitting testing...\n",
      "train- tri, tmp_train_ids, tmp_val_ids, tr_dat, val_dat 0 [117 178   4  54  67  92 159  55  16 107  59 170 125 168  27  20 141  22\n",
      "   9 180  91  28  31   3 152   5  87 121 112 135 116 145  96  71 177 104\n",
      "  75  50  36  84  88  58 106  47  72 127  45  35  93  77  13 114 105 103\n",
      "  81 147 157 176 137 179 143  37  74  62  89 108  99 181  57 166  25 140\n",
      " 139 122 129 134  33  90 158 142  17 169  46 120   2  40  98 173  15  14\n",
      " 102  85  56  48 110  79 109   1 128 153 124 172 160  78 149  43   8 133\n",
      "  76 111 136  30 163 146  34 148 123 162  66 118 115  86  80 150 154  19\n",
      " 100 167  38 165 174 175 155  41  51  44  69  82  42 132 119   7  52  29\n",
      "  83  18 151 130  61 171  95  12 126  11  68  10  23 161 156 131  60  94\n",
      "  39  21   0  65  49  73  26 138   6  24  97 144  64  53  70 101 164  32\n",
      " 113  63] [117 178   4  54  67  92 159  55  16 107  59 170 125 168  27  20 141  22\n",
      "   9 180  91  28  31   3 152   5  87 121 112 135 116 145  96  71 177 104\n",
      "  75  50  36  84  88  58 106  47  72 127  45  35  93  77  13 114 105 103\n",
      "  81 147 157 176 137 179 143  37  74  62  89 108  99 181  57 166  25 140\n",
      " 139 122 129 134  33  90 158 142  17 169  46 120   2  40  98 173  15  14\n",
      " 102  85  56  48 110  79 109   1 128 153 124 172 160  78 149  43   8 133\n",
      "  76 111 136  30 163 146  34 148 123 162  66 118 115  86  80 150 154  19\n",
      " 100 167  38 165 174 175 155  41  51  44  69  82  42 132 119   7  52  29\n",
      "  83  18 151 130  61 171  95  12 126  11  68  10  23 161 156 131  60  94\n",
      "  39  21   0  65  49  73  26 138   6  24  97 144  64  53  70 101 164  32\n",
      " 113  63] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "            *** MODEL TRAINING AND VALIDATION PHASE FOR TEST 2 \n",
      "            TRAINING 1 START... XXXXX \n",
      "            ***************************************************************************************************\n",
      "            Validation=> 182 ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            Training=> 182 ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            \n",
      "\n",
      "            *** ML MODEL FOR TEST:2, TRAINING:1 AND MODEL: LR \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: logistic_regression_parameters\n",
      "All parameters: ['penalty', 'solver', 'max_iter', 'C'], [['l2', 'elasticnet'], [50, 100, 130, 150, 170, 200], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]], [0, 2, 3]\n",
      "Parameters:  {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\"\n",
      "            Selected features: ['W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "            \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: LR - {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 84 candidates, totalling 420 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[88 13]\n",
      " [10 71]]\n",
      "[[71 10 13 88]\n",
      " [88 13 10 71]]\n",
      "[[159  23]\n",
      " [ 23 159]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.36263736263736\n",
      " Precision = [0.8452381  0.89795918]\n",
      " Recall = [0.87654321 0.87128713]\n",
      " F1 score = [0.86060606 0.88442211]\n",
      " AUC score = 87.39151692947073\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.36263736263736\n",
      " Precision = [0.89795918 0.8452381 ]\n",
      " Recall = [0.87128713 0.87654321]\n",
      " F1 score = [0.88442211 0.86060606]\n",
      " AUC score = 87.39151692947073\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[88 13]\n",
      " [10 71]]\n",
      " Accuracy (acc): 87.363\n",
      " Precision (prc): 84.524\n",
      " Recall (rec): 87.654\n",
      " Sensitivity (sns): 87.654\n",
      " Specificity (spc): 87.129\n",
      " F1 Score (f1s): 86.061\n",
      " ROC AUC (AUC): 0.874\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: LogisticRegression(max_iter=50) \n",
      "        Best parameters of the model: {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
      "\n",
      "                             model  \\\n",
      "0  LogisticRegression(max_iter=50)   \n",
      "\n",
      "                              model_parameters  model_scores  \\\n",
      "0  {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'}         88.01   \n",
      "\n",
      "       confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
      "0  [[88, 13], [10, 71]]    87.363     84.524  87.654       87.654   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       87.129    86.061    0.874   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:2, TRAINING:1 AND MODEL: SVC \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: support_vector_classifier_parameters\n",
      "All parameters: ['probability', 'C', 'kernel', 'gamma', 'degree', 'class_weightdict'], [[True], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], ['linear', 'rbf', 'poly']], [0, 1, 2]\n",
      "Parameters:  {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "210 fits failed out of a total of 420.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.80735294        nan 0.80735294        nan 0.80735294        nan\n",
      " 0.80735294        nan 0.80735294        nan 0.80735294        nan\n",
      " 0.83235294        nan 0.83235294        nan 0.83235294        nan\n",
      " 0.83235294        nan 0.83235294        nan 0.83235294        nan\n",
      " 0.86838235        nan 0.86838235        nan 0.86838235        nan\n",
      " 0.86838235        nan 0.86838235        nan 0.86838235        nan\n",
      " 0.88014706        nan 0.88014706        nan 0.88014706        nan\n",
      " 0.88014706        nan 0.88014706        nan 0.88014706        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.82134615        nan 0.82134615        nan 0.82134615        nan\n",
      " 0.82134615        nan 0.82134615        nan 0.82134615        nan\n",
      " 0.8275            nan 0.8275            nan 0.8275            nan\n",
      " 0.8275            nan 0.8275            nan 0.8275            nan\n",
      " 0.86139423        nan 0.86139423        nan 0.86139423        nan\n",
      " 0.86139423        nan 0.86139423        nan 0.86139423        nan\n",
      " 0.87370192        nan 0.87370192        nan 0.87370192        nan\n",
      " 0.87370192        nan 0.87370192        nan 0.87370192        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.8395254         nan 0.8395254         nan 0.8395254         nan\n",
      " 0.8395254         nan 0.8395254         nan 0.8395254         nan\n",
      " 0.82952562        nan 0.82952562        nan 0.82952562        nan\n",
      " 0.82952562        nan 0.82952562        nan 0.82952562        nan\n",
      " 0.85071407        nan 0.85071407        nan 0.85071407        nan\n",
      " 0.85071407        nan 0.85071407        nan 0.85071407        nan\n",
      " 0.85597723        nan 0.85597723        nan 0.85597723        nan\n",
      " 0.85597723        nan 0.85597723        nan 0.85597723        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.83512023        nan 0.83512023        nan 0.83512023        nan\n",
      " 0.83512023        nan 0.83512023        nan 0.83512023        nan\n",
      " 0.83364128        nan 0.83364128        nan 0.83364128        nan\n",
      " 0.83364128        nan 0.83364128        nan 0.83364128        nan\n",
      " 0.85080054        nan 0.85080054        nan 0.85080054        nan\n",
      " 0.85080054        nan 0.85080054        nan 0.85080054        nan\n",
      " 0.85552991        nan 0.85552991        nan 0.85552991        nan\n",
      " 0.85552991        nan 0.85552991        nan 0.85552991        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: SVC - {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[80 21]\n",
      " [11 70]]\n",
      "[[70 11 21 80]\n",
      " [80 21 11 70]]\n",
      "[[150  32]\n",
      " [ 32 150]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 82.41758241758241\n",
      " Precision = [0.76923077 0.87912088]\n",
      " Recall = [0.86419753 0.79207921]\n",
      " F1 score = [0.81395349 0.83333333]\n",
      " AUC score = 82.81383693924948\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 82.41758241758241\n",
      " Precision = [0.87912088 0.76923077]\n",
      " Recall = [0.79207921 0.86419753]\n",
      " F1 score = [0.83333333 0.81395349]\n",
      " AUC score = 82.81383693924948\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[80 21]\n",
      " [11 70]]\n",
      " Accuracy (acc): 82.418\n",
      " Precision (prc): 76.923\n",
      " Recall (rec): 86.42\n",
      " Sensitivity (sns): 86.42\n",
      " Specificity (spc): 79.208\n",
      " F1 Score (f1s): 81.395\n",
      " ROC AUC (AUC): 0.828\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: SVC(C=0.1, kernel='linear', probability=True) \n",
      "        Best parameters of the model: {'C': 0.1, 'kernel': 'linear', 'probability': True} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
      "\n",
      "                                           model  \\\n",
      "0  SVC(C=0.1, kernel='linear', probability=True)   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'C': 0.1, 'kernel': 'linear', 'probability': ...         86.84   \n",
      "\n",
      "       confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
      "0  [[80, 21], [11, 70]]    82.418     76.923   86.42        86.42   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       79.208    81.395    0.828   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:2, TRAINING:1 AND MODEL: NB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: naive_bayes_parameters\n",
      "All parameters: ['var_smoothing'], [[1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]], [0]\n",
      "Parameters:  {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: NB - {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[73 28]\n",
      " [11 70]]\n",
      "[[70 11 28 73]\n",
      " [73 28 11 70]]\n",
      "[[143  39]\n",
      " [ 39 143]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 78.57142857142857\n",
      " Precision = [0.71428571 0.86904762]\n",
      " Recall = [0.86419753 0.72277228]\n",
      " F1 score = [0.78212291 0.78918919]\n",
      " AUC score = 79.34849040459602\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 78.57142857142857\n",
      " Precision = [0.86904762 0.71428571]\n",
      " Recall = [0.72277228 0.86419753]\n",
      " F1 score = [0.78918919 0.78212291]\n",
      " AUC score = 79.348490404596\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[73 28]\n",
      " [11 70]]\n",
      " Accuracy (acc): 78.571\n",
      " Precision (prc): 71.429\n",
      " Recall (rec): 86.42\n",
      " Sensitivity (sns): 86.42\n",
      " Specificity (spc): 72.277\n",
      " F1 Score (f1s): 78.212\n",
      " ROC AUC (AUC): 0.793\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GaussianNB(var_smoothing=1.0) \n",
      "        Best parameters of the model: {'var_smoothing': 1.0} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
      "\n",
      "                           model        model_parameters  model_scores  \\\n",
      "0  GaussianNB(var_smoothing=1.0)  {'var_smoothing': 1.0}         86.84   \n",
      "\n",
      "       confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
      "0  [[73, 28], [11, 70]]    78.571     71.429   86.42        86.42   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       72.277    78.212    0.793   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:2, TRAINING:1 AND MODEL: KNN \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: k_nearest_neighbors_parameters\n",
      "All parameters: ['n_neighbors', 'p', 'metric', 'n_splits'], [[2, 3, 5, 10, 15, 25, 35], ['manhattan', 'minkowski', 'euclidean']], [0, 2]\n",
      "Parameters:  {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: KNN - {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[84 17]\n",
      " [11 70]]\n",
      "[[70 11 17 84]\n",
      " [84 17 11 70]]\n",
      "[[154  28]\n",
      " [ 28 154]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 84.61538461538461\n",
      " Precision = [0.8045977  0.88421053]\n",
      " Recall = [0.86419753 0.83168317]\n",
      " F1 score = [0.83333333 0.85714286]\n",
      " AUC score = 84.79403495905147\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 84.61538461538461\n",
      " Precision = [0.88421053 0.8045977 ]\n",
      " Recall = [0.83168317 0.86419753]\n",
      " F1 score = [0.85714286 0.83333333]\n",
      " AUC score = 84.79403495905146\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[84 17]\n",
      " [11 70]]\n",
      " Accuracy (acc): 84.615\n",
      " Precision (prc): 80.46\n",
      " Recall (rec): 86.42\n",
      " Sensitivity (sns): 86.42\n",
      " Specificity (spc): 83.168\n",
      " F1 Score (f1s): 83.333\n",
      " ROC AUC (AUC): 0.848\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: KNeighborsClassifier(metric='manhattan', n_neighbors=35) \n",
      "        Best parameters of the model: {'metric': 'manhattan', 'n_neighbors': 35} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
      "\n",
      "                                               model  \\\n",
      "0  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
      "\n",
      "                             model_parameters  model_scores  \\\n",
      "0  {'metric': 'manhattan', 'n_neighbors': 35}         89.19   \n",
      "\n",
      "       confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
      "0  [[84, 17], [11, 70]]    84.615      80.46   86.42        86.42   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       83.168    83.333    0.848   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:2, TRAINING:1 AND MODEL: DT \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: decision_tree_parameters\n",
      "All parameters: ['max_depth', 'criterion', 'splitter', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: DT - {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[89 12]\n",
      " [ 5 76]]\n",
      "[[76  5 12 89]\n",
      " [89 12  5 76]]\n",
      "[[165  17]\n",
      " [ 17 165]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 90.65934065934066\n",
      " Precision = [0.86363636 0.94680851]\n",
      " Recall = [0.9382716  0.88118812]\n",
      " F1 score = [0.89940828 0.91282051]\n",
      " AUC score = 90.97298618750762\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 90.65934065934066\n",
      " Precision = [0.94680851 0.86363636]\n",
      " Recall = [0.88118812 0.9382716 ]\n",
      " F1 score = [0.91282051 0.89940828]\n",
      " AUC score = 90.97298618750763\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[89 12]\n",
      " [ 5 76]]\n",
      " Accuracy (acc): 90.659\n",
      " Precision (prc): 86.364\n",
      " Recall (rec): 93.827\n",
      " Sensitivity (sns): 93.827\n",
      " Specificity (spc): 88.119\n",
      " F1 Score (f1s): 89.941\n",
      " ROC AUC (AUC): 0.91\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: DecisionTreeClassifier(criterion='entropy', max_depth=3) \n",
      "        Best parameters of the model: {'criterion': 'entropy', 'max_depth': 3} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
      "\n",
      "                                               model  \\\n",
      "0  DecisionTreeClassifier(criterion='entropy', ma...   \n",
      "\n",
      "                           model_parameters  model_scores  \\\n",
      "0  {'criterion': 'entropy', 'max_depth': 3}         86.84   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[89, 12], [5, 76]]    90.659     86.364  93.827       93.827       88.119   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    89.941     0.91   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:2, TRAINING:1 AND MODEL: RF \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: random_forest_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'criterion', 'splitter', 'max_features', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.79411765 0.81911765 0.76764706 0.80588235 0.74338235 0.74264706\n",
      " 0.73014706 0.78088235 0.79338235 0.83161765 0.86838235 0.85588235\n",
      " 0.73161765 0.75661765 0.78088235 0.75661765 0.71838235 0.73161765\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.90144231 0.95375    0.97846154 1.         1.         1.\n",
      " 1.         1.         1.         0.89524038 0.96307692 0.99692308\n",
      " 0.98461538 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.81003926 0.80738285 0.77318868 0.78338995 0.77248827 0.78715047\n",
      " 0.7579058  0.80211522 0.80300232 0.8246683  0.8133206  0.80468609\n",
      " 0.74074732 0.75574732 0.77829004 0.75601042 0.7270473  0.73812934\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.89011715 0.91142877 0.96825444 0.99259259 1.         1.\n",
      " 1.         1.         1.         0.88431691 0.91330608 0.95508805\n",
      " 0.97599992 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: RF - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[101   0]\n",
      " [  0  81]]\n",
      "[[ 81   0   0 101]\n",
      " [101   0   0  81]]\n",
      "[[182   0]\n",
      " [  0 182]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 100.0\n",
      " Precision = [1. 1.]\n",
      " Recall = [1. 1.]\n",
      " F1 score = [1. 1.]\n",
      " AUC score = 100.0\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 100.0\n",
      " Precision = [1. 1.]\n",
      " Recall = [1. 1.]\n",
      " F1 score = [1. 1.]\n",
      " AUC score = 100.0\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[101   0]\n",
      " [  0  81]]\n",
      " Accuracy (acc): 100.0\n",
      " Precision (prc): 100.0\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 100.0\n",
      " F1 Score (f1s): 100.0\n",
      " ROC AUC (AUC): 1.0\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "405 fits failed out of a total of 1215.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 467, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.83014706 0.84411765 0.89264706 0.86764706 0.86764706 0.88014706\n",
      " 0.88014706 0.88014706 0.88014706 0.82941176 0.86838235 0.86764706\n",
      " 0.86764706 0.88014706 0.88014706 0.88014706 0.88014706 0.88014706\n",
      " 0.78014706 0.83014706 0.84338235 0.85441176 0.89264706 0.89264706\n",
      " 0.89264706 0.89264706 0.89191176 0.81764706 0.84264706 0.85441176\n",
      " 0.88014706 0.86764706 0.89191176 0.85514706 0.86764706 0.88014706\n",
      " 0.74411765 0.78161765 0.80514706 0.87941176 0.85514706 0.87941176\n",
      " 0.86764706 0.88014706 0.84264706 0.78014706 0.84191176 0.81764706\n",
      " 0.85441176 0.85441176 0.88014706 0.85514706 0.86764706 0.89191176\n",
      " 0.76764706 0.87941176 0.71985294 0.80514706 0.84264706 0.86764706\n",
      " 0.83014706 0.86764706 0.88014706 0.77941176 0.84264706 0.83014706\n",
      " 0.87941176 0.86764706 0.85514706 0.90441176 0.86764706 0.86691176\n",
      " 0.79338235 0.81838235 0.80441176 0.86764706 0.86764706 0.84264706\n",
      " 0.88014706 0.89191176 0.86764706 0.89191176 0.84485294 0.88014706\n",
      " 0.88014706 0.89264706 0.86764706 0.88014706 0.88014706 0.88014706\n",
      " 0.83161765 0.80661765 0.89264706 0.88014706 0.89191176 0.88014706\n",
      " 0.88014706 0.88014706 0.88014706 0.78014706 0.82941176 0.84264706\n",
      " 0.90441176 0.90441176 0.88014706 0.88014706 0.88014706 0.90441176\n",
      " 0.84264706 0.84191176 0.83014706 0.88014706 0.89191176 0.87941176\n",
      " 0.89191176 0.89191176 0.88014706 0.81764706 0.79264706 0.83014706\n",
      " 0.87941176 0.88014706 0.87941176 0.86764706 0.86764706 0.85514706\n",
      " 0.79264706 0.81764706 0.83014706 0.88014706 0.86691176 0.87941176\n",
      " 0.88014706 0.87941176 0.89191176 0.75588235 0.89191176 0.82941176\n",
      " 0.86691176 0.89264706 0.85514706 0.88014706 0.88014706 0.88014706\n",
      " 0.75661765 0.84264706 0.80514706 0.86764706 0.85514706 0.84264706\n",
      " 0.87941176 0.86691176 0.87941176 0.78014706 0.78014706 0.81691176\n",
      " 0.85514706 0.88014706 0.85514706 0.88014706 0.87941176 0.88014706\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.88600962 0.87985577 0.90451923 0.88293269 0.88605769 0.90759615\n",
      " 0.89216346 0.88600962 0.88908654 0.90456731 0.91990385 0.92307692\n",
      " 0.92615385 0.94153846 0.91985577 0.92302885 0.92605769 0.92302885\n",
      " 0.94461538 0.96       0.98461538 0.98461538 0.99076923 1.\n",
      " 1.         0.99076923 1.         0.95995192 0.98456731 0.99076923\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.96923077 0.98769231 0.98461538 1.         0.99692308 0.99692308\n",
      " 1.         1.         1.         0.96610577 0.96918269 0.97836538\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.97230769 0.99076923 0.99379808 0.99692308 1.         1.\n",
      " 1.         1.         1.         0.95692308 0.98461538 0.98153846\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.95990385 0.98149038 0.98153846 0.99076923 1.         1.\n",
      " 1.         1.         1.         0.87985577 0.90134615 0.89528846\n",
      " 0.89831731 0.88908654 0.89216346 0.89841346 0.88908654 0.89216346\n",
      " 0.89826923 0.92293269 0.93836538 0.93225962 0.95076923 0.92923077\n",
      " 0.93538462 0.93225962 0.92923077 0.94461538 0.95692308 0.96605769\n",
      " 0.99076923 0.99692308 0.99384615 1.         0.99692308 0.99692308\n",
      " 0.97225962 0.98144231 0.98461538 0.99072115 0.99384615 1.\n",
      " 1.         1.         1.         0.97230769 0.98144231 0.99384615\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.96923077 0.98153846 0.98149038 0.996875   1.         1.\n",
      " 1.         1.         1.         0.95995192 0.97533654 0.98769231\n",
      " 1.         0.99692308 0.99692308 1.         1.         1.\n",
      " 0.95375    0.99072115 0.99384615 1.         1.         1.\n",
      " 1.         1.         1.         0.95692308 0.97841346 0.98153846\n",
      " 0.99379808 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.84558302 0.8379453  0.88666667 0.87041971 0.86679813 0.85708812\n",
      " 0.87931034 0.87931034 0.87931034 0.83679654 0.86646214 0.85131351\n",
      " 0.85838209 0.87454844 0.87324974 0.87931034 0.88187445 0.87581384\n",
      " 0.7902381  0.84232288 0.85035873 0.83095282 0.87860936 0.86708479\n",
      " 0.87490196 0.87246294 0.87324675 0.81396104 0.8298125  0.84535588\n",
      " 0.87215775 0.86679813 0.88163593 0.85991641 0.8648728  0.87673077\n",
      " 0.72585627 0.79232043 0.82626728 0.8492073  0.86211982 0.86963741\n",
      " 0.86166667 0.85589744 0.85862069 0.78495551 0.81481183 0.84543372\n",
      " 0.86185098 0.85364761 0.87673077 0.83484594 0.85922764 0.88254064\n",
      " 0.77226265 0.85795852 0.75127203 0.82905523 0.85708637 0.87089431\n",
      " 0.83777603 0.85636364 0.88021505 0.78121511 0.84791789 0.84966088\n",
      " 0.86601313 0.85560606 0.85747738 0.86570091 0.86166667 0.87073892\n",
      " 0.77612371 0.82801993 0.83727099 0.84893352 0.8643591  0.84271505\n",
      " 0.87415445 0.87324675 0.87089431 0.86054284 0.82630631 0.87081071\n",
      " 0.87010974 0.88666667 0.87285873 0.87931034 0.87931034 0.88187445\n",
      " 0.83805556 0.83131407 0.89192982 0.88187445 0.88407225 0.84984429\n",
      " 0.87931034 0.87931034 0.87931034 0.80302545 0.82163684 0.81804779\n",
      " 0.89398374 0.87966387 0.85333333 0.87172764 0.87172764 0.88899225\n",
      " 0.8157994  0.84186021 0.83256098 0.87415445 0.87077593 0.84899225\n",
      " 0.88497696 0.87892857 0.86604582 0.80974483 0.80998122 0.82323057\n",
      " 0.86054284 0.86848485 0.8745898  0.87333333 0.85316704 0.8544086\n",
      " 0.79383803 0.82654559 0.84312567 0.85463762 0.87329408 0.86959831\n",
      " 0.87673077 0.87207373 0.86789916 0.76646901 0.87648003 0.84748763\n",
      " 0.8586568  0.86102418 0.84824974 0.87172764 0.85428112 0.87777603\n",
      " 0.80067197 0.8330618  0.81834673 0.85922764 0.86597701 0.83843995\n",
      " 0.87207373 0.85626498 0.86963741 0.78718992 0.81069124 0.82920772\n",
      " 0.83818182 0.87171542 0.85236058 0.86848485 0.86601313 0.87416667\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.87351991 0.86586329 0.883675   0.87189173 0.87348389 0.88420149\n",
      " 0.87704733 0.87228882 0.87526369 0.87826218 0.89368683 0.89371954\n",
      " 0.89516076 0.90739871 0.89473685 0.89637434 0.89548884 0.89761086\n",
      " 0.93866032 0.94145192 0.95577525 0.95975145 0.96008617 0.96191498\n",
      " 0.96339314 0.96576851 0.97474651 0.95556483 0.97570548 0.98480222\n",
      " 0.99541985 0.99391626 0.99544298 0.99847328 0.9969697  0.99847328\n",
      " 0.97058868 0.98616559 0.98761851 1.         0.99692289 0.99844961\n",
      " 1.         1.         1.         0.96927226 0.96922875 0.97997872\n",
      " 0.99544298 1.         1.         1.         1.         1.\n",
      " 0.9798185  0.98333797 0.99382137 0.99083969 0.99694656 1.\n",
      " 1.         1.         1.         0.96864927 0.9787162  0.98909884\n",
      " 0.99393939 0.99847328 1.         1.         1.         1.\n",
      " 0.95547004 0.98602022 0.98597384 0.99079235 0.9969697  1.\n",
      " 1.         1.         1.         0.84964737 0.86965466 0.87344724\n",
      " 0.87916741 0.87404321 0.8770021  0.88149956 0.87404321 0.87577141\n",
      " 0.87930936 0.89793731 0.90468036 0.89997972 0.90722005 0.89700767\n",
      " 0.90019608 0.90007556 0.89834423 0.92523183 0.94556024 0.95182097\n",
      " 0.96583485 0.96045321 0.9687762  0.96618449 0.97182535 0.96318083\n",
      " 0.95490026 0.97267    0.98606432 0.98776007 0.99534884 0.99544298\n",
      " 0.99544298 0.99694656 1.         0.97225908 0.98005392 0.990837\n",
      " 0.99090909 0.99847328 1.         1.         1.         1.\n",
      " 0.9633538  0.97850816 0.9891225  0.99689848 0.99694656 1.\n",
      " 1.         1.         1.         0.96443502 0.96956257 0.98769122\n",
      " 0.9969697  0.99844961 0.99844961 1.         1.         1.\n",
      " 0.95679307 0.98174556 0.996875   0.99694656 0.99544298 1.\n",
      " 1.         1.         1.         0.94826276 0.98141259 0.98907461\n",
      " 0.99079107 0.99694656 1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: RandomForestClassifier(max_depth=25, n_estimators=50) \n",
      "        Best parameters of the model: {'criterion': 'gini', 'max_depth': 25, 'n_estimators': 50} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
      "\n",
      "                                               model  \\\n",
      "0  (DecisionTreeClassifier(max_depth=25, max_feat...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'criterion': 'gini', 'max_depth': 25, 'n_esti...         90.44   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[101, 0], [0, 81]]     100.0      100.0   100.0        100.0        100.0   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0     100.0      1.0   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:2, TRAINING:1 AND MODEL: GB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: gradient_boosting_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'learning_rate', 'max_features', 'loss', 'min_samples_split', 'min_samples_leaf'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.5, 1.0]], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: GB - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 405 candidates, totalling 2025 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[89 12]\n",
      " [ 5 76]]\n",
      "[[76  5 12 89]\n",
      " [89 12  5 76]]\n",
      "[[165  17]\n",
      " [ 17 165]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 90.65934065934066\n",
      " Precision = [0.86363636 0.94680851]\n",
      " Recall = [0.9382716  0.88118812]\n",
      " F1 score = [0.89940828 0.91282051]\n",
      " AUC score = 90.97298618750762\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 90.65934065934066\n",
      " Precision = [0.94680851 0.86363636]\n",
      " Recall = [0.88118812 0.9382716 ]\n",
      " F1 score = [0.91282051 0.89940828]\n",
      " AUC score = 90.97298618750763\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[89 12]\n",
      " [ 5 76]]\n",
      " Accuracy (acc): 90.659\n",
      " Precision (prc): 86.364\n",
      " Recall (rec): 93.827\n",
      " Sensitivity (sns): 93.827\n",
      " Specificity (spc): 88.119\n",
      " F1 Score (f1s): 89.941\n",
      " ROC AUC (AUC): 0.91\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=50) \n",
      "        Best parameters of the model: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 50} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
      "\n",
      "                                               model  \\\n",
      "0  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'learning_rate': 0.01, 'max_depth': 2, 'n_est...         89.26   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[89, 12], [5, 76]]    90.659     86.364  93.827       93.827       88.119   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    89.941     0.91   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:2, TRAINING:1 AND MODEL: XGB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: xtreme_gradient_boosting_parameters\n",
      "All parameters: ['max_depth', 'eta', 'max_leaves'], [[2, 3, 6, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: XGB - {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[101   0]\n",
      " [  0  81]]\n",
      "[[ 81   0   0 101]\n",
      " [101   0   0  81]]\n",
      "[[182   0]\n",
      " [  0 182]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 100.0\n",
      " Precision = [1. 1.]\n",
      " Recall = [1. 1.]\n",
      " F1 score = [1. 1.]\n",
      " AUC score = 100.0\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 100.0\n",
      " Precision = [1. 1.]\n",
      " Recall = [1. 1.]\n",
      " F1 score = [1. 1.]\n",
      " AUC score = 100.0\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[101   0]\n",
      " [  0  81]]\n",
      " Accuracy (acc): 100.0\n",
      " Precision (prc): 100.0\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 100.0\n",
      " F1 Score (f1s): 100.0\n",
      " ROC AUC (AUC): 1.0\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False, eta=0.05,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.0500000007, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, ...) \n",
      "        Best parameters of the model: {'eta': 0.05, 'max_depth': 6} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
      "\n",
      "                                               model  \\\n",
      "0  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
      "\n",
      "                model_parameters  model_scores     confusion_matrix  accuracy  \\\n",
      "0  {'eta': 0.05, 'max_depth': 6}         89.19  [[101, 0], [0, 81]]     100.0   \n",
      "\n",
      "   precision  recall  sensitivity  specificity  f1_score  roc_auc  \n",
      "0      100.0   100.0        100.0        100.0     100.0      1.0   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            Best model index calculation  \n",
      "            \n",
      "WWWWWWWWWWWWWWWWWWW recall    Training_No  Model_No Model_Name  \\\n",
      "0            1         1         LR   \n",
      "1            1         2        SVC   \n",
      "2            1         3         NB   \n",
      "3            1         4        KNN   \n",
      "4            1         5         DT   \n",
      "5            1         6         RF   \n",
      "6            1         7         GB   \n",
      "7            1         8        XGB   \n",
      "\n",
      "                                              method  \\\n",
      "0  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
      "1  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
      "2  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
      "3  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
      "4  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
      "5  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
      "6  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
      "7  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
      "\n",
      "                                               model  \\\n",
      "0                    LogisticRegression(max_iter=50)   \n",
      "1      SVC(C=0.1, kernel='linear', probability=True)   \n",
      "2                      GaussianNB(var_smoothing=1.0)   \n",
      "3  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
      "4  DecisionTreeClassifier(criterion='entropy', ma...   \n",
      "5  (DecisionTreeClassifier(max_depth=25, max_feat...   \n",
      "6  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
      "7  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0        {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'}         88.01   \n",
      "1  {'C': 0.1, 'kernel': 'linear', 'probability': ...         86.84   \n",
      "2                             {'var_smoothing': 1.0}         86.84   \n",
      "3         {'metric': 'manhattan', 'n_neighbors': 35}         89.19   \n",
      "4           {'criterion': 'entropy', 'max_depth': 3}         86.84   \n",
      "5  {'criterion': 'gini', 'max_depth': 25, 'n_esti...         90.44   \n",
      "6  {'learning_rate': 0.01, 'max_depth': 2, 'n_est...         89.26   \n",
      "7                      {'eta': 0.05, 'max_depth': 6}         89.19   \n",
      "\n",
      "       confusion_matrix  accuracy  precision   recall  sensitivity  \\\n",
      "0  [[88, 13], [10, 71]]    87.363     84.524   87.654       87.654   \n",
      "1  [[80, 21], [11, 70]]    82.418     76.923   86.420       86.420   \n",
      "2  [[73, 28], [11, 70]]    78.571     71.429   86.420       86.420   \n",
      "3  [[84, 17], [11, 70]]    84.615     80.460   86.420       86.420   \n",
      "4   [[89, 12], [5, 76]]    90.659     86.364   93.827       93.827   \n",
      "5   [[101, 0], [0, 81]]   100.000    100.000  100.000      100.000   \n",
      "6   [[89, 12], [5, 76]]    90.659     86.364   93.827       93.827   \n",
      "7   [[101, 0], [0, 81]]   100.000    100.000  100.000      100.000   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       87.129    86.061    0.874  \n",
      "1       79.208    81.395    0.828  \n",
      "2       72.277    78.212    0.793  \n",
      "3       83.168    83.333    0.848  \n",
      "4       88.119    89.941    0.910  \n",
      "5      100.000   100.000    1.000  \n",
      "6       88.119    89.941    0.910  \n",
      "7      100.000   100.000    1.000  \n",
      "\n",
      "            ***************************************************************************************************\n",
      "            TRAINING 1 END... \n",
      "            \n",
      "train- tri, tmp_train_ids, tmp_val_ids, tr_dat, val_dat 1 [ 31 153 137  51  83   9  99  58  77  15 177  70 180 167  86 174 114 136\n",
      "  52 101  78 162 126  48  62  47   8  64   4  89 173  10  88 139  19 179\n",
      "  33  93  74  73 170  75 175  23 171 100   5   7 144  53  68  24 141  91\n",
      "  84 131 155  29  26 122 118  41 166 145 104 127   3  38  39 148  81 154\n",
      " 146 152 172  57 160 140 134  87 176   0 105   1  82  66  25  49  34  32\n",
      "  69 117  17 168   6 103 125  59 109  18 133  43  98 178  37 132 156 151\n",
      " 119  79 130  60 150 115 124 161 113 169 121  21  92 106 181  65  76 135\n",
      " 164 147  27  30  55  54 123  11 110 159 107  80 129  50  90 165  71  56\n",
      "  45 116 138 111  16  96 120 163  40 102  95  28  46  67  20  72  97  94\n",
      "  35  12  36  85  13 157  42 142 149 108  61  14   2 143  63  22 128 112\n",
      " 158  44] [ 31 153 137  51  83   9  99  58  77  15 177  70 180 167  86 174 114 136\n",
      "  52 101  78 162 126  48  62  47   8  64   4  89 173  10  88 139  19 179\n",
      "  33  93  74  73 170  75 175  23 171 100   5   7 144  53  68  24 141  91\n",
      "  84 131 155  29  26 122 118  41 166 145 104 127   3  38  39 148  81 154\n",
      " 146 152 172  57 160 140 134  87 176   0 105   1  82  66  25  49  34  32\n",
      "  69 117  17 168   6 103 125  59 109  18 133  43  98 178  37 132 156 151\n",
      " 119  79 130  60 150 115 124 161 113 169 121  21  92 106 181  65  76 135\n",
      " 164 147  27  30  55  54 123  11 110 159 107  80 129  50  90 165  71  56\n",
      "  45 116 138 111  16  96 120 163  40 102  95  28  46  67  20  72  97  94\n",
      "  35  12  36  85  13 157  42 142 149 108  61  14   2 143  63  22 128 112\n",
      " 158  44] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "            *** MODEL TRAINING AND VALIDATION PHASE FOR TEST 2 \n",
      "            TRAINING 2 START... XXXXX \n",
      "            ***************************************************************************************************\n",
      "            Validation=> 182 ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            Training=> 182 ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            \n",
      "\n",
      "            *** ML MODEL FOR TEST:2, TRAINING:2 AND MODEL: LR \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: logistic_regression_parameters\n",
      "All parameters: ['penalty', 'solver', 'max_iter', 'C'], [['l2', 'elasticnet'], [50, 100, 130, 150, 170, 200], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]], [0, 2, 3]\n",
      "Parameters:  {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: LR - {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 84 candidates, totalling 420 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[88 13]\n",
      " [10 71]]\n",
      "[[71 10 13 88]\n",
      " [88 13 10 71]]\n",
      "[[159  23]\n",
      " [ 23 159]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.36263736263736\n",
      " Precision = [0.8452381  0.89795918]\n",
      " Recall = [0.87654321 0.87128713]\n",
      " F1 score = [0.86060606 0.88442211]\n",
      " AUC score = 87.39151692947073\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.36263736263736\n",
      " Precision = [0.89795918 0.8452381 ]\n",
      " Recall = [0.87128713 0.87654321]\n",
      " F1 score = [0.88442211 0.86060606]\n",
      " AUC score = 87.39151692947073\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[88 13]\n",
      " [10 71]]\n",
      " Accuracy (acc): 87.363\n",
      " Precision (prc): 84.524\n",
      " Recall (rec): 87.654\n",
      " Sensitivity (sns): 87.654\n",
      " Specificity (spc): 87.129\n",
      " F1 Score (f1s): 86.061\n",
      " ROC AUC (AUC): 0.874\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: LogisticRegression(max_iter=50) \n",
      "        Best parameters of the model: {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
      "\n",
      "                             model  \\\n",
      "0  LogisticRegression(max_iter=50)   \n",
      "\n",
      "                              model_parameters  model_scores  \\\n",
      "0  {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'}         88.01   \n",
      "\n",
      "       confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
      "0  [[88, 13], [10, 71]]    87.363     84.524  87.654       87.654   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       87.129    86.061    0.874   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:2, TRAINING:2 AND MODEL: SVC \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: support_vector_classifier_parameters\n",
      "All parameters: ['probability', 'C', 'kernel', 'gamma', 'degree', 'class_weightdict'], [[True], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], ['linear', 'rbf', 'poly']], [0, 1, 2]\n",
      "Parameters:  {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "210 fits failed out of a total of 420.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.80735294        nan 0.80735294        nan 0.80735294        nan\n",
      " 0.80735294        nan 0.80735294        nan 0.80735294        nan\n",
      " 0.83235294        nan 0.83235294        nan 0.83235294        nan\n",
      " 0.83235294        nan 0.83235294        nan 0.83235294        nan\n",
      " 0.86838235        nan 0.86838235        nan 0.86838235        nan\n",
      " 0.86838235        nan 0.86838235        nan 0.86838235        nan\n",
      " 0.88014706        nan 0.88014706        nan 0.88014706        nan\n",
      " 0.88014706        nan 0.88014706        nan 0.88014706        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.82134615        nan 0.82134615        nan 0.82134615        nan\n",
      " 0.82134615        nan 0.82134615        nan 0.82134615        nan\n",
      " 0.8275            nan 0.8275            nan 0.8275            nan\n",
      " 0.8275            nan 0.8275            nan 0.8275            nan\n",
      " 0.86139423        nan 0.86139423        nan 0.86139423        nan\n",
      " 0.86139423        nan 0.86139423        nan 0.86139423        nan\n",
      " 0.87370192        nan 0.87370192        nan 0.87370192        nan\n",
      " 0.87370192        nan 0.87370192        nan 0.87370192        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.8395254         nan 0.8395254         nan 0.8395254         nan\n",
      " 0.8395254         nan 0.8395254         nan 0.8395254         nan\n",
      " 0.82952562        nan 0.82952562        nan 0.82952562        nan\n",
      " 0.82952562        nan 0.82952562        nan 0.82952562        nan\n",
      " 0.85071407        nan 0.85071407        nan 0.85071407        nan\n",
      " 0.85071407        nan 0.85071407        nan 0.85071407        nan\n",
      " 0.85597723        nan 0.85597723        nan 0.85597723        nan\n",
      " 0.85597723        nan 0.85597723        nan 0.85597723        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.83512023        nan 0.83512023        nan 0.83512023        nan\n",
      " 0.83512023        nan 0.83512023        nan 0.83512023        nan\n",
      " 0.83364128        nan 0.83364128        nan 0.83364128        nan\n",
      " 0.83364128        nan 0.83364128        nan 0.83364128        nan\n",
      " 0.85080054        nan 0.85080054        nan 0.85080054        nan\n",
      " 0.85080054        nan 0.85080054        nan 0.85080054        nan\n",
      " 0.85552991        nan 0.85552991        nan 0.85552991        nan\n",
      " 0.85552991        nan 0.85552991        nan 0.85552991        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: SVC - {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[80 21]\n",
      " [11 70]]\n",
      "[[70 11 21 80]\n",
      " [80 21 11 70]]\n",
      "[[150  32]\n",
      " [ 32 150]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 82.41758241758241\n",
      " Precision = [0.76923077 0.87912088]\n",
      " Recall = [0.86419753 0.79207921]\n",
      " F1 score = [0.81395349 0.83333333]\n",
      " AUC score = 82.81383693924948\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 82.41758241758241\n",
      " Precision = [0.87912088 0.76923077]\n",
      " Recall = [0.79207921 0.86419753]\n",
      " F1 score = [0.83333333 0.81395349]\n",
      " AUC score = 82.81383693924948\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[80 21]\n",
      " [11 70]]\n",
      " Accuracy (acc): 82.418\n",
      " Precision (prc): 76.923\n",
      " Recall (rec): 86.42\n",
      " Sensitivity (sns): 86.42\n",
      " Specificity (spc): 79.208\n",
      " F1 Score (f1s): 81.395\n",
      " ROC AUC (AUC): 0.828\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: SVC(C=0.1, kernel='linear', probability=True) \n",
      "        Best parameters of the model: {'C': 0.1, 'kernel': 'linear', 'probability': True} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
      "\n",
      "                                           model  \\\n",
      "0  SVC(C=0.1, kernel='linear', probability=True)   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'C': 0.1, 'kernel': 'linear', 'probability': ...         86.84   \n",
      "\n",
      "       confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
      "0  [[80, 21], [11, 70]]    82.418     76.923   86.42        86.42   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       79.208    81.395    0.828   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:2, TRAINING:2 AND MODEL: NB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: naive_bayes_parameters\n",
      "All parameters: ['var_smoothing'], [[1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]], [0]\n",
      "Parameters:  {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: NB - {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[73 28]\n",
      " [11 70]]\n",
      "[[70 11 28 73]\n",
      " [73 28 11 70]]\n",
      "[[143  39]\n",
      " [ 39 143]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 78.57142857142857\n",
      " Precision = [0.71428571 0.86904762]\n",
      " Recall = [0.86419753 0.72277228]\n",
      " F1 score = [0.78212291 0.78918919]\n",
      " AUC score = 79.34849040459602\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 78.57142857142857\n",
      " Precision = [0.86904762 0.71428571]\n",
      " Recall = [0.72277228 0.86419753]\n",
      " F1 score = [0.78918919 0.78212291]\n",
      " AUC score = 79.348490404596\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[73 28]\n",
      " [11 70]]\n",
      " Accuracy (acc): 78.571\n",
      " Precision (prc): 71.429\n",
      " Recall (rec): 86.42\n",
      " Sensitivity (sns): 86.42\n",
      " Specificity (spc): 72.277\n",
      " F1 Score (f1s): 78.212\n",
      " ROC AUC (AUC): 0.793\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GaussianNB(var_smoothing=1.0) \n",
      "        Best parameters of the model: {'var_smoothing': 1.0} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
      "\n",
      "                           model        model_parameters  model_scores  \\\n",
      "0  GaussianNB(var_smoothing=1.0)  {'var_smoothing': 1.0}         86.84   \n",
      "\n",
      "       confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
      "0  [[73, 28], [11, 70]]    78.571     71.429   86.42        86.42   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       72.277    78.212    0.793   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:2, TRAINING:2 AND MODEL: KNN \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: k_nearest_neighbors_parameters\n",
      "All parameters: ['n_neighbors', 'p', 'metric', 'n_splits'], [[2, 3, 5, 10, 15, 25, 35], ['manhattan', 'minkowski', 'euclidean']], [0, 2]\n",
      "Parameters:  {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: KNN - {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[84 17]\n",
      " [11 70]]\n",
      "[[70 11 17 84]\n",
      " [84 17 11 70]]\n",
      "[[154  28]\n",
      " [ 28 154]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 84.61538461538461\n",
      " Precision = [0.8045977  0.88421053]\n",
      " Recall = [0.86419753 0.83168317]\n",
      " F1 score = [0.83333333 0.85714286]\n",
      " AUC score = 84.79403495905147\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 84.61538461538461\n",
      " Precision = [0.88421053 0.8045977 ]\n",
      " Recall = [0.83168317 0.86419753]\n",
      " F1 score = [0.85714286 0.83333333]\n",
      " AUC score = 84.79403495905146\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[84 17]\n",
      " [11 70]]\n",
      " Accuracy (acc): 84.615\n",
      " Precision (prc): 80.46\n",
      " Recall (rec): 86.42\n",
      " Sensitivity (sns): 86.42\n",
      " Specificity (spc): 83.168\n",
      " F1 Score (f1s): 83.333\n",
      " ROC AUC (AUC): 0.848\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: KNeighborsClassifier(metric='manhattan', n_neighbors=35) \n",
      "        Best parameters of the model: {'metric': 'manhattan', 'n_neighbors': 35} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
      "\n",
      "                                               model  \\\n",
      "0  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
      "\n",
      "                             model_parameters  model_scores  \\\n",
      "0  {'metric': 'manhattan', 'n_neighbors': 35}         89.19   \n",
      "\n",
      "       confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
      "0  [[84, 17], [11, 70]]    84.615      80.46   86.42        86.42   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       83.168    83.333    0.848   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:2, TRAINING:2 AND MODEL: DT \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: decision_tree_parameters\n",
      "All parameters: ['max_depth', 'criterion', 'splitter', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: DT - {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[89 12]\n",
      " [ 5 76]]\n",
      "[[76  5 12 89]\n",
      " [89 12  5 76]]\n",
      "[[165  17]\n",
      " [ 17 165]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 90.65934065934066\n",
      " Precision = [0.86363636 0.94680851]\n",
      " Recall = [0.9382716  0.88118812]\n",
      " F1 score = [0.89940828 0.91282051]\n",
      " AUC score = 90.97298618750762\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 90.65934065934066\n",
      " Precision = [0.94680851 0.86363636]\n",
      " Recall = [0.88118812 0.9382716 ]\n",
      " F1 score = [0.91282051 0.89940828]\n",
      " AUC score = 90.97298618750763\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[89 12]\n",
      " [ 5 76]]\n",
      " Accuracy (acc): 90.659\n",
      " Precision (prc): 86.364\n",
      " Recall (rec): 93.827\n",
      " Sensitivity (sns): 93.827\n",
      " Specificity (spc): 88.119\n",
      " F1 Score (f1s): 89.941\n",
      " ROC AUC (AUC): 0.91\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: DecisionTreeClassifier(criterion='entropy', max_depth=3) \n",
      "        Best parameters of the model: {'criterion': 'entropy', 'max_depth': 3} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
      "\n",
      "                                               model  \\\n",
      "0  DecisionTreeClassifier(criterion='entropy', ma...   \n",
      "\n",
      "                           model_parameters  model_scores  \\\n",
      "0  {'criterion': 'entropy', 'max_depth': 3}         86.84   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[89, 12], [5, 76]]    90.659     86.364  93.827       93.827       88.119   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    89.941     0.91   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:2, TRAINING:2 AND MODEL: RF \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: random_forest_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'criterion', 'splitter', 'max_features', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.79411765 0.83161765 0.80588235 0.75588235 0.75588235 0.76838235\n",
      " 0.78088235 0.76838235 0.73088235 0.83161765 0.86838235 0.85661765\n",
      " 0.71911765 0.75661765 0.75588235 0.73161765 0.76838235 0.78161765\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.90144231 0.95375    0.97538462 0.99692308 1.         1.\n",
      " 1.         1.         1.         0.89524038 0.96307692 0.99692308\n",
      " 0.98461538 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.81003926 0.80821618 0.80605915 0.77996789 0.76445657 0.7947589\n",
      " 0.80809223 0.77289637 0.7448275  0.8246683  0.8133206  0.80781442\n",
      " 0.72652279 0.75970741 0.76036378 0.74128571 0.7529268  0.77761962\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.89011715 0.91142877 0.96520101 0.9910422  1.         1.\n",
      " 1.         1.         1.         0.88431691 0.91330608 0.95508805\n",
      " 0.9787706  1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: RF - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "405 fits failed out of a total of 1215.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 467, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.84411765 0.84411765 0.88014706 0.88014706 0.86838235 0.88014706\n",
      " 0.88014706 0.88014706 0.88014706 0.83088235 0.84338235 0.88014706\n",
      " 0.88014706 0.89191176 0.88014706 0.88014706 0.88014706 0.88014706\n",
      " 0.85441176 0.81838235 0.84191176 0.85514706 0.86764706 0.88014706\n",
      " 0.89264706 0.89264706 0.90441176 0.70735294 0.74264706 0.81764706\n",
      " 0.85441176 0.86764706 0.86764706 0.84264706 0.86764706 0.87941176\n",
      " 0.75514706 0.87941176 0.84264706 0.83014706 0.83014706 0.83014706\n",
      " 0.87941176 0.87941176 0.87941176 0.73161765 0.81691176 0.79264706\n",
      " 0.89264706 0.85514706 0.87941176 0.87941176 0.86764706 0.87941176\n",
      " 0.74485294 0.85441176 0.81764706 0.89191176 0.84264706 0.86764706\n",
      " 0.85441176 0.88014706 0.86764706 0.83014706 0.78014706 0.79264706\n",
      " 0.85514706 0.84191176 0.86764706 0.84264706 0.85514706 0.87941176\n",
      " 0.83088235 0.84264706 0.83014706 0.89264706 0.86691176 0.87941176\n",
      " 0.84264706 0.88014706 0.86691176 0.84338235 0.83235294 0.86764706\n",
      " 0.88014706 0.88014706 0.86838235 0.88014706 0.88014706 0.88014706\n",
      " 0.88014706 0.89191176 0.89264706 0.88014706 0.89191176 0.89264706\n",
      " 0.86838235 0.88014706 0.88014706 0.80514706 0.83014706 0.90441176\n",
      " 0.86764706 0.88014706 0.89191176 0.90441176 0.88014706 0.88014706\n",
      " 0.80588235 0.83088235 0.83014706 0.86691176 0.89264706 0.85514706\n",
      " 0.87941176 0.89191176 0.88014706 0.76764706 0.81764706 0.83014706\n",
      " 0.87941176 0.86764706 0.86764706 0.89191176 0.86764706 0.88014706\n",
      " 0.75514706 0.81764706 0.86764706 0.86691176 0.84264706 0.85514706\n",
      " 0.87941176 0.89191176 0.88014706 0.79117647 0.75735294 0.81764706\n",
      " 0.86691176 0.83088235 0.88014706 0.87941176 0.87941176 0.86691176\n",
      " 0.82941176 0.87941176 0.84264706 0.84191176 0.89191176 0.83014706\n",
      " 0.86764706 0.87941176 0.86764706 0.79191176 0.79338235 0.84191176\n",
      " 0.85441176 0.89191176 0.84264706 0.86764706 0.89191176 0.86764706\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.87365385 0.89519231 0.88610577 0.89225962 0.89524038 0.89221154\n",
      " 0.88293269 0.88908654 0.88908654 0.89216346 0.92600962 0.92918269\n",
      " 0.93846154 0.91072115 0.91990385 0.93846154 0.92615385 0.93533654\n",
      " 0.95692308 0.97841346 0.97225962 0.99076923 0.99384615 0.99379808\n",
      " 0.99384615 1.         1.         0.96908654 0.97846154 0.99076923\n",
      " 0.99384615 1.         1.         1.         1.         1.\n",
      " 0.94153846 0.98769231 0.99384615 0.99692308 1.         0.99692308\n",
      " 1.         1.         1.         0.95682692 0.98461538 0.99076923\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.95379808 0.99076923 0.99379808 0.99072115 1.         1.\n",
      " 1.         1.         1.         0.96610577 0.98764423 0.97533654\n",
      " 0.99692308 0.99692308 1.         1.         1.         1.\n",
      " 0.95990385 0.96918269 0.98456731 0.99692308 1.         1.\n",
      " 1.         1.         1.         0.88278846 0.88903846 0.88302885\n",
      " 0.88600962 0.90447115 0.90139423 0.88908654 0.88600962 0.88908654\n",
      " 0.92918269 0.94769231 0.93528846 0.91995192 0.92615385 0.916875\n",
      " 0.92       0.92610577 0.916875   0.96302885 0.97225962 0.99072115\n",
      " 0.99692308 0.99076923 0.98153846 1.         1.         1.\n",
      " 0.96615385 0.96923077 0.99072115 0.99692308 1.         0.99692308\n",
      " 1.         1.         1.         0.97533654 0.98769231 0.99379808\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.94764423 0.99076923 0.99379808 1.         0.99692308 0.99692308\n",
      " 1.         1.         1.         0.98144231 0.97836538 0.99384615\n",
      " 1.         0.99384615 1.         0.996875   1.         1.\n",
      " 0.97230769 0.99692308 0.98456731 0.99076923 0.99692308 0.99692308\n",
      " 1.         1.         1.         0.96923077 0.98769231 0.97841346\n",
      " 0.99692308 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.83519861 0.84117562 0.87081071 0.86156863 0.84459101 0.87081071\n",
      " 0.88187445 0.87931034 0.87324974 0.8472138  0.85366541 0.84386728\n",
      " 0.87172764 0.88407225 0.87931034 0.87931034 0.8546491  0.86754564\n",
      " 0.82939835 0.82041375 0.85142857 0.84217195 0.87333333 0.87777603\n",
      " 0.87246294 0.88666667 0.88536797 0.76308244 0.78624226 0.81504106\n",
      " 0.85740558 0.86731183 0.84422764 0.85103799 0.87333333 0.86642857\n",
      " 0.76059532 0.84558302 0.84146628 0.82146651 0.8492046  0.83583876\n",
      " 0.86642857 0.88065041 0.86399225 0.7808267  0.81322353 0.7956592\n",
      " 0.85101445 0.86353799 0.87852535 0.8746289  0.86254992 0.86112554\n",
      " 0.78479669 0.84844888 0.82891468 0.87557532 0.84795699 0.87089431\n",
      " 0.85344038 0.86940476 0.8648728  0.79681818 0.77461167 0.81902113\n",
      " 0.83671855 0.85693098 0.86731183 0.847422   0.85756098 0.87809524\n",
      " 0.79201357 0.83756461 0.84486772 0.88104839 0.85431034 0.8746289\n",
      " 0.85020465 0.85333333 0.86232558 0.85366541 0.83360326 0.87041971\n",
      " 0.87931034 0.87324974 0.86798658 0.87931034 0.87931034 0.87931034\n",
      " 0.87010974 0.86951471 0.87746606 0.87687132 0.88662742 0.88422764\n",
      " 0.87161804 0.87324974 0.87931034 0.79481538 0.79851903 0.86070942\n",
      " 0.8588122  0.85428112 0.87649225 0.87078028 0.87172764 0.87777603\n",
      " 0.79808759 0.82951718 0.82824387 0.83859509 0.87246294 0.85991641\n",
      " 0.85506494 0.88147152 0.85998522 0.79320912 0.80769085 0.84254106\n",
      " 0.8746289  0.84597701 0.86731183 0.87648003 0.8648337  0.87172764\n",
      " 0.76609546 0.81936161 0.86203324 0.85917051 0.84099167 0.84913001\n",
      " 0.87809524 0.87892857 0.87415445 0.78672263 0.79817802 0.83127992\n",
      " 0.84294372 0.84828523 0.86940476 0.85262902 0.85793164 0.8647619\n",
      " 0.78445887 0.86169355 0.85106487 0.83815891 0.87542313 0.85216908\n",
      " 0.8648337  0.87207373 0.8648728  0.79921412 0.81573892 0.85482164\n",
      " 0.84844888 0.84959831 0.83923077 0.85316704 0.87043164 0.86125122\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.86270473 0.86619495 0.87325648 0.87811016 0.87997696 0.87569421\n",
      " 0.87310881 0.87404321 0.87399142 0.87551792 0.89926007 0.90211736\n",
      " 0.90703384 0.892239   0.89477454 0.90328467 0.89923455 0.90429997\n",
      " 0.93706655 0.95971004 0.95062489 0.96435178 0.96887439 0.96587261\n",
      " 0.96450007 0.97620587 0.97043332 0.96323781 0.97559175 0.98776045\n",
      " 0.98186305 0.99245842 0.99241268 0.99847328 1.         0.99847328\n",
      " 0.94878157 0.98616559 0.99384615 0.99389313 1.         0.99844961\n",
      " 1.         1.         1.         0.95833842 0.9770444  0.98625741\n",
      " 0.99694656 0.99694656 1.         1.         1.         1.\n",
      " 0.95675368 0.96996423 0.99229466 0.99224676 0.99694656 0.9969697\n",
      " 1.         1.         1.         0.95333959 0.98769066 0.98749981\n",
      " 0.99692289 0.99692289 1.         1.         1.         1.\n",
      " 0.96005621 0.97516772 0.98764404 0.99692289 0.99847328 1.\n",
      " 1.         1.         1.         0.86044205 0.8684956  0.87129612\n",
      " 0.87230072 0.88007183 0.87839602 0.87404321 0.8748734  0.87404321\n",
      " 0.90107677 0.91192805 0.90178461 0.89584044 0.89661778 0.89306312\n",
      " 0.89333041 0.89674764 0.89429821 0.93088269 0.94401543 0.96034598\n",
      " 0.96897197 0.96857733 0.95261308 0.96476837 0.97187057 0.96476745\n",
      " 0.95869075 0.97368739 0.9832721  0.99088508 0.99243529 0.9893815\n",
      " 0.99694656 0.99241268 1.         0.96064238 0.97581709 0.99083681\n",
      " 0.99694656 0.99847328 1.         1.         1.         1.\n",
      " 0.96082961 0.98322801 0.99231833 0.99847328 0.99844961 0.99844961\n",
      " 1.         1.         1.         0.97114468 0.97399244 0.99689922\n",
      " 0.99847328 0.99537269 1.         0.9984252  1.         1.\n",
      " 0.96650375 0.98933611 0.99067326 0.9878283  0.99692308 0.99844961\n",
      " 1.         1.         1.         0.96059636 0.97871356 0.98592214\n",
      " 0.99692289 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[93  8]\n",
      " [ 1 80]]\n",
      "[[80  1  8 93]\n",
      " [93  8  1 80]]\n",
      "[[173   9]\n",
      " [  9 173]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 95.05494505494505\n",
      " Precision = [0.90909091 0.9893617 ]\n",
      " Recall = [0.98765432 0.92079208]\n",
      " F1 score = [0.94674556 0.95384615]\n",
      " AUC score = 95.42232000977874\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 95.05494505494505\n",
      " Precision = [0.9893617  0.90909091]\n",
      " Recall = [0.92079208 0.98765432]\n",
      " F1 score = [0.95384615 0.94674556]\n",
      " AUC score = 95.42232000977874\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[93  8]\n",
      " [ 1 80]]\n",
      " Accuracy (acc): 95.055\n",
      " Precision (prc): 90.909\n",
      " Recall (rec): 98.765\n",
      " Sensitivity (sns): 98.765\n",
      " Specificity (spc): 92.079\n",
      " F1 Score (f1s): 94.675\n",
      " ROC AUC (AUC): 0.954\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: RandomForestClassifier(max_depth=5) \n",
      "        Best parameters of the model: {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 100} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
      "\n",
      "                                               model  \\\n",
      "0  (DecisionTreeClassifier(max_depth=5, max_featu...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'criterion': 'gini', 'max_depth': 5, 'n_estim...         90.44   \n",
      "\n",
      "     confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[93, 8], [1, 80]]    95.055     90.909  98.765       98.765       92.079   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    94.675    0.954   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:2, TRAINING:2 AND MODEL: GB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: gradient_boosting_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'learning_rate', 'max_features', 'loss', 'min_samples_split', 'min_samples_leaf'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.5, 1.0]], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: GB - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 405 candidates, totalling 2025 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[89 12]\n",
      " [ 5 76]]\n",
      "[[76  5 12 89]\n",
      " [89 12  5 76]]\n",
      "[[165  17]\n",
      " [ 17 165]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 90.65934065934066\n",
      " Precision = [0.86363636 0.94680851]\n",
      " Recall = [0.9382716  0.88118812]\n",
      " F1 score = [0.89940828 0.91282051]\n",
      " AUC score = 90.97298618750762\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 90.65934065934066\n",
      " Precision = [0.94680851 0.86363636]\n",
      " Recall = [0.88118812 0.9382716 ]\n",
      " F1 score = [0.91282051 0.89940828]\n",
      " AUC score = 90.97298618750763\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[89 12]\n",
      " [ 5 76]]\n",
      " Accuracy (acc): 90.659\n",
      " Precision (prc): 86.364\n",
      " Recall (rec): 93.827\n",
      " Sensitivity (sns): 93.827\n",
      " Specificity (spc): 88.119\n",
      " F1 Score (f1s): 89.941\n",
      " ROC AUC (AUC): 0.91\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=50) \n",
      "        Best parameters of the model: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 50} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
      "\n",
      "                                               model  \\\n",
      "0  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'learning_rate': 0.01, 'max_depth': 2, 'n_est...         89.26   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[89, 12], [5, 76]]    90.659     86.364  93.827       93.827       88.119   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    89.941     0.91   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:2, TRAINING:2 AND MODEL: XGB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: xtreme_gradient_boosting_parameters\n",
      "All parameters: ['max_depth', 'eta', 'max_leaves'], [[2, 3, 6, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'sdb1', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco3', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm5', 'plm6', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'n1', 'n2', 'n3', 'n4', 'n5', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: XGB - {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "\n",
      "        From training? False, Data shape: (182, 18), Indices: [0, 2, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (182, 14), Target shape: (182,), Metadata: (182, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[101   0]\n",
      " [  0  81]]\n",
      "[[ 81   0   0 101]\n",
      " [101   0   0  81]]\n",
      "[[182   0]\n",
      " [  0 182]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 100.0\n",
      " Precision = [1. 1.]\n",
      " Recall = [1. 1.]\n",
      " F1 score = [1. 1.]\n",
      " AUC score = 100.0\n",
      " Support = [ 81 101]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 100.0\n",
      " Precision = [1. 1.]\n",
      " Recall = [1. 1.]\n",
      " F1 score = [1. 1.]\n",
      " AUC score = 100.0\n",
      " Support = [101  81]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[101   0]\n",
      " [  0  81]]\n",
      " Accuracy (acc): 100.0\n",
      " Precision (prc): 100.0\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 100.0\n",
      " F1 Score (f1s): 100.0\n",
      " ROC AUC (AUC): 1.0\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False, eta=0.05,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.0500000007, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, ...) \n",
      "        Best parameters of the model: {'eta': 0.05, 'max_depth': 6} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
      "\n",
      "                                               model  \\\n",
      "0  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
      "\n",
      "                model_parameters  model_scores     confusion_matrix  accuracy  \\\n",
      "0  {'eta': 0.05, 'max_depth': 6}         89.19  [[101, 0], [0, 81]]     100.0   \n",
      "\n",
      "   precision  recall  sensitivity  specificity  f1_score  roc_auc  \n",
      "0      100.0   100.0        100.0        100.0     100.0      1.0   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            Best model index calculation  \n",
      "            \n",
      "\n",
      "            ***************************************************************************************************\n",
      "            TRAINING 2 END... \n",
      "            \n",
      "\n",
      "        ### MODEL EVALUATION PHASE \n",
      "        EVALUATION 2 START... XXXXX \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "        From training? False, Data shape: (48, 18), Indices: [129, 1, 3, 27, 10, 11, 12, 13, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 39, 40, 41, 42, 43, 44, 45, 46, 73, 74, 85, 86, 87, 88, 102, 108, 109, 110, 111, 112]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (48, 14), Target shape: (48,), Metadata: (48, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[21  5]\n",
      " [ 0 22]]\n",
      "[[22  0  5 21]\n",
      " [21  5  0 22]]\n",
      "[[43  5]\n",
      " [ 5 43]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 89.58333333333334\n",
      " Precision = [0.81481481 1.        ]\n",
      " Recall = [1.         0.80769231]\n",
      " F1 score = [0.89795918 0.89361702]\n",
      " AUC score = 90.38461538461539\n",
      " Support = [22 26]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 89.58333333333334\n",
      " Precision = [1.         0.81481481]\n",
      " Recall = [0.80769231 1.        ]\n",
      " F1 score = [0.89361702 0.89795918]\n",
      " AUC score = 90.38461538461539\n",
      " Support = [26 22]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[21  5]\n",
      " [ 0 22]]\n",
      " Accuracy (acc): 89.583\n",
      " Precision (prc): 81.481\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 80.769\n",
      " F1 Score (f1s): 89.796\n",
      " ROC AUC (AUC): 0.904\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[21  5]\n",
      " [ 1 21]]\n",
      "[[21  1  5 21]\n",
      " [21  5  1 21]]\n",
      "[[42  6]\n",
      " [ 6 42]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.5\n",
      " Precision = [0.80769231 0.95454545]\n",
      " Recall = [0.95454545 0.80769231]\n",
      " F1 score = [0.875 0.875]\n",
      " AUC score = 88.11188811188812\n",
      " Support = [22 26]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.5\n",
      " Precision = [0.95454545 0.80769231]\n",
      " Recall = [0.80769231 0.95454545]\n",
      " F1 score = [0.875 0.875]\n",
      " AUC score = 88.11188811188812\n",
      " Support = [26 22]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[21  5]\n",
      " [ 1 21]]\n",
      " Accuracy (acc): 87.5\n",
      " Precision (prc): 80.769\n",
      " Recall (rec): 95.455\n",
      " Sensitivity (sns): 95.455\n",
      " Specificity (spc): 80.769\n",
      " F1 Score (f1s): 87.5\n",
      " ROC AUC (AUC): 0.881\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[20  6]\n",
      " [ 0 22]]\n",
      "[[22  0  6 20]\n",
      " [20  6  0 22]]\n",
      "[[42  6]\n",
      " [ 6 42]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.5\n",
      " Precision = [0.78571429 1.        ]\n",
      " Recall = [1.         0.76923077]\n",
      " F1 score = [0.88       0.86956522]\n",
      " AUC score = 88.46153846153845\n",
      " Support = [22 26]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.5\n",
      " Precision = [1.         0.78571429]\n",
      " Recall = [0.76923077 1.        ]\n",
      " F1 score = [0.86956522 0.88      ]\n",
      " AUC score = 88.46153846153845\n",
      " Support = [26 22]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[20  6]\n",
      " [ 0 22]]\n",
      " Accuracy (acc): 87.5\n",
      " Precision (prc): 78.571\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 76.923\n",
      " F1 Score (f1s): 88.0\n",
      " ROC AUC (AUC): 0.885\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[21  5]\n",
      " [ 0 22]]\n",
      "[[22  0  5 21]\n",
      " [21  5  0 22]]\n",
      "[[43  5]\n",
      " [ 5 43]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 89.58333333333334\n",
      " Precision = [0.81481481 1.        ]\n",
      " Recall = [1.         0.80769231]\n",
      " F1 score = [0.89795918 0.89361702]\n",
      " AUC score = 90.38461538461539\n",
      " Support = [22 26]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 89.58333333333334\n",
      " Precision = [1.         0.81481481]\n",
      " Recall = [0.80769231 1.        ]\n",
      " F1 score = [0.89361702 0.89795918]\n",
      " AUC score = 90.38461538461539\n",
      " Support = [26 22]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[21  5]\n",
      " [ 0 22]]\n",
      " Accuracy (acc): 89.583\n",
      " Precision (prc): 81.481\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 80.769\n",
      " F1 Score (f1s): 89.796\n",
      " ROC AUC (AUC): 0.904\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[21  5]\n",
      " [ 0 22]]\n",
      "[[22  0  5 21]\n",
      " [21  5  0 22]]\n",
      "[[43  5]\n",
      " [ 5 43]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 89.58333333333334\n",
      " Precision = [0.81481481 1.        ]\n",
      " Recall = [1.         0.80769231]\n",
      " F1 score = [0.89795918 0.89361702]\n",
      " AUC score = 90.38461538461539\n",
      " Support = [22 26]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 89.58333333333334\n",
      " Precision = [1.         0.81481481]\n",
      " Recall = [0.80769231 1.        ]\n",
      " F1 score = [0.89361702 0.89795918]\n",
      " AUC score = 90.38461538461539\n",
      " Support = [26 22]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[21  5]\n",
      " [ 0 22]]\n",
      " Accuracy (acc): 89.583\n",
      " Precision (prc): 81.481\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 80.769\n",
      " F1 Score (f1s): 89.796\n",
      " ROC AUC (AUC): 0.904\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[21  5]\n",
      " [ 0 22]]\n",
      "[[22  0  5 21]\n",
      " [21  5  0 22]]\n",
      "[[43  5]\n",
      " [ 5 43]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 89.58333333333334\n",
      " Precision = [0.81481481 1.        ]\n",
      " Recall = [1.         0.80769231]\n",
      " F1 score = [0.89795918 0.89361702]\n",
      " AUC score = 90.38461538461539\n",
      " Support = [22 26]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 89.58333333333334\n",
      " Precision = [1.         0.81481481]\n",
      " Recall = [0.80769231 1.        ]\n",
      " F1 score = [0.89361702 0.89795918]\n",
      " AUC score = 90.38461538461539\n",
      " Support = [26 22]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[21  5]\n",
      " [ 0 22]]\n",
      " Accuracy (acc): 89.583\n",
      " Precision (prc): 81.481\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 80.769\n",
      " F1 Score (f1s): 89.796\n",
      " ROC AUC (AUC): 0.904\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[21  5]\n",
      " [ 0 22]]\n",
      "[[22  0  5 21]\n",
      " [21  5  0 22]]\n",
      "[[43  5]\n",
      " [ 5 43]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 89.58333333333334\n",
      " Precision = [0.81481481 1.        ]\n",
      " Recall = [1.         0.80769231]\n",
      " F1 score = [0.89795918 0.89361702]\n",
      " AUC score = 90.38461538461539\n",
      " Support = [22 26]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 89.58333333333334\n",
      " Precision = [1.         0.81481481]\n",
      " Recall = [0.80769231 1.        ]\n",
      " F1 score = [0.89361702 0.89795918]\n",
      " AUC score = 90.38461538461539\n",
      " Support = [26 22]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[21  5]\n",
      " [ 0 22]]\n",
      " Accuracy (acc): 89.583\n",
      " Precision (prc): 81.481\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 80.769\n",
      " F1 Score (f1s): 89.796\n",
      " ROC AUC (AUC): 0.904\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[21  5]\n",
      " [ 0 22]]\n",
      "[[22  0  5 21]\n",
      " [21  5  0 22]]\n",
      "[[43  5]\n",
      " [ 5 43]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 89.58333333333334\n",
      " Precision = [0.81481481 1.        ]\n",
      " Recall = [1.         0.80769231]\n",
      " F1 score = [0.89795918 0.89361702]\n",
      " AUC score = 90.38461538461539\n",
      " Support = [22 26]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 89.58333333333334\n",
      " Precision = [1.         0.81481481]\n",
      " Recall = [0.80769231 1.        ]\n",
      " F1 score = [0.89361702 0.89795918]\n",
      " AUC score = 90.38461538461539\n",
      " Support = [26 22]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[21  5]\n",
      " [ 0 22]]\n",
      " Accuracy (acc): 89.583\n",
      " Precision (prc): 81.481\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 80.769\n",
      " F1 Score (f1s): 89.796\n",
      " ROC AUC (AUC): 0.904\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "            ===================================================================================================\n",
      "            TEST 2 END...\n",
      "            \n",
      "test- tsi, train_all_ids, test_ids, tr_all_dat, ts_dat 2 [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [51, 4, 52, 14, 15, 16, 17, 28, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 53, 54, 75, 76, 89, 90, 91, 92, 47, 113, 114, 115, 116, 117, 48, 49, 50] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['nfle21', 'sdb3', 'nfle22', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'narco3', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'nfle23', 'nfle24', 'plm5', 'plm6', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'nfle17', 'n11', 'n12', 'n13', 'n14', 'n15', 'nfle18', 'nfle19', 'nfle20']\n",
      "\n",
      "            ### MODEL TEST PHASE \n",
      "            TEST 3 START... XXXXX \n",
      "            ===================================================================================================\n",
      "            Test=> 45 ['nfle21', 'sdb3', 'nfle22', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'narco3', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'nfle23', 'nfle24', 'plm5', 'plm6', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'nfle17', 'n11', 'n12', 'n13', 'n14', 'n15', 'nfle18', 'nfle19', 'nfle20'] \n",
      "            Training (Including Validation)=> 185 ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            \n",
      "Random 5 percentage splitting testing...\n",
      "train- tri, tmp_train_ids, tmp_val_ids, tr_dat, val_dat 0 [ 94 119  23  15  24  31   3   0 118  34  38  47  16  74  86  54  43 154\n",
      " 129 143 123  83 183  21 161 181 127 170 139  46 182  80  49 172  98 114\n",
      " 137 147  70   5 179 106  82  30   7 107  90  51 108 105 109  67  72  41\n",
      "  95  93  39 152  81  55  85  25 159  42  48 178 145  18 116  91 110 101\n",
      " 184  12 168  50   6 142  73 124 131 136  89 149  22  92 160 144  14 171\n",
      "  45 122 141 169 100  10 175  79  19 104  87  71 120 112   8 111  60 130\n",
      " 155 126 174 162   1 151  69   9 135   4 113 138  56 165 148  26 150 125\n",
      " 164  62 117  88  52 156 102  40 167 176 177 157  17  36  13  57  68  33\n",
      "  84  20 134 121  27  75  65 180  66 153 132  29 173  97  76 128  59  37\n",
      "  28  61 163 158 133  58  96  53  78  64 146  11  44  77 140   2  35  99\n",
      "  63  32 103 166 115] [ 94 119  23  15  24  31   3   0 118  34  38  47  16  74  86  54  43 154\n",
      " 129 143 123  83 183  21 161 181 127 170 139  46 182  80  49 172  98 114\n",
      " 137 147  70   5 179 106  82  30   7 107  90  51 108 105 109  67  72  41\n",
      "  95  93  39 152  81  55  85  25 159  42  48 178 145  18 116  91 110 101\n",
      " 184  12 168  50   6 142  73 124 131 136  89 149  22  92 160 144  14 171\n",
      "  45 122 141 169 100  10 175  79  19 104  87  71 120 112   8 111  60 130\n",
      " 155 126 174 162   1 151  69   9 135   4 113 138  56 165 148  26 150 125\n",
      " 164  62 117  88  52 156 102  40 167 176 177 157  17  36  13  57  68  33\n",
      "  84  20 134 121  27  75  65 180  66 153 132  29 173  97  76 128  59  37\n",
      "  28  61 163 158 133  58  96  53  78  64 146  11  44  77 140   2  35  99\n",
      "  63  32 103 166 115] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "            *** MODEL TRAINING AND VALIDATION PHASE FOR TEST 3 \n",
      "            TRAINING 1 START... XXXXX \n",
      "            ***************************************************************************************************\n",
      "            Validation=> 185 ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            Training=> 185 ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            \n",
      "\n",
      "            *** ML MODEL FOR TEST:3, TRAINING:1 AND MODEL: LR \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: logistic_regression_parameters\n",
      "All parameters: ['penalty', 'solver', 'max_iter', 'C'], [['l2', 'elasticnet'], [50, 100, 130, 150, 170, 200], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]], [0, 2, 3]\n",
      "Parameters:  {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\"\n",
      "            Selected features: ['W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "            \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: LR - {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 84 candidates, totalling 420 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[91 11]\n",
      " [ 9 74]]\n",
      "[[74  9 11 91]\n",
      " [91 11  9 74]]\n",
      "[[165  20]\n",
      " [ 20 165]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 89.1891891891892\n",
      " Precision = [0.87058824 0.91      ]\n",
      " Recall = [0.89156627 0.89215686]\n",
      " F1 score = [0.88095238 0.9009901 ]\n",
      " AUC score = 89.18615639026694\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 89.1891891891892\n",
      " Precision = [0.91       0.87058824]\n",
      " Recall = [0.89215686 0.89156627]\n",
      " F1 score = [0.9009901  0.88095238]\n",
      " AUC score = 89.18615639026693\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[91 11]\n",
      " [ 9 74]]\n",
      " Accuracy (acc): 89.189\n",
      " Precision (prc): 87.059\n",
      " Recall (rec): 89.157\n",
      " Sensitivity (sns): 89.157\n",
      " Specificity (spc): 89.216\n",
      " F1 Score (f1s): 88.095\n",
      " ROC AUC (AUC): 0.892\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: LogisticRegression(max_iter=50) \n",
      "        Best parameters of the model: {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
      "\n",
      "                             model  \\\n",
      "0  LogisticRegression(max_iter=50)   \n",
      "\n",
      "                              model_parameters  model_scores  \\\n",
      "0  {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'}         90.59   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[91, 11], [9, 74]]    89.189     87.059  89.157       89.157       89.216   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    88.095    0.892   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:3, TRAINING:1 AND MODEL: SVC \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: support_vector_classifier_parameters\n",
      "All parameters: ['probability', 'C', 'kernel', 'gamma', 'degree', 'class_weightdict'], [[True], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], ['linear', 'rbf', 'poly']], [0, 1, 2]\n",
      "Parameters:  {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "210 fits failed out of a total of 420.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.87058824        nan 0.87058824        nan 0.87058824        nan\n",
      " 0.87058824        nan 0.87058824        nan 0.87058824        nan\n",
      " 0.87058824        nan 0.87058824        nan 0.87058824        nan\n",
      " 0.87058824        nan 0.87058824        nan 0.87058824        nan\n",
      " 0.89411765        nan 0.89411765        nan 0.89411765        nan\n",
      " 0.89411765        nan 0.89411765        nan 0.89411765        nan\n",
      " 0.90588235        nan 0.90588235        nan 0.90588235        nan\n",
      " 0.90588235        nan 0.90588235        nan 0.90588235        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.86766169        nan 0.86766169        nan 0.86766169        nan\n",
      " 0.86766169        nan 0.86766169        nan 0.86766169        nan\n",
      " 0.86766169        nan 0.86766169        nan 0.86766169        nan\n",
      " 0.86766169        nan 0.86766169        nan 0.86766169        nan\n",
      " 0.87969245        nan 0.87969245        nan 0.87969245        nan\n",
      " 0.87969245        nan 0.87969245        nan 0.87969245        nan\n",
      " 0.89470828        nan 0.89470828        nan 0.89470828        nan\n",
      " 0.89470828        nan 0.89470828        nan 0.89470828        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.87253589        nan 0.87253589        nan 0.87253589        nan\n",
      " 0.87253589        nan 0.87253589        nan 0.87253589        nan\n",
      " 0.8506203         nan 0.8506203         nan 0.8506203         nan\n",
      " 0.8506203         nan 0.8506203         nan 0.8506203         nan\n",
      " 0.86243031        nan 0.86243031        nan 0.86243031        nan\n",
      " 0.86243031        nan 0.86243031        nan 0.86243031        nan\n",
      " 0.88249886        nan 0.88249886        nan 0.88249886        nan\n",
      " 0.88249886        nan 0.88249886        nan 0.88249886        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.86259916        nan 0.86259916        nan 0.86259916        nan\n",
      " 0.86259916        nan 0.86259916        nan 0.86259916        nan\n",
      " 0.85462585        nan 0.85462585        nan 0.85462585        nan\n",
      " 0.85462585        nan 0.85462585        nan 0.85462585        nan\n",
      " 0.86153249        nan 0.86153249        nan 0.86153249        nan\n",
      " 0.86153249        nan 0.86153249        nan 0.86153249        nan\n",
      " 0.87649469        nan 0.87649469        nan 0.87649469        nan\n",
      " 0.87649469        nan 0.87649469        nan 0.87649469        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: SVC - {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[91 11]\n",
      " [ 8 75]]\n",
      "[[75  8 11 91]\n",
      " [91 11  8 75]]\n",
      "[[166  19]\n",
      " [ 19 166]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 89.72972972972974\n",
      " Precision = [0.87209302 0.91919192]\n",
      " Recall = [0.90361446 0.89215686]\n",
      " F1 score = [0.88757396 0.90547264]\n",
      " AUC score = 89.78856602882117\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 89.72972972972974\n",
      " Precision = [0.91919192 0.87209302]\n",
      " Recall = [0.89215686 0.90361446]\n",
      " F1 score = [0.90547264 0.88757396]\n",
      " AUC score = 89.78856602882117\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[91 11]\n",
      " [ 8 75]]\n",
      " Accuracy (acc): 89.73\n",
      " Precision (prc): 87.209\n",
      " Recall (rec): 90.361\n",
      " Sensitivity (sns): 90.361\n",
      " Specificity (spc): 89.216\n",
      " F1 Score (f1s): 88.757\n",
      " ROC AUC (AUC): 0.898\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: SVC(kernel='linear', probability=True) \n",
      "        Best parameters of the model: {'C': 1.0, 'kernel': 'linear', 'probability': True} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
      "\n",
      "                                    model  \\\n",
      "0  SVC(kernel='linear', probability=True)   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'C': 1.0, 'kernel': 'linear', 'probability': ...         91.76   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[91, 11], [8, 75]]     89.73     87.209  90.361       90.361       89.216   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    88.757    0.898   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:3, TRAINING:1 AND MODEL: NB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: naive_bayes_parameters\n",
      "All parameters: ['var_smoothing'], [[1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]], [0]\n",
      "Parameters:  {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: NB - {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[87 15]\n",
      " [ 5 78]]\n",
      "[[78  5 15 87]\n",
      " [87 15  5 78]]\n",
      "[[165  20]\n",
      " [ 20 165]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 89.1891891891892\n",
      " Precision = [0.83870968 0.94565217]\n",
      " Recall = [0.93975904 0.85294118]\n",
      " F1 score = [0.88636364 0.89690722]\n",
      " AUC score = 89.63501063075834\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 89.1891891891892\n",
      " Precision = [0.94565217 0.83870968]\n",
      " Recall = [0.85294118 0.93975904]\n",
      " F1 score = [0.89690722 0.88636364]\n",
      " AUC score = 89.63501063075833\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[87 15]\n",
      " [ 5 78]]\n",
      " Accuracy (acc): 89.189\n",
      " Precision (prc): 83.871\n",
      " Recall (rec): 93.976\n",
      " Sensitivity (sns): 93.976\n",
      " Specificity (spc): 85.294\n",
      " F1 Score (f1s): 88.636\n",
      " ROC AUC (AUC): 0.896\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GaussianNB(var_smoothing=0.005623413251903491) \n",
      "        Best parameters of the model: {'var_smoothing': 0.005623413251903491} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
      "\n",
      "                                            model  \\\n",
      "0  GaussianNB(var_smoothing=0.005623413251903491)   \n",
      "\n",
      "                          model_parameters  model_scores     confusion_matrix  \\\n",
      "0  {'var_smoothing': 0.005623413251903491}         88.24  [[87, 15], [5, 78]]   \n",
      "\n",
      "   accuracy  precision  recall  sensitivity  specificity  f1_score  roc_auc  \n",
      "0    89.189     83.871  93.976       93.976       85.294    88.636    0.896   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:3, TRAINING:1 AND MODEL: KNN \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: k_nearest_neighbors_parameters\n",
      "All parameters: ['n_neighbors', 'p', 'metric', 'n_splits'], [[2, 3, 5, 10, 15, 25, 35], ['manhattan', 'minkowski', 'euclidean']], [0, 2]\n",
      "Parameters:  {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: KNN - {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[90 12]\n",
      " [ 4 79]]\n",
      "[[79  4 12 90]\n",
      " [90 12  4 79]]\n",
      "[[169  16]\n",
      " [ 16 169]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 91.35135135135135\n",
      " Precision = [0.86813187 0.95744681]\n",
      " Recall = [0.95180723 0.88235294]\n",
      " F1 score = [0.90804598 0.91836735]\n",
      " AUC score = 91.70800850460667\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 91.35135135135135\n",
      " Precision = [0.95744681 0.86813187]\n",
      " Recall = [0.88235294 0.95180723]\n",
      " F1 score = [0.91836735 0.90804598]\n",
      " AUC score = 91.70800850460665\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[90 12]\n",
      " [ 4 79]]\n",
      " Accuracy (acc): 91.351\n",
      " Precision (prc): 86.813\n",
      " Recall (rec): 95.181\n",
      " Sensitivity (sns): 95.181\n",
      " Specificity (spc): 88.235\n",
      " F1 Score (f1s): 90.805\n",
      " ROC AUC (AUC): 0.917\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: KNeighborsClassifier() \n",
      "        Best parameters of the model: {'metric': 'minkowski', 'n_neighbors': 5} \n",
      "        Best model scores:                                               method                   model  \\\n",
      "0  GridSearchCV(cv=5, estimator=KNeighborsClassif...  KNeighborsClassifier()   \n",
      "\n",
      "                            model_parameters  model_scores  \\\n",
      "0  {'metric': 'minkowski', 'n_neighbors': 5}         91.76   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[90, 12], [4, 79]]    91.351     86.813  95.181       95.181       88.235   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    90.805    0.917   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:3, TRAINING:1 AND MODEL: DT \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: decision_tree_parameters\n",
      "All parameters: ['max_depth', 'criterion', 'splitter', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: DT - {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[93  9]\n",
      " [ 3 80]]\n",
      "[[80  3  9 93]\n",
      " [93  9  3 80]]\n",
      "[[173  12]\n",
      " [ 12 173]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 93.51351351351352\n",
      " Precision = [0.8988764 0.96875  ]\n",
      " Recall = [0.96385542 0.91176471]\n",
      " F1 score = [0.93023256 0.93939394]\n",
      " AUC score = 93.78100637845499\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 93.51351351351352\n",
      " Precision = [0.96875   0.8988764]\n",
      " Recall = [0.91176471 0.96385542]\n",
      " F1 score = [0.93939394 0.93023256]\n",
      " AUC score = 93.78100637845499\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[93  9]\n",
      " [ 3 80]]\n",
      " Accuracy (acc): 93.514\n",
      " Precision (prc): 89.888\n",
      " Recall (rec): 96.386\n",
      " Sensitivity (sns): 96.386\n",
      " Specificity (spc): 91.176\n",
      " F1 Score (f1s): 93.023\n",
      " ROC AUC (AUC): 0.938\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: DecisionTreeClassifier(max_depth=3) \n",
      "        Best parameters of the model: {'criterion': 'gini', 'max_depth': 3} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
      "\n",
      "                                 model                       model_parameters  \\\n",
      "0  DecisionTreeClassifier(max_depth=3)  {'criterion': 'gini', 'max_depth': 3}   \n",
      "\n",
      "   model_scores    confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
      "0         92.94  [[93, 9], [3, 80]]    93.514     89.888  96.386       96.386   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       91.176    93.023    0.938   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:3, TRAINING:1 AND MODEL: RF \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: random_forest_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'criterion', 'splitter', 'max_features', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.91764706 0.92941176 0.89191176 0.87941176 0.86838235 0.89338235\n",
      " 0.91617647 0.91691176 0.88014706 0.91764706 0.92941176 0.83382353\n",
      " 0.85735294 0.84558824 0.86911765 0.84485294 0.88161765 0.88161765\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.94287653 0.96689281 0.97589326 1.         1.         1.\n",
      " 1.         1.         1.         0.94581637 0.96399819 0.95187698\n",
      " 0.99099955 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.89218638 0.88103386 0.86154166 0.85515511 0.84751143 0.85561842\n",
      " 0.88189315 0.8677939  0.85829698 0.89777778 0.8914361  0.83036243\n",
      " 0.85028752 0.84201166 0.86362085 0.84800611 0.87006024 0.87006024\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.91992779 0.93121037 0.97023713 0.99851852 1.         1.\n",
      " 1.         1.         1.         0.91766768 0.92562564 0.9603443\n",
      " 0.99544298 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: RF - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[89 13]\n",
      " [ 7 76]]\n",
      "[[76  7 13 89]\n",
      " [89 13  7 76]]\n",
      "[[165  20]\n",
      " [ 20 165]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 89.1891891891892\n",
      " Precision = [0.85393258 0.92708333]\n",
      " Recall = [0.91566265 0.87254902]\n",
      " F1 score = [0.88372093 0.8989899 ]\n",
      " AUC score = 89.41058351051264\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 89.1891891891892\n",
      " Precision = [0.92708333 0.85393258]\n",
      " Recall = [0.87254902 0.91566265]\n",
      " F1 score = [0.8989899  0.88372093]\n",
      " AUC score = 89.41058351051264\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[89 13]\n",
      " [ 7 76]]\n",
      " Accuracy (acc): 89.189\n",
      " Precision (prc): 85.393\n",
      " Recall (rec): 91.566\n",
      " Sensitivity (sns): 91.566\n",
      " Specificity (spc): 87.255\n",
      " F1 Score (f1s): 88.372\n",
      " ROC AUC (AUC): 0.894\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: RandomForestClassifier(max_depth=2, n_estimators=5) \n",
      "        Best parameters of the model: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 5} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
      "\n",
      "                                               model  \\\n",
      "0  (DecisionTreeClassifier(max_depth=2, max_featu...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'criterion': 'gini', 'max_depth': 2, 'n_estim...         94.12   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[89, 13], [7, 76]]    89.189     85.393  91.566       91.566       87.255   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    88.372    0.894   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:3, TRAINING:1 AND MODEL: GB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: gradient_boosting_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'learning_rate', 'max_features', 'loss', 'min_samples_split', 'min_samples_leaf'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.5, 1.0]], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "405 fits failed out of a total of 1215.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 467, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.88235294 0.94117647 0.90588235 0.91764706 0.90588235 0.91764706\n",
      " 0.91764706 0.91764706 0.91764706 0.84705882 0.92941176 0.91764706\n",
      " 0.92941176 0.91764706 0.91764706 0.92941176 0.92941176 0.91764706\n",
      " 0.90441176 0.90441176 0.90441176 0.89338235 0.92941176 0.91764706\n",
      " 0.91764706 0.92941176 0.92941176 0.86911765 0.84485294 0.83235294\n",
      " 0.91764706 0.94117647 0.91691176 0.94117647 0.92941176 0.91691176\n",
      " 0.84411765 0.83161765 0.88161765 0.90441176 0.88161765 0.91691176\n",
      " 0.91764706 0.91764706 0.92941176 0.85661765 0.89338235 0.88235294\n",
      " 0.91764706 0.91764706 0.92941176 0.90514706 0.92941176 0.92941176\n",
      " 0.88088235 0.84705882 0.88161765 0.90514706 0.91764706 0.91764706\n",
      " 0.89264706 0.92941176 0.92867647 0.80808824 0.81029412 0.84338235\n",
      " 0.90441176 0.90367647 0.90441176 0.91764706 0.91691176 0.92941176\n",
      " 0.85661765 0.86838235 0.89264706 0.91617647 0.90514706 0.88088235\n",
      " 0.91691176 0.91691176 0.91691176 0.83088235 0.92941176 0.92941176\n",
      " 0.91764706 0.89411765 0.90588235 0.91764706 0.91764706 0.91764706\n",
      " 0.84632353 0.91764706 0.91691176 0.89411765 0.91764706 0.92941176\n",
      " 0.91764706 0.91764706 0.91764706 0.91617647 0.89411765 0.88235294\n",
      " 0.91764706 0.92941176 0.92941176 0.92941176 0.92941176 0.92941176\n",
      " 0.89264706 0.87058824 0.91691176 0.90514706 0.90588235 0.90588235\n",
      " 0.91764706 0.91691176 0.92941176 0.85588235 0.90514706 0.82058824\n",
      " 0.89264706 0.89338235 0.90441176 0.91691176 0.92941176 0.92941176\n",
      " 0.88014706 0.84705882 0.86911765 0.89411765 0.92867647 0.89338235\n",
      " 0.94117647 0.91764706 0.92941176 0.85661765 0.83382353 0.89338235\n",
      " 0.91764706 0.91691176 0.90514706 0.91617647 0.92941176 0.91691176\n",
      " 0.83235294 0.86911765 0.86838235 0.90588235 0.83382353 0.94117647\n",
      " 0.91764706 0.94117647 0.91691176 0.73602941 0.87941176 0.89411765\n",
      " 0.91764706 0.91764706 0.91691176 0.90514706 0.89264706 0.90588235\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.90972411 0.91881502 0.91886024 0.89760289 0.91275441 0.91279964\n",
      " 0.91275441 0.92483039 0.91881502 0.92763455 0.94595206 0.93383085\n",
      " 0.94278607 0.93686115 0.93686115 0.93387607 0.93984622 0.93681592\n",
      " 0.94287653 0.96698327 0.98498417 0.97299864 0.99402985 0.99095432\n",
      " 0.99701493 1.         0.9969697  0.96698327 0.97290819 0.99402985\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.95192221 0.96987788 0.99095432 0.99393939 0.99701493 1.\n",
      " 1.         1.         1.         0.95793758 0.97299864 0.98801447\n",
      " 0.99701493 0.99099955 0.99701493 1.         1.         1.\n",
      " 0.97286296 0.97892356 0.98792402 0.99398462 0.9969697  0.99701493\n",
      " 1.         1.         1.         0.96996834 0.98190864 0.99099955\n",
      " 0.99398462 1.         1.         1.         1.         1.\n",
      " 0.96689281 0.97892356 0.99095432 0.9969697  0.9969697  0.9969697\n",
      " 1.         1.         1.         0.89791949 0.93364993 0.92483039\n",
      " 0.91886024 0.92772501 0.93075531 0.91582994 0.91582994 0.92781547\n",
      " 0.91872456 0.94292175 0.9458616  0.94581637 0.939801   0.94287653\n",
      " 0.93686115 0.93989145 0.93686115 0.92772501 0.99099955 0.98493894\n",
      " 0.99099955 0.98792402 0.99099955 0.99701493 1.         1.\n",
      " 0.97295341 0.98195387 0.9819991  0.99393939 1.         1.\n",
      " 1.         1.         1.         0.96698327 0.99701493 0.9880597\n",
      " 0.99402985 1.         1.         1.         1.         1.\n",
      " 0.97598372 0.98190864 0.98792402 1.         1.         1.\n",
      " 1.         1.         1.         0.97589326 0.97598372 0.98792402\n",
      " 1.         0.99701493 1.         1.         1.         1.\n",
      " 0.96372682 0.97593849 0.99090909 0.9969697  0.9969697  1.\n",
      " 1.         1.         1.         0.96381728 0.98801447 0.97892356\n",
      " 0.99701493 0.9969697  0.99398462 1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.88875327 0.89930505 0.89278997 0.89777778 0.9001626  0.89777778\n",
      " 0.90060606 0.90356589 0.90356589 0.81086927 0.88952381 0.89777778\n",
      " 0.90791789 0.90060606 0.89777778 0.90508961 0.90791789 0.89777778\n",
      " 0.87188848 0.87724413 0.88896628 0.88582146 0.90508961 0.90157539\n",
      " 0.90185728 0.90508961 0.90508961 0.83229333 0.86152986 0.85175937\n",
      " 0.89632369 0.91107994 0.89863799 0.9170088  0.90508961 0.89863799\n",
      " 0.84212366 0.84050836 0.87277778 0.88905096 0.87976085 0.89863799\n",
      " 0.90157539 0.899029   0.90508961 0.84947498 0.87967416 0.87746169\n",
      " 0.8893409  0.90157539 0.90508961 0.8897491  0.90508961 0.90508961\n",
      " 0.85569569 0.8575     0.87796397 0.89028752 0.899029   0.899029\n",
      " 0.88569567 0.90508961 0.90462833 0.81953463 0.83043743 0.84895406\n",
      " 0.89458456 0.89039029 0.89175627 0.89882698 0.89863799 0.90508961\n",
      " 0.84880952 0.83792961 0.88569567 0.9018695  0.89237537 0.88218145\n",
      " 0.89863799 0.89863799 0.89863799 0.79202635 0.8949323  0.89077503\n",
      " 0.89206349 0.8843459  0.88057547 0.8834632  0.90060606 0.90356589\n",
      " 0.8447619  0.88731183 0.90146628 0.88796223 0.89777778 0.90508961\n",
      " 0.89777778 0.89777778 0.89777778 0.87027273 0.8671071  0.88333333\n",
      " 0.89882698 0.90791789 0.90508961 0.90791789 0.90791789 0.90508961\n",
      " 0.8520514  0.87216031 0.89863799 0.88687329 0.89454545 0.90064516\n",
      " 0.899029   0.89863799 0.90508961 0.84364442 0.88934506 0.85013146\n",
      " 0.87127236 0.88526556 0.89175627 0.89863799 0.90508961 0.90508961\n",
      " 0.82550454 0.86044812 0.87017142 0.88887027 0.90462833 0.88896628\n",
      " 0.91194444 0.90157539 0.90508961 0.86366708 0.85793011 0.88008961\n",
      " 0.90185728 0.89863799 0.88987207 0.90064516 0.90508961 0.89863799\n",
      " 0.84703685 0.87283154 0.87603533 0.89541789 0.85675708 0.91107994\n",
      " 0.899029   0.91397849 0.89863799 0.78410509 0.87854055 0.89108225\n",
      " 0.89770751 0.899029   0.89863799 0.89540567 0.88824206 0.89771833\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.88863215 0.89672169 0.89542432 0.88374647 0.89343382 0.89351593\n",
      " 0.89600636 0.89746064 0.89798974 0.89931965 0.91384127 0.90617456\n",
      " 0.91080452 0.9114677  0.90767755 0.90735558 0.91185211 0.91158052\n",
      " 0.9375087  0.94897866 0.96200601 0.95173261 0.95294617 0.95546832\n",
      " 0.96400406 0.96137425 0.95176677 0.95970569 0.96445122 0.9880597\n",
      " 0.98685706 0.99553328 0.99402968 0.99851852 0.99849624 0.99851852\n",
      " 0.96333136 0.97133316 0.99241233 0.99546508 0.995511   1.\n",
      " 1.         1.         1.         0.95791325 0.97582438 0.99095449\n",
      " 0.99699248 0.99544298 0.99699248 1.         1.         1.\n",
      " 0.97155207 0.97308324 0.9924125  0.9924807  0.99548804 0.99699248\n",
      " 1.         1.         1.         0.96401256 0.98199669 0.99393922\n",
      " 0.99248052 0.99701493 0.99562044 1.         1.         1.\n",
      " 0.96125613 0.97315511 0.9924579  0.9969918  0.9969918  0.99847328\n",
      " 1.         1.         1.         0.86569517 0.90803233 0.90003031\n",
      " 0.89030392 0.9004286  0.8981211  0.89509983 0.89387323 0.90038213\n",
      " 0.89784516 0.909714   0.90728434 0.91500282 0.90924842 0.91219543\n",
      " 0.90896353 0.91051374 0.90900266 0.92505812 0.95441103 0.95651472\n",
      " 0.95419826 0.95675565 0.95035751 0.94924541 0.9573028  0.955983\n",
      " 0.96722164 0.97463156 0.97324223 0.98810096 0.9838322  0.99703704\n",
      " 0.99851852 0.99562044 0.99701476 0.96410993 0.98268908 0.99391609\n",
      " 0.99699248 0.99849624 1.         1.         1.         1.\n",
      " 0.96751437 0.98205741 0.98942621 0.99851852 0.99701476 1.\n",
      " 1.         1.         1.         0.96868687 0.97896862 0.9923636\n",
      " 0.99849624 0.99701476 0.99849624 1.         1.         1.\n",
      " 0.96531998 0.98024417 0.99539636 0.99405062 0.99548872 0.99851852\n",
      " 1.         1.         1.         0.96535443 0.97638129 0.98336421\n",
      " 0.9940518  0.9969918  0.99696952 1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: GB - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 405 candidates, totalling 2025 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[92 10]\n",
      " [ 2 81]]\n",
      "[[81  2 10 92]\n",
      " [92 10  2 81]]\n",
      "[[173  12]\n",
      " [ 12 173]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 93.51351351351352\n",
      " Precision = [0.89010989 0.9787234 ]\n",
      " Recall = [0.97590361 0.90196078]\n",
      " F1 score = [0.93103448 0.93877551]\n",
      " AUC score = 93.89321993857784\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 93.51351351351352\n",
      " Precision = [0.9787234  0.89010989]\n",
      " Recall = [0.90196078 0.97590361]\n",
      " F1 score = [0.93877551 0.93103448]\n",
      " AUC score = 93.89321993857786\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[92 10]\n",
      " [ 2 81]]\n",
      " Accuracy (acc): 93.514\n",
      " Precision (prc): 89.011\n",
      " Recall (rec): 97.59\n",
      " Sensitivity (sns): 97.59\n",
      " Specificity (spc): 90.196\n",
      " F1 Score (f1s): 93.103\n",
      " ROC AUC (AUC): 0.939\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GradientBoostingClassifier(learning_rate=0.05, max_depth=2, n_estimators=50) \n",
      "        Best parameters of the model: {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 50} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
      "\n",
      "                                               model  \\\n",
      "0  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'learning_rate': 0.05, 'max_depth': 2, 'n_est...         94.12   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[92, 10], [2, 81]]    93.514     89.011   97.59        97.59       90.196   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    93.103    0.939   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:3, TRAINING:1 AND MODEL: XGB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: xtreme_gradient_boosting_parameters\n",
      "All parameters: ['max_depth', 'eta', 'max_leaves'], [[2, 3, 6, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: XGB - {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[93  9]\n",
      " [ 4 79]]\n",
      "[[79  4  9 93]\n",
      " [93  9  4 79]]\n",
      "[[172  13]\n",
      " [ 13 172]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 92.97297297297298\n",
      " Precision = [0.89772727 0.95876289]\n",
      " Recall = [0.95180723 0.91176471]\n",
      " F1 score = [0.92397661 0.93467337]\n",
      " AUC score = 93.17859673990077\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 92.97297297297298\n",
      " Precision = [0.95876289 0.89772727]\n",
      " Recall = [0.91176471 0.95180723]\n",
      " F1 score = [0.93467337 0.92397661]\n",
      " AUC score = 93.17859673990077\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[93  9]\n",
      " [ 4 79]]\n",
      " Accuracy (acc): 92.973\n",
      " Precision (prc): 89.773\n",
      " Recall (rec): 95.181\n",
      " Sensitivity (sns): 95.181\n",
      " Specificity (spc): 91.176\n",
      " F1 Score (f1s): 92.398\n",
      " ROC AUC (AUC): 0.932\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False, eta=0.05,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.0500000007, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=2, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, ...) \n",
      "        Best parameters of the model: {'eta': 0.05, 'max_depth': 2} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
      "\n",
      "                                               model  \\\n",
      "0  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
      "\n",
      "                model_parameters  model_scores    confusion_matrix  accuracy  \\\n",
      "0  {'eta': 0.05, 'max_depth': 2}         94.12  [[93, 9], [4, 79]]    92.973   \n",
      "\n",
      "   precision  recall  sensitivity  specificity  f1_score  roc_auc  \n",
      "0     89.773  95.181       95.181       91.176    92.398    0.932   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            Best model index calculation  \n",
      "            \n",
      "WWWWWWWWWWWWWWWWWWW recall    Training_No  Model_No Model_Name  \\\n",
      "0            1         1         LR   \n",
      "1            1         2        SVC   \n",
      "2            1         3         NB   \n",
      "3            1         4        KNN   \n",
      "4            1         5         DT   \n",
      "5            1         6         RF   \n",
      "6            1         7         GB   \n",
      "7            1         8        XGB   \n",
      "\n",
      "                                              method  \\\n",
      "0  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
      "1  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
      "2  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
      "3  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
      "4  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
      "5  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
      "6  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
      "7  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
      "\n",
      "                                               model  \\\n",
      "0                    LogisticRegression(max_iter=50)   \n",
      "1             SVC(kernel='linear', probability=True)   \n",
      "2     GaussianNB(var_smoothing=0.005623413251903491)   \n",
      "3                             KNeighborsClassifier()   \n",
      "4                DecisionTreeClassifier(max_depth=3)   \n",
      "5  (DecisionTreeClassifier(max_depth=2, max_featu...   \n",
      "6  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
      "7  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0        {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'}         90.59   \n",
      "1  {'C': 1.0, 'kernel': 'linear', 'probability': ...         91.76   \n",
      "2            {'var_smoothing': 0.005623413251903491}         88.24   \n",
      "3          {'metric': 'minkowski', 'n_neighbors': 5}         91.76   \n",
      "4              {'criterion': 'gini', 'max_depth': 3}         92.94   \n",
      "5  {'criterion': 'gini', 'max_depth': 2, 'n_estim...         94.12   \n",
      "6  {'learning_rate': 0.05, 'max_depth': 2, 'n_est...         94.12   \n",
      "7                      {'eta': 0.05, 'max_depth': 2}         94.12   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[91, 11], [9, 74]]    89.189     87.059  89.157       89.157       89.216   \n",
      "1  [[91, 11], [8, 75]]    89.730     87.209  90.361       90.361       89.216   \n",
      "2  [[87, 15], [5, 78]]    89.189     83.871  93.976       93.976       85.294   \n",
      "3  [[90, 12], [4, 79]]    91.351     86.813  95.181       95.181       88.235   \n",
      "4   [[93, 9], [3, 80]]    93.514     89.888  96.386       96.386       91.176   \n",
      "5  [[89, 13], [7, 76]]    89.189     85.393  91.566       91.566       87.255   \n",
      "6  [[92, 10], [2, 81]]    93.514     89.011  97.590       97.590       90.196   \n",
      "7   [[93, 9], [4, 79]]    92.973     89.773  95.181       95.181       91.176   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    88.095    0.892  \n",
      "1    88.757    0.898  \n",
      "2    88.636    0.896  \n",
      "3    90.805    0.917  \n",
      "4    93.023    0.938  \n",
      "5    88.372    0.894  \n",
      "6    93.103    0.939  \n",
      "7    92.398    0.932  \n",
      "\n",
      "            ***************************************************************************************************\n",
      "            TRAINING 1 END... \n",
      "            \n",
      "train- tri, tmp_train_ids, tmp_val_ids, tr_dat, val_dat 1 [ 59 100 136 174  69  10  13  49  75  26 147 113  92 178  54   1  74  85\n",
      " 128 171  99 119  62 142  81 166   6  44 145 104 135 156 169  95 133 124\n",
      "  70  60 137 105 162  33  72 122   9 180  68 102 123 127 149  51 112 103\n",
      "  34  39 150  64  86 143   3 157  40  97  25  16 158  11  96  46  45  80\n",
      "  15 167 134 140 126 170  58 146 115 154 184  37  21 125  82 129  77  20\n",
      "  12  18  29  22 175  94  84  57  61 106 159 121  91  48 131  78  19  14\n",
      " 179  41 101  90  27 120 132 139 177  23  89  65 168 161  83 111 163 176\n",
      " 155  55 173 160  79  73  71  38   4 116  43 110 165 181  47 141  76  88\n",
      " 144  56  36   5 108  93 148   8 164 183 130  28 114 152  52  17  63  32\n",
      "  31 117 172  35  50  24  87  53 153   0 138  98 107   7  67  66 118   2\n",
      " 182  30 151  42 109] [ 59 100 136 174  69  10  13  49  75  26 147 113  92 178  54   1  74  85\n",
      " 128 171  99 119  62 142  81 166   6  44 145 104 135 156 169  95 133 124\n",
      "  70  60 137 105 162  33  72 122   9 180  68 102 123 127 149  51 112 103\n",
      "  34  39 150  64  86 143   3 157  40  97  25  16 158  11  96  46  45  80\n",
      "  15 167 134 140 126 170  58 146 115 154 184  37  21 125  82 129  77  20\n",
      "  12  18  29  22 175  94  84  57  61 106 159 121  91  48 131  78  19  14\n",
      " 179  41 101  90  27 120 132 139 177  23  89  65 168 161  83 111 163 176\n",
      " 155  55 173 160  79  73  71  38   4 116  43 110 165 181  47 141  76  88\n",
      " 144  56  36   5 108  93 148   8 164 183 130  28 114 152  52  17  63  32\n",
      "  31 117 172  35  50  24  87  53 153   0 138  98 107   7  67  66 118   2\n",
      " 182  30 151  42 109] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "            *** MODEL TRAINING AND VALIDATION PHASE FOR TEST 3 \n",
      "            TRAINING 2 START... XXXXX \n",
      "            ***************************************************************************************************\n",
      "            Validation=> 185 ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            Training=> 185 ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            \n",
      "\n",
      "            *** ML MODEL FOR TEST:3, TRAINING:2 AND MODEL: LR \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: logistic_regression_parameters\n",
      "All parameters: ['penalty', 'solver', 'max_iter', 'C'], [['l2', 'elasticnet'], [50, 100, 130, 150, 170, 200], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]], [0, 2, 3]\n",
      "Parameters:  {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: LR - {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 84 candidates, totalling 420 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[91 11]\n",
      " [ 9 74]]\n",
      "[[74  9 11 91]\n",
      " [91 11  9 74]]\n",
      "[[165  20]\n",
      " [ 20 165]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 89.1891891891892\n",
      " Precision = [0.87058824 0.91      ]\n",
      " Recall = [0.89156627 0.89215686]\n",
      " F1 score = [0.88095238 0.9009901 ]\n",
      " AUC score = 89.18615639026694\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 89.1891891891892\n",
      " Precision = [0.91       0.87058824]\n",
      " Recall = [0.89215686 0.89156627]\n",
      " F1 score = [0.9009901  0.88095238]\n",
      " AUC score = 89.18615639026693\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[91 11]\n",
      " [ 9 74]]\n",
      " Accuracy (acc): 89.189\n",
      " Precision (prc): 87.059\n",
      " Recall (rec): 89.157\n",
      " Sensitivity (sns): 89.157\n",
      " Specificity (spc): 89.216\n",
      " F1 Score (f1s): 88.095\n",
      " ROC AUC (AUC): 0.892\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: LogisticRegression(max_iter=50) \n",
      "        Best parameters of the model: {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
      "\n",
      "                             model  \\\n",
      "0  LogisticRegression(max_iter=50)   \n",
      "\n",
      "                              model_parameters  model_scores  \\\n",
      "0  {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'}         90.59   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[91, 11], [9, 74]]    89.189     87.059  89.157       89.157       89.216   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    88.095    0.892   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:3, TRAINING:2 AND MODEL: SVC \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: support_vector_classifier_parameters\n",
      "All parameters: ['probability', 'C', 'kernel', 'gamma', 'degree', 'class_weightdict'], [[True], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], ['linear', 'rbf', 'poly']], [0, 1, 2]\n",
      "Parameters:  {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "210 fits failed out of a total of 420.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.87058824        nan 0.87058824        nan 0.87058824        nan\n",
      " 0.87058824        nan 0.87058824        nan 0.87058824        nan\n",
      " 0.87058824        nan 0.87058824        nan 0.87058824        nan\n",
      " 0.87058824        nan 0.87058824        nan 0.87058824        nan\n",
      " 0.89411765        nan 0.89411765        nan 0.89411765        nan\n",
      " 0.89411765        nan 0.89411765        nan 0.89411765        nan\n",
      " 0.90588235        nan 0.90588235        nan 0.90588235        nan\n",
      " 0.90588235        nan 0.90588235        nan 0.90588235        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.86766169        nan 0.86766169        nan 0.86766169        nan\n",
      " 0.86766169        nan 0.86766169        nan 0.86766169        nan\n",
      " 0.86766169        nan 0.86766169        nan 0.86766169        nan\n",
      " 0.86766169        nan 0.86766169        nan 0.86766169        nan\n",
      " 0.87969245        nan 0.87969245        nan 0.87969245        nan\n",
      " 0.87969245        nan 0.87969245        nan 0.87969245        nan\n",
      " 0.89470828        nan 0.89470828        nan 0.89470828        nan\n",
      " 0.89470828        nan 0.89470828        nan 0.89470828        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.87253589        nan 0.87253589        nan 0.87253589        nan\n",
      " 0.87253589        nan 0.87253589        nan 0.87253589        nan\n",
      " 0.8506203         nan 0.8506203         nan 0.8506203         nan\n",
      " 0.8506203         nan 0.8506203         nan 0.8506203         nan\n",
      " 0.86243031        nan 0.86243031        nan 0.86243031        nan\n",
      " 0.86243031        nan 0.86243031        nan 0.86243031        nan\n",
      " 0.88249886        nan 0.88249886        nan 0.88249886        nan\n",
      " 0.88249886        nan 0.88249886        nan 0.88249886        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.86259916        nan 0.86259916        nan 0.86259916        nan\n",
      " 0.86259916        nan 0.86259916        nan 0.86259916        nan\n",
      " 0.85462585        nan 0.85462585        nan 0.85462585        nan\n",
      " 0.85462585        nan 0.85462585        nan 0.85462585        nan\n",
      " 0.86153249        nan 0.86153249        nan 0.86153249        nan\n",
      " 0.86153249        nan 0.86153249        nan 0.86153249        nan\n",
      " 0.87649469        nan 0.87649469        nan 0.87649469        nan\n",
      " 0.87649469        nan 0.87649469        nan 0.87649469        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: SVC - {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[91 11]\n",
      " [ 8 75]]\n",
      "[[75  8 11 91]\n",
      " [91 11  8 75]]\n",
      "[[166  19]\n",
      " [ 19 166]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 89.72972972972974\n",
      " Precision = [0.87209302 0.91919192]\n",
      " Recall = [0.90361446 0.89215686]\n",
      " F1 score = [0.88757396 0.90547264]\n",
      " AUC score = 89.78856602882117\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 89.72972972972974\n",
      " Precision = [0.91919192 0.87209302]\n",
      " Recall = [0.89215686 0.90361446]\n",
      " F1 score = [0.90547264 0.88757396]\n",
      " AUC score = 89.78856602882117\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[91 11]\n",
      " [ 8 75]]\n",
      " Accuracy (acc): 89.73\n",
      " Precision (prc): 87.209\n",
      " Recall (rec): 90.361\n",
      " Sensitivity (sns): 90.361\n",
      " Specificity (spc): 89.216\n",
      " F1 Score (f1s): 88.757\n",
      " ROC AUC (AUC): 0.898\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: SVC(kernel='linear', probability=True) \n",
      "        Best parameters of the model: {'C': 1.0, 'kernel': 'linear', 'probability': True} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
      "\n",
      "                                    model  \\\n",
      "0  SVC(kernel='linear', probability=True)   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'C': 1.0, 'kernel': 'linear', 'probability': ...         91.76   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[91, 11], [8, 75]]     89.73     87.209  90.361       90.361       89.216   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    88.757    0.898   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:3, TRAINING:2 AND MODEL: NB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: naive_bayes_parameters\n",
      "All parameters: ['var_smoothing'], [[1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]], [0]\n",
      "Parameters:  {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: NB - {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[87 15]\n",
      " [ 5 78]]\n",
      "[[78  5 15 87]\n",
      " [87 15  5 78]]\n",
      "[[165  20]\n",
      " [ 20 165]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 89.1891891891892\n",
      " Precision = [0.83870968 0.94565217]\n",
      " Recall = [0.93975904 0.85294118]\n",
      " F1 score = [0.88636364 0.89690722]\n",
      " AUC score = 89.63501063075834\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 89.1891891891892\n",
      " Precision = [0.94565217 0.83870968]\n",
      " Recall = [0.85294118 0.93975904]\n",
      " F1 score = [0.89690722 0.88636364]\n",
      " AUC score = 89.63501063075833\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[87 15]\n",
      " [ 5 78]]\n",
      " Accuracy (acc): 89.189\n",
      " Precision (prc): 83.871\n",
      " Recall (rec): 93.976\n",
      " Sensitivity (sns): 93.976\n",
      " Specificity (spc): 85.294\n",
      " F1 Score (f1s): 88.636\n",
      " ROC AUC (AUC): 0.896\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GaussianNB(var_smoothing=0.005623413251903491) \n",
      "        Best parameters of the model: {'var_smoothing': 0.005623413251903491} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
      "\n",
      "                                            model  \\\n",
      "0  GaussianNB(var_smoothing=0.005623413251903491)   \n",
      "\n",
      "                          model_parameters  model_scores     confusion_matrix  \\\n",
      "0  {'var_smoothing': 0.005623413251903491}         88.24  [[87, 15], [5, 78]]   \n",
      "\n",
      "   accuracy  precision  recall  sensitivity  specificity  f1_score  roc_auc  \n",
      "0    89.189     83.871  93.976       93.976       85.294    88.636    0.896   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:3, TRAINING:2 AND MODEL: KNN \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: k_nearest_neighbors_parameters\n",
      "All parameters: ['n_neighbors', 'p', 'metric', 'n_splits'], [[2, 3, 5, 10, 15, 25, 35], ['manhattan', 'minkowski', 'euclidean']], [0, 2]\n",
      "Parameters:  {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: KNN - {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[90 12]\n",
      " [ 4 79]]\n",
      "[[79  4 12 90]\n",
      " [90 12  4 79]]\n",
      "[[169  16]\n",
      " [ 16 169]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 91.35135135135135\n",
      " Precision = [0.86813187 0.95744681]\n",
      " Recall = [0.95180723 0.88235294]\n",
      " F1 score = [0.90804598 0.91836735]\n",
      " AUC score = 91.70800850460667\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 91.35135135135135\n",
      " Precision = [0.95744681 0.86813187]\n",
      " Recall = [0.88235294 0.95180723]\n",
      " F1 score = [0.91836735 0.90804598]\n",
      " AUC score = 91.70800850460665\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[90 12]\n",
      " [ 4 79]]\n",
      " Accuracy (acc): 91.351\n",
      " Precision (prc): 86.813\n",
      " Recall (rec): 95.181\n",
      " Sensitivity (sns): 95.181\n",
      " Specificity (spc): 88.235\n",
      " F1 Score (f1s): 90.805\n",
      " ROC AUC (AUC): 0.917\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: KNeighborsClassifier() \n",
      "        Best parameters of the model: {'metric': 'minkowski', 'n_neighbors': 5} \n",
      "        Best model scores:                                               method                   model  \\\n",
      "0  GridSearchCV(cv=5, estimator=KNeighborsClassif...  KNeighborsClassifier()   \n",
      "\n",
      "                            model_parameters  model_scores  \\\n",
      "0  {'metric': 'minkowski', 'n_neighbors': 5}         91.76   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[90, 12], [4, 79]]    91.351     86.813  95.181       95.181       88.235   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    90.805    0.917   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:3, TRAINING:2 AND MODEL: DT \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: decision_tree_parameters\n",
      "All parameters: ['max_depth', 'criterion', 'splitter', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: DT - {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[93  9]\n",
      " [ 5 78]]\n",
      "[[78  5  9 93]\n",
      " [93  9  5 78]]\n",
      "[[171  14]\n",
      " [ 14 171]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 92.43243243243244\n",
      " Precision = [0.89655172 0.94897959]\n",
      " Recall = [0.93975904 0.91176471]\n",
      " F1 score = [0.91764706 0.93      ]\n",
      " AUC score = 92.57618710134656\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 92.43243243243244\n",
      " Precision = [0.94897959 0.89655172]\n",
      " Recall = [0.91176471 0.93975904]\n",
      " F1 score = [0.93       0.91764706]\n",
      " AUC score = 92.57618710134658\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[93  9]\n",
      " [ 5 78]]\n",
      " Accuracy (acc): 92.432\n",
      " Precision (prc): 89.655\n",
      " Recall (rec): 93.976\n",
      " Sensitivity (sns): 93.976\n",
      " Specificity (spc): 91.176\n",
      " F1 Score (f1s): 91.765\n",
      " ROC AUC (AUC): 0.926\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: DecisionTreeClassifier(max_depth=2) \n",
      "        Best parameters of the model: {'criterion': 'gini', 'max_depth': 2} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
      "\n",
      "                                 model                       model_parameters  \\\n",
      "0  DecisionTreeClassifier(max_depth=2)  {'criterion': 'gini', 'max_depth': 2}   \n",
      "\n",
      "   model_scores    confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
      "0         92.94  [[93, 9], [5, 78]]    92.432     89.655  93.976       93.976   \n",
      "\n",
      "   specificity  f1_score  roc_auc  \n",
      "0       91.176    91.765    0.926   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:3, TRAINING:2 AND MODEL: RF \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: random_forest_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'criterion', 'splitter', 'max_features', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.92941176 0.92941176 0.89191176 0.89191176 0.88014706 0.88088235\n",
      " 0.87941176 0.88088235 0.86764706 0.91764706 0.92941176 0.83382353\n",
      " 0.85661765 0.86911765 0.86985294 0.86985294 0.82058824 0.86985294\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.94287653 0.96689281 0.97589326 1.         1.         1.\n",
      " 1.         1.         1.         0.94581637 0.96399819 0.95187698\n",
      " 0.99099955 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.89817672 0.88103386 0.85876646 0.85623863 0.8462978  0.85748607\n",
      " 0.85515511 0.8487367  0.85653205 0.89777778 0.8914361  0.83560437\n",
      " 0.85117461 0.86362085 0.86224415 0.86224415 0.82777361 0.85672691\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.91992779 0.93121037 0.97023713 0.99851852 1.         1.\n",
      " 1.         1.         1.         0.91766768 0.92562564 0.9603443\n",
      " 0.99544298 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: RF - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[100   2]\n",
      " [  0  83]]\n",
      "[[ 83   0   2 100]\n",
      " [100   2   0  83]]\n",
      "[[183   2]\n",
      " [  2 183]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 98.91891891891892\n",
      " Precision = [0.97647059 1.        ]\n",
      " Recall = [1.         0.98039216]\n",
      " F1 score = [0.98809524 0.99009901]\n",
      " AUC score = 99.01960784313727\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 98.91891891891892\n",
      " Precision = [1.         0.97647059]\n",
      " Recall = [0.98039216 1.        ]\n",
      " F1 score = [0.99009901 0.98809524]\n",
      " AUC score = 99.01960784313725\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[100   2]\n",
      " [  0  83]]\n",
      " Accuracy (acc): 98.919\n",
      " Precision (prc): 97.647\n",
      " Recall (rec): 100.0\n",
      " Sensitivity (sns): 100.0\n",
      " Specificity (spc): 98.039\n",
      " F1 Score (f1s): 98.81\n",
      " ROC AUC (AUC): 0.99\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: RandomForestClassifier(max_depth=7, n_estimators=15) \n",
      "        Best parameters of the model: {'criterion': 'gini', 'max_depth': 7, 'n_estimators': 15} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
      "\n",
      "                                               model  \\\n",
      "0  (DecisionTreeClassifier(max_depth=7, max_featu...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'criterion': 'gini', 'max_depth': 7, 'n_estim...         94.12   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[100, 2], [0, 83]]    98.919     97.647   100.0        100.0       98.039   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0     98.81     0.99   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:3, TRAINING:2 AND MODEL: GB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: gradient_boosting_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'learning_rate', 'max_features', 'loss', 'min_samples_split', 'min_samples_leaf'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.5, 1.0]], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "405 fits failed out of a total of 1215.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 467, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.90514706 0.91764706 0.91764706 0.91764706 0.92941176 0.91764706\n",
      " 0.91764706 0.91764706 0.91764706 0.91691176 0.90588235 0.91764706\n",
      " 0.92941176 0.87058824 0.91764706 0.90588235 0.91764706 0.91764706\n",
      " 0.91691176 0.88014706 0.89264706 0.92941176 0.91691176 0.90514706\n",
      " 0.92941176 0.92941176 0.92941176 0.89264706 0.86985294 0.92941176\n",
      " 0.94117647 0.92941176 0.92941176 0.91691176 0.91691176 0.91764706\n",
      " 0.84411765 0.88014706 0.89338235 0.86838235 0.92941176 0.92941176\n",
      " 0.91691176 0.92941176 0.92941176 0.77132353 0.90294118 0.88161765\n",
      " 0.89338235 0.92867647 0.90514706 0.92941176 0.94117647 0.90514706\n",
      " 0.86838235 0.83308824 0.91764706 0.89264706 0.89264706 0.91617647\n",
      " 0.92941176 0.92941176 0.89264706 0.84485294 0.84485294 0.91764706\n",
      " 0.90514706 0.91764706 0.91691176 0.91764706 0.92941176 0.94117647\n",
      " 0.82132353 0.94044118 0.86911765 0.89338235 0.94117647 0.88014706\n",
      " 0.91691176 0.89338235 0.92941176 0.82352941 0.89411765 0.89411765\n",
      " 0.87058824 0.91764706 0.91764706 0.91764706 0.91764706 0.91764706\n",
      " 0.86838235 0.91764706 0.92941176 0.92941176 0.88235294 0.92941176\n",
      " 0.91764706 0.91764706 0.91764706 0.82132353 0.94117647 0.90588235\n",
      " 0.92941176 0.94117647 0.92941176 0.89411765 0.92941176 0.92941176\n",
      " 0.75882353 0.86838235 0.80882353 0.92867647 0.91764706 0.92941176\n",
      " 0.92941176 0.92867647 0.92941176 0.87941176 0.89264706 0.84485294\n",
      " 0.91691176 0.91764706 0.92867647 0.91691176 0.94117647 0.92941176\n",
      " 0.83235294 0.88014706 0.86911765 0.94117647 0.90514706 0.91764706\n",
      " 0.92941176 0.92941176 0.92941176 0.88088235 0.78382353 0.89338235\n",
      " 0.91691176 0.92867647 0.92941176 0.94117647 0.92941176 0.90514706\n",
      " 0.82132353 0.91691176 0.90514706 0.94117647 0.94117647 0.90588235\n",
      " 0.91691176 0.92941176 0.94117647 0.78529412 0.89338235 0.85661765\n",
      " 0.92867647 0.91691176 0.92941176 0.92941176 0.92941176 0.92941176\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.90081411 0.9278607  0.90388964 0.90972411 0.90981456 0.91289009\n",
      " 0.91872456 0.92781547 0.92781547 0.9488919  0.94875622 0.93984622\n",
      " 0.9458616  0.9488919  0.95187698 0.93984622 0.93686115 0.94278607\n",
      " 0.96698327 0.9608322  0.99398462 0.99095432 0.99701493 0.9969697\n",
      " 1.         0.99398462 1.         0.96996834 0.97593849 0.98493894\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.97286296 0.97598372 0.98489371 0.9969697  1.         1.\n",
      " 1.         1.         1.         0.96386251 0.98787879 0.98493894\n",
      " 1.         1.         1.         0.9969697  1.         1.\n",
      " 0.97286296 0.97896879 0.97589326 0.9969697  1.         1.\n",
      " 1.         1.         1.         0.96689281 0.98195387 0.98493894\n",
      " 1.         0.9969697  1.         1.         1.         1.\n",
      " 0.95798281 0.98195387 0.99398462 0.99090909 1.         1.\n",
      " 1.         1.         1.         0.88557214 0.92175486 0.92772501\n",
      " 0.90981456 0.91587517 0.92777024 0.93080054 0.92175486 0.92483039\n",
      " 0.93966531 0.9608322  0.95178652 0.93080054 0.93686115 0.9488919\n",
      " 0.93681592 0.93984622 0.9428313  0.95486205 0.96992311 0.98801447\n",
      " 0.99095432 0.99402985 0.99099955 0.99393939 1.         1.\n",
      " 0.96684758 0.96992311 0.98796924 0.9969697  1.         0.9969697\n",
      " 1.         1.         1.         0.97295341 0.98489371 0.97887834\n",
      " 1.         1.         0.99701493 0.99701493 1.         1.\n",
      " 0.95499774 0.98801447 0.9850294  1.         0.9969697  1.\n",
      " 1.         1.         1.         0.97584803 0.98796924 0.99701493\n",
      " 0.99398462 1.         1.         1.         1.         1.\n",
      " 0.98195387 0.98498417 1.         1.         1.         0.9969697\n",
      " 1.         1.         1.         0.98489371 0.98801447 0.98792402\n",
      " 0.9969697  0.99090909 1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.90102282 0.89276637 0.891791   0.89454545 0.90508961 0.90060606\n",
      " 0.89777778 0.88884135 0.90060606 0.90226051 0.89714286 0.89014011\n",
      " 0.90376812 0.8745898  0.89777778 0.89426357 0.89777778 0.89777778\n",
      " 0.88572173 0.86866852 0.88770283 0.899029   0.90146628 0.89132616\n",
      " 0.91087772 0.90508961 0.90508961 0.86061312 0.85998066 0.90568182\n",
      " 0.89654003 0.90791789 0.90501934 0.89863799 0.89863799 0.899029\n",
      " 0.85918571 0.87277349 0.89192611 0.87514593 0.90508961 0.90791789\n",
      " 0.89863799 0.90508961 0.90508961 0.76560459 0.88280771 0.86615978\n",
      " 0.88506354 0.90549283 0.8983655  0.90791789 0.91107994 0.89257739\n",
      " 0.86922691 0.84212474 0.899029   0.88770283 0.87030303 0.89774661\n",
      " 0.90508961 0.90508961 0.88569567 0.84146825 0.8513387  0.899029\n",
      " 0.89132616 0.89276637 0.90146628 0.90060606 0.90508961 0.91107994\n",
      " 0.83160394 0.91148317 0.87212121 0.87095098 0.91107994 0.87924405\n",
      " 0.90442611 0.88582146 0.90508961 0.81290835 0.86573613 0.8735023\n",
      " 0.87859649 0.90356589 0.90060606 0.88063492 0.90356589 0.90060606\n",
      " 0.85734065 0.87916964 0.90791789 0.90791789 0.87493612 0.90508961\n",
      " 0.90060606 0.89777778 0.89777778 0.8353221  0.90830474 0.89227307\n",
      " 0.90376812 0.90791789 0.90508961 0.8904065  0.90508961 0.90508961\n",
      " 0.7692639  0.8683222  0.84269556 0.90185312 0.899029   0.90227273\n",
      " 0.90508961 0.90462833 0.90508961 0.8795254  0.87051479 0.86456095\n",
      " 0.89863799 0.89620072 0.90185312 0.89863799 0.91107994 0.90508961\n",
      " 0.82633229 0.87482513 0.86723397 0.91477273 0.89512378 0.89268998\n",
      " 0.90501934 0.90508961 0.90508961 0.87147095 0.77946237 0.8836885\n",
      " 0.89943223 0.90462833 0.90791789 0.91477273 0.90508961 0.89540567\n",
      " 0.81639003 0.89621212 0.88526556 0.90564516 0.90259045 0.89551479\n",
      " 0.89863799 0.90508961 0.91107994 0.8261624  0.86416762 0.87037624\n",
      " 0.89919355 0.89863799 0.90508961 0.90791789 0.90238429 0.90508961\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.87988384 0.90140645 0.88964691 0.89192105 0.89160255 0.89071177\n",
      " 0.89796912 0.90159219 0.89908716 0.91281786 0.91651964 0.90781792\n",
      " 0.91506886 0.91276689 0.916919   0.90929479 0.91025413 0.91344495\n",
      " 0.93968221 0.93731624 0.95709094 0.96369131 0.96135067 0.96268441\n",
      " 0.96690095 0.9583525  0.96135561 0.96269096 0.96888658 0.97624814\n",
      " 0.99703704 0.98823336 0.9911316  0.9940513  0.99553344 0.995511\n",
      " 0.96289041 0.9818634  0.98500142 0.99250297 1.         1.\n",
      " 1.         1.         1.         0.96245638 0.98057381 0.98933488\n",
      " 0.99701476 0.99407358 1.         0.99847328 1.         1.\n",
      " 0.97584786 0.97753136 0.98181627 0.99400673 0.99553344 1.\n",
      " 1.         1.         1.         0.96682324 0.97914432 0.99088561\n",
      " 0.99402952 0.99548804 0.99851852 1.         1.         1.\n",
      " 0.95937091 0.97628988 0.99696952 0.99539636 1.         1.\n",
      " 1.         1.         1.         0.87193817 0.88234489 0.89175129\n",
      " 0.88930596 0.89369034 0.90037716 0.90321374 0.89711008 0.89871449\n",
      " 0.9072815  0.9152346  0.90916652 0.90183913 0.91154909 0.91793027\n",
      " 0.90773674 0.91060595 0.9134609  0.93839856 0.94199696 0.94866352\n",
      " 0.95139918 0.95837526 0.9514465  0.95291649 0.96137425 0.955983\n",
      " 0.96250156 0.96707222 0.97929385 0.98370204 0.99407358 0.98679004\n",
      " 1.         1.         0.99851852 0.96729003 0.9835632  0.98334212\n",
      " 0.99407358 0.99849624 0.99849624 0.99849624 0.99849624 1.\n",
      " 0.95625307 0.97633889 0.98644321 0.99701493 0.99548804 1.\n",
      " 1.         1.         1.         0.96878467 0.97365483 0.99551117\n",
      " 0.99252525 0.99851852 0.99851852 1.         0.99851852 1.\n",
      " 0.97180471 0.97060196 0.99701476 0.99261389 0.99413896 0.99847328\n",
      " 1.         1.         1.         0.97907869 0.98219845 0.99238884\n",
      " 0.99551032 0.99247867 1.         0.99701476 1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: GB - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 405 candidates, totalling 2025 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[101   1]\n",
      " [  1  82]]\n",
      "[[ 82   1   1 101]\n",
      " [101   1   1  82]]\n",
      "[[183   2]\n",
      " [  2 183]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 98.91891891891892\n",
      " Precision = [0.98795181 0.99019608]\n",
      " Recall = [0.98795181 0.99019608]\n",
      " F1 score = [0.98795181 0.99019608]\n",
      " AUC score = 98.90739428301441\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 98.91891891891892\n",
      " Precision = [0.99019608 0.98795181]\n",
      " Recall = [0.99019608 0.98795181]\n",
      " F1 score = [0.99019608 0.98795181]\n",
      " AUC score = 98.90739428301441\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[101   1]\n",
      " [  1  82]]\n",
      " Accuracy (acc): 98.919\n",
      " Precision (prc): 98.795\n",
      " Recall (rec): 98.795\n",
      " Sensitivity (sns): 98.795\n",
      " Specificity (spc): 99.02\n",
      " F1 Score (f1s): 98.795\n",
      " ROC AUC (AUC): 0.989\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GradientBoostingClassifier(learning_rate=0.01, max_depth=5) \n",
      "        Best parameters of the model: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
      "\n",
      "                                               model  \\\n",
      "0  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'learning_rate': 0.01, 'max_depth': 5, 'n_est...         94.12   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[101, 1], [1, 82]]    98.919     98.795  98.795       98.795        99.02   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    98.795    0.989   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:3, TRAINING:2 AND MODEL: XGB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: xtreme_gradient_boosting_parameters\n",
      "All parameters: ['max_depth', 'eta', 'max_leaves'], [[2, 3, 6, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco4', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm7', 'plm8', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: XGB - {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[93  9]\n",
      " [ 4 79]]\n",
      "[[79  4  9 93]\n",
      " [93  9  4 79]]\n",
      "[[172  13]\n",
      " [ 13 172]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 92.97297297297298\n",
      " Precision = [0.89772727 0.95876289]\n",
      " Recall = [0.95180723 0.91176471]\n",
      " F1 score = [0.92397661 0.93467337]\n",
      " AUC score = 93.17859673990077\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 92.97297297297298\n",
      " Precision = [0.95876289 0.89772727]\n",
      " Recall = [0.91176471 0.95180723]\n",
      " F1 score = [0.93467337 0.92397661]\n",
      " AUC score = 93.17859673990077\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[93  9]\n",
      " [ 4 79]]\n",
      " Accuracy (acc): 92.973\n",
      " Precision (prc): 89.773\n",
      " Recall (rec): 95.181\n",
      " Sensitivity (sns): 95.181\n",
      " Specificity (spc): 91.176\n",
      " F1 Score (f1s): 92.398\n",
      " ROC AUC (AUC): 0.932\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False, eta=0.05,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.0500000007, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=2, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, ...) \n",
      "        Best parameters of the model: {'eta': 0.05, 'max_depth': 2} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
      "\n",
      "                                               model  \\\n",
      "0  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
      "\n",
      "                model_parameters  model_scores    confusion_matrix  accuracy  \\\n",
      "0  {'eta': 0.05, 'max_depth': 2}         94.12  [[93, 9], [4, 79]]    92.973   \n",
      "\n",
      "   precision  recall  sensitivity  specificity  f1_score  roc_auc  \n",
      "0     89.773  95.181       95.181       91.176    92.398    0.932   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            Best model index calculation  \n",
      "            \n",
      "\n",
      "            ***************************************************************************************************\n",
      "            TRAINING 2 END... \n",
      "            \n",
      "\n",
      "        ### MODEL EVALUATION PHASE \n",
      "        EVALUATION 3 START... XXXXX \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "        From training? False, Data shape: (45, 18), Indices: [51, 4, 52, 14, 15, 16, 17, 28, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 53, 54, 75, 76, 89, 90, 91, 92, 47, 113, 114, 115, 116, 117, 48, 49, 50]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (45, 14), Target shape: (45,), Metadata: (45, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1] [1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[20  5]\n",
      " [ 2 18]]\n",
      "[[18  2  5 20]\n",
      " [20  5  2 18]]\n",
      "[[38  7]\n",
      " [ 7 38]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 84.44444444444444\n",
      " Precision = [0.7826087  0.90909091]\n",
      " Recall = [0.9 0.8]\n",
      " F1 score = [0.8372093  0.85106383]\n",
      " AUC score = 85.00000000000001\n",
      " Support = [20 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 84.44444444444444\n",
      " Precision = [0.90909091 0.7826087 ]\n",
      " Recall = [0.8 0.9]\n",
      " F1 score = [0.85106383 0.8372093 ]\n",
      " AUC score = 85.0\n",
      " Support = [25 20]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[20  5]\n",
      " [ 2 18]]\n",
      " Accuracy (acc): 84.444\n",
      " Precision (prc): 78.261\n",
      " Recall (rec): 90.0\n",
      " Sensitivity (sns): 90.0\n",
      " Specificity (spc): 80.0\n",
      " F1 Score (f1s): 83.721\n",
      " ROC AUC (AUC): 0.85\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1] [1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[20  5]\n",
      " [ 2 18]]\n",
      "[[18  2  5 20]\n",
      " [20  5  2 18]]\n",
      "[[38  7]\n",
      " [ 7 38]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 84.44444444444444\n",
      " Precision = [0.7826087  0.90909091]\n",
      " Recall = [0.9 0.8]\n",
      " F1 score = [0.8372093  0.85106383]\n",
      " AUC score = 85.00000000000001\n",
      " Support = [20 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 84.44444444444444\n",
      " Precision = [0.90909091 0.7826087 ]\n",
      " Recall = [0.8 0.9]\n",
      " F1 score = [0.85106383 0.8372093 ]\n",
      " AUC score = 85.0\n",
      " Support = [25 20]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[20  5]\n",
      " [ 2 18]]\n",
      " Accuracy (acc): 84.444\n",
      " Precision (prc): 78.261\n",
      " Recall (rec): 90.0\n",
      " Sensitivity (sns): 90.0\n",
      " Specificity (spc): 80.0\n",
      " F1 Score (f1s): 83.721\n",
      " ROC AUC (AUC): 0.85\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1] [1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[20  5]\n",
      " [ 2 18]]\n",
      "[[18  2  5 20]\n",
      " [20  5  2 18]]\n",
      "[[38  7]\n",
      " [ 7 38]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 84.44444444444444\n",
      " Precision = [0.7826087  0.90909091]\n",
      " Recall = [0.9 0.8]\n",
      " F1 score = [0.8372093  0.85106383]\n",
      " AUC score = 85.00000000000001\n",
      " Support = [20 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 84.44444444444444\n",
      " Precision = [0.90909091 0.7826087 ]\n",
      " Recall = [0.8 0.9]\n",
      " F1 score = [0.85106383 0.8372093 ]\n",
      " AUC score = 85.0\n",
      " Support = [25 20]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[20  5]\n",
      " [ 2 18]]\n",
      " Accuracy (acc): 84.444\n",
      " Precision (prc): 78.261\n",
      " Recall (rec): 90.0\n",
      " Sensitivity (sns): 90.0\n",
      " Specificity (spc): 80.0\n",
      " F1 Score (f1s): 83.721\n",
      " ROC AUC (AUC): 0.85\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1] [1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[20  5]\n",
      " [ 3 17]]\n",
      "[[17  3  5 20]\n",
      " [20  5  3 17]]\n",
      "[[37  8]\n",
      " [ 8 37]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 82.22222222222221\n",
      " Precision = [0.77272727 0.86956522]\n",
      " Recall = [0.85 0.8 ]\n",
      " F1 score = [0.80952381 0.83333333]\n",
      " AUC score = 82.5\n",
      " Support = [20 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 82.22222222222221\n",
      " Precision = [0.86956522 0.77272727]\n",
      " Recall = [0.8  0.85]\n",
      " F1 score = [0.83333333 0.80952381]\n",
      " AUC score = 82.5\n",
      " Support = [25 20]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[20  5]\n",
      " [ 3 17]]\n",
      " Accuracy (acc): 82.222\n",
      " Precision (prc): 77.273\n",
      " Recall (rec): 85.0\n",
      " Sensitivity (sns): 85.0\n",
      " Specificity (spc): 80.0\n",
      " F1 Score (f1s): 80.952\n",
      " ROC AUC (AUC): 0.825\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1] [1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "[[19  6]\n",
      " [ 4 16]]\n",
      "[[16  4  6 19]\n",
      " [19  6  4 16]]\n",
      "[[35 10]\n",
      " [10 35]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 77.77777777777779\n",
      " Precision = [0.72727273 0.82608696]\n",
      " Recall = [0.8  0.76]\n",
      " F1 score = [0.76190476 0.79166667]\n",
      " AUC score = 78.0\n",
      " Support = [20 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 77.77777777777779\n",
      " Precision = [0.82608696 0.72727273]\n",
      " Recall = [0.76 0.8 ]\n",
      " F1 score = [0.79166667 0.76190476]\n",
      " AUC score = 78.0\n",
      " Support = [25 20]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[19  6]\n",
      " [ 4 16]]\n",
      " Accuracy (acc): 77.778\n",
      " Precision (prc): 72.727\n",
      " Recall (rec): 80.0\n",
      " Sensitivity (sns): 80.0\n",
      " Specificity (spc): 76.0\n",
      " F1 Score (f1s): 76.19\n",
      " ROC AUC (AUC): 0.78\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1] [1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[20  5]\n",
      " [ 3 17]]\n",
      "[[17  3  5 20]\n",
      " [20  5  3 17]]\n",
      "[[37  8]\n",
      " [ 8 37]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 82.22222222222221\n",
      " Precision = [0.77272727 0.86956522]\n",
      " Recall = [0.85 0.8 ]\n",
      " F1 score = [0.80952381 0.83333333]\n",
      " AUC score = 82.5\n",
      " Support = [20 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 82.22222222222221\n",
      " Precision = [0.86956522 0.77272727]\n",
      " Recall = [0.8  0.85]\n",
      " F1 score = [0.83333333 0.80952381]\n",
      " AUC score = 82.5\n",
      " Support = [25 20]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[20  5]\n",
      " [ 3 17]]\n",
      " Accuracy (acc): 82.222\n",
      " Precision (prc): 77.273\n",
      " Recall (rec): 85.0\n",
      " Sensitivity (sns): 85.0\n",
      " Specificity (spc): 80.0\n",
      " F1 Score (f1s): 80.952\n",
      " ROC AUC (AUC): 0.825\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1] [1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "[[19  6]\n",
      " [ 5 15]]\n",
      "[[15  5  6 19]\n",
      " [19  6  5 15]]\n",
      "[[34 11]\n",
      " [11 34]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 75.55555555555556\n",
      " Precision = [0.71428571 0.79166667]\n",
      " Recall = [0.75 0.76]\n",
      " F1 score = [0.73170732 0.7755102 ]\n",
      " AUC score = 75.5\n",
      " Support = [20 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 75.55555555555556\n",
      " Precision = [0.79166667 0.71428571]\n",
      " Recall = [0.76 0.75]\n",
      " F1 score = [0.7755102  0.73170732]\n",
      " AUC score = 75.5\n",
      " Support = [25 20]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[19  6]\n",
      " [ 5 15]]\n",
      " Accuracy (acc): 75.556\n",
      " Precision (prc): 71.429\n",
      " Recall (rec): 75.0\n",
      " Sensitivity (sns): 75.0\n",
      " Specificity (spc): 76.0\n",
      " F1 Score (f1s): 73.171\n",
      " ROC AUC (AUC): 0.755\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1] [1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[19  6]\n",
      " [ 3 17]]\n",
      "[[17  3  6 19]\n",
      " [19  6  3 17]]\n",
      "[[36  9]\n",
      " [ 9 36]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 80.0\n",
      " Precision = [0.73913043 0.86363636]\n",
      " Recall = [0.85 0.76]\n",
      " F1 score = [0.79069767 0.80851064]\n",
      " AUC score = 80.5\n",
      " Support = [20 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 80.0\n",
      " Precision = [0.86363636 0.73913043]\n",
      " Recall = [0.76 0.85]\n",
      " F1 score = [0.80851064 0.79069767]\n",
      " AUC score = 80.5\n",
      " Support = [25 20]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[19  6]\n",
      " [ 3 17]]\n",
      " Accuracy (acc): 80.0\n",
      " Precision (prc): 73.913\n",
      " Recall (rec): 85.0\n",
      " Sensitivity (sns): 85.0\n",
      " Specificity (spc): 76.0\n",
      " F1 Score (f1s): 79.07\n",
      " ROC AUC (AUC): 0.805\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "            ===================================================================================================\n",
      "            TEST 3 END...\n",
      "            \n",
      "test- tsi, train_all_ids, test_ids, tr_all_dat, ts_dat 3 [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [5, 18, 19, 20, 21, 62, 29, 55, 56, 57, 58, 59, 60, 61, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 77, 78, 93, 94, 95, 96, 118, 119, 120, 121, 122] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['sdb4', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'nfle32', 'narco4', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'plm7', 'plm8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'n16', 'n1101', 'n1102', 'n1103', 'n1104']\n",
      "\n",
      "            ### MODEL TEST PHASE \n",
      "            TEST 4 START... XXXXX \n",
      "            ===================================================================================================\n",
      "            Test=> 45 ['sdb4', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'nfle32', 'narco4', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021', 'plm7', 'plm8', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'n16', 'n1101', 'n1102', 'n1103', 'n1104'] \n",
      "            Training (Including Validation)=> 185 ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            \n",
      "Random 5 percentage splitting testing...\n",
      "train- tri, tmp_train_ids, tmp_val_ids, tr_dat, val_dat 0 [ 94 119  23  15  24  31   3   0 118  34  38  47  16  74  86  54  43 154\n",
      " 129 143 123  83 183  21 161 181 127 170 139  46 182  80  49 172  98 114\n",
      " 137 147  70   5 179 106  82  30   7 107  90  51 108 105 109  67  72  41\n",
      "  95  93  39 152  81  55  85  25 159  42  48 178 145  18 116  91 110 101\n",
      " 184  12 168  50   6 142  73 124 131 136  89 149  22  92 160 144  14 171\n",
      "  45 122 141 169 100  10 175  79  19 104  87  71 120 112   8 111  60 130\n",
      " 155 126 174 162   1 151  69   9 135   4 113 138  56 165 148  26 150 125\n",
      " 164  62 117  88  52 156 102  40 167 176 177 157  17  36  13  57  68  33\n",
      "  84  20 134 121  27  75  65 180  66 153 132  29 173  97  76 128  59  37\n",
      "  28  61 163 158 133  58  96  53  78  64 146  11  44  77 140   2  35  99\n",
      "  63  32 103 166 115] [ 94 119  23  15  24  31   3   0 118  34  38  47  16  74  86  54  43 154\n",
      " 129 143 123  83 183  21 161 181 127 170 139  46 182  80  49 172  98 114\n",
      " 137 147  70   5 179 106  82  30   7 107  90  51 108 105 109  67  72  41\n",
      "  95  93  39 152  81  55  85  25 159  42  48 178 145  18 116  91 110 101\n",
      " 184  12 168  50   6 142  73 124 131 136  89 149  22  92 160 144  14 171\n",
      "  45 122 141 169 100  10 175  79  19 104  87  71 120 112   8 111  60 130\n",
      " 155 126 174 162   1 151  69   9 135   4 113 138  56 165 148  26 150 125\n",
      " 164  62 117  88  52 156 102  40 167 176 177 157  17  36  13  57  68  33\n",
      "  84  20 134 121  27  75  65 180  66 153 132  29 173  97  76 128  59  37\n",
      "  28  61 163 158 133  58  96  53  78  64 146  11  44  77 140   2  35  99\n",
      "  63  32 103 166 115] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "            *** MODEL TRAINING AND VALIDATION PHASE FOR TEST 4 \n",
      "            TRAINING 1 START... XXXXX \n",
      "            ***************************************************************************************************\n",
      "            Validation=> 185 ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            Training=> 185 ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            \n",
      "\n",
      "            *** ML MODEL FOR TEST:4, TRAINING:1 AND MODEL: LR \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: logistic_regression_parameters\n",
      "All parameters: ['penalty', 'solver', 'max_iter', 'C'], [['l2', 'elasticnet'], [50, 100, 130, 150, 170, 200], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]], [0, 2, 3]\n",
      "Parameters:  {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\"\n",
      "            Selected features: ['W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "            \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: LR - {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 84 candidates, totalling 420 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[87 15]\n",
      " [ 8 75]]\n",
      "[[75  8 15 87]\n",
      " [87 15  8 75]]\n",
      "[[162  23]\n",
      " [ 23 162]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.56756756756758\n",
      " Precision = [0.83333333 0.91578947]\n",
      " Recall = [0.90361446 0.85294118]\n",
      " F1 score = [0.86705202 0.88324873]\n",
      " AUC score = 87.82778171509568\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.56756756756758\n",
      " Precision = [0.91578947 0.83333333]\n",
      " Recall = [0.85294118 0.90361446]\n",
      " F1 score = [0.88324873 0.86705202]\n",
      " AUC score = 87.82778171509568\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[87 15]\n",
      " [ 8 75]]\n",
      " Accuracy (acc): 87.568\n",
      " Precision (prc): 83.333\n",
      " Recall (rec): 90.361\n",
      " Sensitivity (sns): 90.361\n",
      " Specificity (spc): 85.294\n",
      " F1 Score (f1s): 86.705\n",
      " ROC AUC (AUC): 0.878\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: LogisticRegression(C=0.5, max_iter=50) \n",
      "        Best parameters of the model: {'C': 0.5, 'max_iter': 50, 'penalty': 'l2'} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
      "\n",
      "                                    model  \\\n",
      "0  LogisticRegression(C=0.5, max_iter=50)   \n",
      "\n",
      "                              model_parameters  model_scores  \\\n",
      "0  {'C': 0.5, 'max_iter': 50, 'penalty': 'l2'}         91.76   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[87, 15], [8, 75]]    87.568     83.333  90.361       90.361       85.294   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    86.705    0.878   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:4, TRAINING:1 AND MODEL: SVC \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: support_vector_classifier_parameters\n",
      "All parameters: ['probability', 'C', 'kernel', 'gamma', 'degree', 'class_weightdict'], [[True], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], ['linear', 'rbf', 'poly']], [0, 1, 2]\n",
      "Parameters:  {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "210 fits failed out of a total of 420.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.84558824        nan 0.84558824        nan 0.84558824        nan\n",
      " 0.84558824        nan 0.84558824        nan 0.84558824        nan\n",
      " 0.88161765        nan 0.88161765        nan 0.88161765        nan\n",
      " 0.88161765        nan 0.88161765        nan 0.88161765        nan\n",
      " 0.91764706        nan 0.91764706        nan 0.91764706        nan\n",
      " 0.91764706        nan 0.91764706        nan 0.91764706        nan\n",
      " 0.91764706        nan 0.91764706        nan 0.91764706        nan\n",
      " 0.91764706        nan 0.91764706        nan 0.91764706        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.85857078        nan 0.85857078        nan 0.85857078        nan\n",
      " 0.85857078        nan 0.85857078        nan 0.85857078        nan\n",
      " 0.86761646        nan 0.86761646        nan 0.86761646        nan\n",
      " 0.86761646        nan 0.86761646        nan 0.86761646        nan\n",
      " 0.89167797        nan 0.89167797        nan 0.89167797        nan\n",
      " 0.89167797        nan 0.89167797        nan 0.89167797        nan\n",
      " 0.90375396        nan 0.90375396        nan 0.90375396        nan\n",
      " 0.90375396        nan 0.90375396        nan 0.90375396        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.85626498        nan 0.85626498        nan 0.85626498        nan\n",
      " 0.85626498        nan 0.85626498        nan 0.85626498        nan\n",
      " 0.86420792        nan 0.86420792        nan 0.86420792        nan\n",
      " 0.86420792        nan 0.86420792        nan 0.86420792        nan\n",
      " 0.88901186        nan 0.88901186        nan 0.88901186        nan\n",
      " 0.88901186        nan 0.88901186        nan 0.88901186        nan\n",
      " 0.88901186        nan 0.88901186        nan 0.88901186        nan\n",
      " 0.88901186        nan 0.88901186        nan 0.88901186        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.84148472        nan 0.84148472        nan 0.84148472        nan\n",
      " 0.84148472        nan 0.84148472        nan 0.84148472        nan\n",
      " 0.8377791         nan 0.8377791         nan 0.8377791         nan\n",
      " 0.8377791         nan 0.8377791         nan 0.8377791         nan\n",
      " 0.85244716        nan 0.85244716        nan 0.85244716        nan\n",
      " 0.85244716        nan 0.85244716        nan 0.85244716        nan\n",
      " 0.86296361        nan 0.86296361        nan 0.86296361        nan\n",
      " 0.86296361        nan 0.86296361        nan 0.86296361        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: SVC - {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[86 16]\n",
      " [ 7 76]]\n",
      "[[76  7 16 86]\n",
      " [86 16  7 76]]\n",
      "[[162  23]\n",
      " [ 23 162]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.56756756756758\n",
      " Precision = [0.82608696 0.92473118]\n",
      " Recall = [0.91566265 0.84313725]\n",
      " F1 score = [0.86857143 0.88205128]\n",
      " AUC score = 87.93999527521854\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.56756756756758\n",
      " Precision = [0.92473118 0.82608696]\n",
      " Recall = [0.84313725 0.91566265]\n",
      " F1 score = [0.88205128 0.86857143]\n",
      " AUC score = 87.93999527521852\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[86 16]\n",
      " [ 7 76]]\n",
      " Accuracy (acc): 87.568\n",
      " Precision (prc): 82.609\n",
      " Recall (rec): 91.566\n",
      " Sensitivity (sns): 91.566\n",
      " Specificity (spc): 84.314\n",
      " F1 Score (f1s): 86.857\n",
      " ROC AUC (AUC): 0.879\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: SVC(C=0.5, kernel='linear', probability=True) \n",
      "        Best parameters of the model: {'C': 0.5, 'kernel': 'linear', 'probability': True} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
      "\n",
      "                                           model  \\\n",
      "0  SVC(C=0.5, kernel='linear', probability=True)   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'C': 0.5, 'kernel': 'linear', 'probability': ...         91.76   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[86, 16], [7, 76]]    87.568     82.609  91.566       91.566       84.314   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    86.857    0.879   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:4, TRAINING:1 AND MODEL: NB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: naive_bayes_parameters\n",
      "All parameters: ['var_smoothing'], [[1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]], [0]\n",
      "Parameters:  {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: NB - {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[71 31]\n",
      " [ 7 76]]\n",
      "[[76  7 31 71]\n",
      " [71 31  7 76]]\n",
      "[[147  38]\n",
      " [ 38 147]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 79.45945945945945\n",
      " Precision = [0.71028037 0.91025641]\n",
      " Recall = [0.91566265 0.69607843]\n",
      " F1 score = [0.8        0.78888889]\n",
      " AUC score = 80.58705409874793\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 79.45945945945945\n",
      " Precision = [0.91025641 0.71028037]\n",
      " Recall = [0.69607843 0.91566265]\n",
      " F1 score = [0.78888889 0.8       ]\n",
      " AUC score = 80.58705409874793\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[71 31]\n",
      " [ 7 76]]\n",
      " Accuracy (acc): 79.459\n",
      " Precision (prc): 71.028\n",
      " Recall (rec): 91.566\n",
      " Sensitivity (sns): 91.566\n",
      " Specificity (spc): 69.608\n",
      " F1 Score (f1s): 80.0\n",
      " ROC AUC (AUC): 0.806\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GaussianNB(var_smoothing=1.0) \n",
      "        Best parameters of the model: {'var_smoothing': 1.0} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
      "\n",
      "                           model        model_parameters  model_scores  \\\n",
      "0  GaussianNB(var_smoothing=1.0)  {'var_smoothing': 1.0}         91.76   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[71, 31], [7, 76]]    79.459     71.028  91.566       91.566       69.608   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0      80.0    0.806   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:4, TRAINING:1 AND MODEL: KNN \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: k_nearest_neighbors_parameters\n",
      "All parameters: ['n_neighbors', 'p', 'metric', 'n_splits'], [[2, 3, 5, 10, 15, 25, 35], ['manhattan', 'minkowski', 'euclidean']], [0, 2]\n",
      "Parameters:  {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: KNN - {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[85 17]\n",
      " [ 8 75]]\n",
      "[[75  8 17 85]\n",
      " [85 17  8 75]]\n",
      "[[160  25]\n",
      " [ 25 160]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 86.48648648648648\n",
      " Precision = [0.81521739 0.91397849]\n",
      " Recall = [0.90361446 0.83333333]\n",
      " F1 score = [0.85714286 0.87179487]\n",
      " AUC score = 86.84738955823295\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 86.48648648648648\n",
      " Precision = [0.91397849 0.81521739]\n",
      " Recall = [0.83333333 0.90361446]\n",
      " F1 score = [0.87179487 0.85714286]\n",
      " AUC score = 86.84738955823295\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[85 17]\n",
      " [ 8 75]]\n",
      " Accuracy (acc): 86.486\n",
      " Precision (prc): 81.522\n",
      " Recall (rec): 90.361\n",
      " Sensitivity (sns): 90.361\n",
      " Specificity (spc): 83.333\n",
      " F1 Score (f1s): 85.714\n",
      " ROC AUC (AUC): 0.868\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: KNeighborsClassifier(metric='manhattan', n_neighbors=15) \n",
      "        Best parameters of the model: {'metric': 'manhattan', 'n_neighbors': 15} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
      "\n",
      "                                               model  \\\n",
      "0  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
      "\n",
      "                             model_parameters  model_scores  \\\n",
      "0  {'metric': 'manhattan', 'n_neighbors': 15}         92.94   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[85, 17], [8, 75]]    86.486     81.522  90.361       90.361       83.333   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    85.714    0.868   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:4, TRAINING:1 AND MODEL: DT \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: decision_tree_parameters\n",
      "All parameters: ['max_depth', 'criterion', 'splitter', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: DT - {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[87 15]\n",
      " [ 4 79]]\n",
      "[[79  4 15 87]\n",
      " [87 15  4 79]]\n",
      "[[166  19]\n",
      " [ 19 166]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 89.72972972972974\n",
      " Precision = [0.84042553 0.95604396]\n",
      " Recall = [0.95180723 0.85294118]\n",
      " F1 score = [0.89265537 0.9015544 ]\n",
      " AUC score = 90.23742026931254\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 89.72972972972974\n",
      " Precision = [0.95604396 0.84042553]\n",
      " Recall = [0.85294118 0.95180723]\n",
      " F1 score = [0.9015544  0.89265537]\n",
      " AUC score = 90.23742026931254\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[87 15]\n",
      " [ 4 79]]\n",
      " Accuracy (acc): 89.73\n",
      " Precision (prc): 84.043\n",
      " Recall (rec): 95.181\n",
      " Sensitivity (sns): 95.181\n",
      " Specificity (spc): 85.294\n",
      " F1 Score (f1s): 89.266\n",
      " ROC AUC (AUC): 0.902\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: DecisionTreeClassifier(max_depth=2) \n",
      "        Best parameters of the model: {'criterion': 'gini', 'max_depth': 2} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
      "\n",
      "                                 model                       model_parameters  \\\n",
      "0  DecisionTreeClassifier(max_depth=2)  {'criterion': 'gini', 'max_depth': 2}   \n",
      "\n",
      "   model_scores     confusion_matrix  accuracy  precision  recall  \\\n",
      "0         91.76  [[87, 15], [4, 79]]     89.73     84.043  95.181   \n",
      "\n",
      "   sensitivity  specificity  f1_score  roc_auc  \n",
      "0       95.181       85.294    89.266    0.902   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:4, TRAINING:1 AND MODEL: RF \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: random_forest_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'criterion', 'splitter', 'max_features', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.91764706 0.74705882 0.80808824 0.82058824 0.80808824 0.79632353\n",
      " 0.80808824 0.78382353 0.74779412 0.90588235 0.77058824 0.78308824\n",
      " 0.79632353 0.82132353 0.79705882 0.76029412 0.80955882 0.77205882\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.95183175 0.87073722 0.99402985 1.         1.         1.\n",
      " 1.         1.         1.         0.93971054 0.87073722 0.97593849\n",
      " 1.         1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.87592444 0.76746773 0.80186803 0.80077075 0.81586022 0.79687195\n",
      " 0.79160135 0.78857044 0.77155425 0.85290799 0.78376503 0.76188729\n",
      " 0.79364807 0.8000828  0.79446459 0.76919339 0.79312288 0.77840426\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.89447679 0.90432202 0.99102233 1.         1.         1.\n",
      " 1.         1.         1.         0.88811243 0.89671258 0.98322164\n",
      " 1.         1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: RF - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[88 14]\n",
      " [ 6 77]]\n",
      "[[77  6 14 88]\n",
      " [88 14  6 77]]\n",
      "[[165  20]\n",
      " [ 20 165]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 89.1891891891892\n",
      " Precision = [0.84615385 0.93617021]\n",
      " Recall = [0.92771084 0.8627451 ]\n",
      " F1 score = [0.88505747 0.89795918]\n",
      " AUC score = 89.52279707063549\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 89.1891891891892\n",
      " Precision = [0.93617021 0.84615385]\n",
      " Recall = [0.8627451  0.92771084]\n",
      " F1 score = [0.89795918 0.88505747]\n",
      " AUC score = 89.52279707063549\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[88 14]\n",
      " [ 6 77]]\n",
      " Accuracy (acc): 89.189\n",
      " Precision (prc): 84.615\n",
      " Recall (rec): 92.771\n",
      " Sensitivity (sns): 92.771\n",
      " Specificity (spc): 86.275\n",
      " F1 Score (f1s): 88.506\n",
      " ROC AUC (AUC): 0.895\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: RandomForestClassifier(max_depth=2, n_estimators=10) \n",
      "        Best parameters of the model: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 10} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
      "\n",
      "                                               model  \\\n",
      "0  (DecisionTreeClassifier(max_depth=2, max_featu...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'criterion': 'gini', 'max_depth': 2, 'n_estim...         92.94   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[88, 14], [6, 77]]    89.189     84.615  92.771       92.771       86.275   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    88.506    0.895   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:4, TRAINING:1 AND MODEL: GB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: gradient_boosting_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'learning_rate', 'max_features', 'loss', 'min_samples_split', 'min_samples_leaf'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.5, 1.0]], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "405 fits failed out of a total of 1215.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 467, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.88014706 0.89411765 0.92941176 0.89411765 0.91764706 0.92941176\n",
      " 0.91764706 0.91764706 0.91764706 0.86985294 0.88235294 0.91764706\n",
      " 0.91764706 0.92941176 0.91764706 0.91764706 0.91764706 0.91764706\n",
      " 0.79558824 0.85735294 0.92941176 0.89264706 0.89338235 0.90514706\n",
      " 0.90514706 0.90514706 0.90514706 0.80661765 0.82058824 0.88088235\n",
      " 0.88088235 0.88014706 0.86911765 0.90514706 0.89264706 0.90514706\n",
      " 0.77279412 0.81985294 0.90514706 0.86911765 0.88088235 0.90514706\n",
      " 0.88088235 0.89264706 0.90514706 0.84485294 0.85588235 0.86838235\n",
      " 0.90514706 0.88161765 0.90514706 0.90514706 0.86911765 0.90514706\n",
      " 0.83308824 0.85882353 0.89264706 0.88088235 0.89264706 0.90514706\n",
      " 0.89264706 0.90514706 0.89264706 0.84485294 0.88088235 0.78235294\n",
      " 0.92867647 0.84558824 0.83235294 0.88088235 0.90514706 0.89264706\n",
      " 0.79779412 0.84411765 0.81985294 0.90514706 0.90514706 0.88088235\n",
      " 0.90514706 0.90514706 0.89264706 0.86764706 0.90588235 0.90514706\n",
      " 0.91691176 0.90588235 0.91764706 0.91764706 0.91764706 0.91764706\n",
      " 0.83382353 0.85588235 0.90588235 0.92941176 0.91764706 0.92941176\n",
      " 0.91764706 0.91764706 0.91764706 0.74779412 0.86691176 0.84411765\n",
      " 0.90514706 0.89191176 0.90514706 0.90514706 0.91691176 0.90514706\n",
      " 0.82132353 0.82058824 0.83161765 0.85735294 0.89338235 0.89264706\n",
      " 0.90514706 0.89264706 0.90514706 0.79558824 0.76029412 0.84338235\n",
      " 0.88161765 0.88088235 0.86911765 0.89264706 0.89338235 0.89264706\n",
      " 0.83235294 0.85588235 0.86911765 0.86838235 0.89264706 0.88088235\n",
      " 0.88161765 0.89264706 0.90514706 0.86985294 0.90441176 0.84558824\n",
      " 0.84411765 0.90514706 0.90514706 0.91691176 0.88088235 0.89338235\n",
      " 0.80955882 0.84411765 0.88088235 0.88088235 0.89338235 0.88088235\n",
      " 0.90514706 0.89264706 0.90514706 0.77058824 0.81838235 0.83235294\n",
      " 0.89264706 0.89338235 0.82132353 0.88088235 0.88088235 0.89264706\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.91587517 0.93080054 0.92170963 0.91872456 0.92180009 0.92478517\n",
      " 0.92478517 0.92781547 0.92180009 0.93378562 0.94581637 0.94577114\n",
      " 0.93387607 0.9428313  0.93984622 0.94287653 0.93681592 0.95486205\n",
      " 0.94893713 0.96395296 0.98190864 0.98792402 1.         0.99398462\n",
      " 0.9969697  0.99398462 0.9969697  0.93993668 0.9727725  0.99398462\n",
      " 0.99701493 0.99398462 1.         1.         1.         1.\n",
      " 0.95789236 0.97593849 0.99393939 0.9969697  0.9969697  0.9969697\n",
      " 0.9969697  1.         1.         0.96684758 0.97901402 0.98796924\n",
      " 0.9969697  1.         1.         1.         1.         1.\n",
      " 0.95196744 0.97602895 0.99099955 1.         1.         1.\n",
      " 1.         1.         1.         0.94893713 0.97304387 0.98493894\n",
      " 1.         0.99393939 1.         1.         1.         1.\n",
      " 0.96377205 0.96996834 0.98787879 0.9969697  1.         1.\n",
      " 1.         1.         1.         0.87693351 0.91876979 0.90388964\n",
      " 0.92483039 0.92175486 0.91881502 0.92781547 0.92478517 0.92478517\n",
      " 0.92772501 0.9278607  0.94577114 0.94287653 0.94884668 0.94590683\n",
      " 0.9458616  0.94581637 0.93989145 0.9066938  0.96395296 0.98796924\n",
      " 0.98493894 0.99701493 1.         1.         0.99701493 1.\n",
      " 0.96390773 0.98190864 0.98792402 1.         1.         0.9969697\n",
      " 1.         1.         1.         0.95802804 0.9850294  0.98190864\n",
      " 1.         0.9969697  1.         1.         1.         1.\n",
      " 0.9547716  0.99099955 0.97878788 1.         1.         0.99398462\n",
      " 1.         1.         1.         0.97281773 0.98489371 0.98190864\n",
      " 0.99701493 0.99701493 0.99701493 0.9969697  1.         1.\n",
      " 0.97281773 0.98796924 0.9850294  0.99701493 0.99701493 1.\n",
      " 1.         1.         1.         0.95472637 0.98195387 0.98208955\n",
      " 0.9969697  0.99701493 1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.85298337 0.86142164 0.90238429 0.88257246 0.88901186 0.90238429\n",
      " 0.89507246 0.89507246 0.89507246 0.85638165 0.86388402 0.89248227\n",
      " 0.88901186 0.89667001 0.89507246 0.89507246 0.89507246 0.89507246\n",
      " 0.80213963 0.85703405 0.89127318 0.87567852 0.87426595 0.88862085\n",
      " 0.88290657 0.88257246 0.88257246 0.80720321 0.81716183 0.86781434\n",
      " 0.86706682 0.85857477 0.86923913 0.88862085 0.87571763 0.88257246\n",
      " 0.77681157 0.81272642 0.88257246 0.85578032 0.86706682 0.88257246\n",
      " 0.87308833 0.88173913 0.88257246 0.82423854 0.83155037 0.86660444\n",
      " 0.87015124 0.86748227 0.88862085 0.88257246 0.86321763 0.88257246\n",
      " 0.81959011 0.84526607 0.87437865 0.86997442 0.87571763 0.88862085\n",
      " 0.88173913 0.88257246 0.88173913 0.82911228 0.84023527 0.80583062\n",
      " 0.89414894 0.8545682  0.84150104 0.86997442 0.88257246 0.87571763\n",
      " 0.77434361 0.83862085 0.83545452 0.88862085 0.87998227 0.86706682\n",
      " 0.88257246 0.88862085 0.87602484 0.85852814 0.89151515 0.88257246\n",
      " 0.89593268 0.88888889 0.89507246 0.89507246 0.89507246 0.89507246\n",
      " 0.82039959 0.83462098 0.88282828 0.89632369 0.88935818 0.89632369\n",
      " 0.88329757 0.88901186 0.88901186 0.77330809 0.86347372 0.84272782\n",
      " 0.87750974 0.87000334 0.87146135 0.88862085 0.88988429 0.88862085\n",
      " 0.82897324 0.83265283 0.83177369 0.84804782 0.87392166 0.86460651\n",
      " 0.88257246 0.87602484 0.88257246 0.81831944 0.79866377 0.84615471\n",
      " 0.86536657 0.87567852 0.86966924 0.87571763 0.87651186 0.88173913\n",
      " 0.83698887 0.8475     0.86923913 0.86663672 0.87312743 0.86965702\n",
      " 0.87612085 0.87571763 0.88862085 0.85198107 0.88157318 0.83894498\n",
      " 0.84988429 0.88862085 0.88862085 0.88417001 0.86136272 0.88256024\n",
      " 0.80588818 0.83842477 0.86395292 0.86281193 0.87475637 0.86747005\n",
      " 0.88862085 0.87571763 0.88257246 0.78748099 0.81963599 0.84123487\n",
      " 0.86826421 0.87651186 0.83730986 0.86395292 0.86965702 0.87571763\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.87210808 0.88114516 0.88334728 0.88312349 0.88102364 0.88017901\n",
      " 0.8838627  0.88423777 0.87980784 0.89825381 0.90246435 0.90107911\n",
      " 0.89107713 0.89830423 0.89925585 0.8958889  0.89147541 0.90080947\n",
      " 0.93500349 0.94157475 0.95407166 0.95015769 0.96961735 0.96383745\n",
      " 0.96821325 0.96527585 0.96811229 0.95422801 0.97721727 0.9822632\n",
      " 0.99254755 0.99102083 0.99701476 0.99849624 0.99701476 0.99849624\n",
      " 0.9620812  0.97320339 0.99546508 0.99409372 0.99847328 0.9969697\n",
      " 0.99847328 1.         1.         0.96409733 0.98035827 0.98945022\n",
      " 0.99548804 1.         1.         1.         1.         1.\n",
      " 0.95399847 0.98033616 0.98954    1.         0.99851852 1.\n",
      " 1.         1.         1.         0.95473426 0.97730483 0.99090806\n",
      " 0.99701476 0.99694656 1.         1.         1.         1.\n",
      " 0.95526764 0.97713955 0.99238816 0.99398446 1.         1.\n",
      " 1.         1.         1.         0.84281398 0.87947277 0.87106884\n",
      " 0.88138349 0.88221831 0.87818962 0.88426    0.88143731 0.88143731\n",
      " 0.89818181 0.8927658  0.89863191 0.89344354 0.8965255  0.89622378\n",
      " 0.89622378 0.89496554 0.8930523  0.9052019  0.9484732  0.95959261\n",
      " 0.95959321 0.96398588 0.9682789  0.96410908 0.96272582 0.96690261\n",
      " 0.9609134  0.97902909 0.98648536 0.99256982 0.995511   0.99847328\n",
      " 0.99701476 0.99851852 1.         0.95905926 0.98354525 0.98789795\n",
      " 0.99400724 0.99847328 0.99849624 1.         1.         1.\n",
      " 0.95610326 0.98243119 0.98771351 0.99849624 1.         0.99696952\n",
      " 1.         1.         1.         0.97141632 0.97635117 0.98343345\n",
      " 0.99701476 0.99701493 0.99699248 0.99847328 1.         1.\n",
      " 0.96720855 0.98498232 0.98799185 0.99261389 0.99548872 1.\n",
      " 1.         0.99851852 1.         0.95928077 0.9790353  0.98935924\n",
      " 0.99548822 0.99551117 0.99851852 1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: GB - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 405 candidates, totalling 2025 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[89 13]\n",
      " [ 4 79]]\n",
      "[[79  4 13 89]\n",
      " [89 13  4 79]]\n",
      "[[168  17]\n",
      " [ 17 168]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 90.81081081081082\n",
      " Precision = [0.85869565 0.95698925]\n",
      " Recall = [0.95180723 0.87254902]\n",
      " F1 score = [0.90285714 0.91282051]\n",
      " AUC score = 91.21781242617529\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 90.81081081081082\n",
      " Precision = [0.95698925 0.85869565]\n",
      " Recall = [0.87254902 0.95180723]\n",
      " F1 score = [0.91282051 0.90285714]\n",
      " AUC score = 91.21781242617529\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[89 13]\n",
      " [ 4 79]]\n",
      " Accuracy (acc): 90.811\n",
      " Precision (prc): 85.87\n",
      " Recall (rec): 95.181\n",
      " Sensitivity (sns): 95.181\n",
      " Specificity (spc): 87.255\n",
      " F1 Score (f1s): 90.286\n",
      " ROC AUC (AUC): 0.912\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=75) \n",
      "        Best parameters of the model: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 75} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
      "\n",
      "                                               model  \\\n",
      "0  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'learning_rate': 0.01, 'max_depth': 2, 'n_est...         94.12   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[89, 13], [4, 79]]    90.811      85.87  95.181       95.181       87.255   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    90.286    0.912   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:4, TRAINING:1 AND MODEL: XGB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: xtreme_gradient_boosting_parameters\n",
      "All parameters: ['max_depth', 'eta', 'max_leaves'], [[2, 3, 6, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: XGB - {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[87 15]\n",
      " [ 6 77]]\n",
      "[[77  6 15 87]\n",
      " [87 15  6 77]]\n",
      "[[164  21]\n",
      " [ 21 164]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 88.64864864864866\n",
      " Precision = [0.83695652 0.93548387]\n",
      " Recall = [0.92771084 0.85294118]\n",
      " F1 score = [0.88       0.89230769]\n",
      " AUC score = 89.03260099220411\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 88.64864864864866\n",
      " Precision = [0.93548387 0.83695652]\n",
      " Recall = [0.85294118 0.92771084]\n",
      " F1 score = [0.89230769 0.88      ]\n",
      " AUC score = 89.03260099220411\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[87 15]\n",
      " [ 6 77]]\n",
      " Accuracy (acc): 88.649\n",
      " Precision (prc): 83.696\n",
      " Recall (rec): 92.771\n",
      " Sensitivity (sns): 92.771\n",
      " Specificity (spc): 85.294\n",
      " F1 Score (f1s): 88.0\n",
      " ROC AUC (AUC): 0.89\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False, eta=0.01,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.00999999978, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=2, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, ...) \n",
      "        Best parameters of the model: {'eta': 0.01, 'max_depth': 2} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
      "\n",
      "                                               model  \\\n",
      "0  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
      "\n",
      "                model_parameters  model_scores     confusion_matrix  accuracy  \\\n",
      "0  {'eta': 0.01, 'max_depth': 2}         91.76  [[87, 15], [6, 77]]    88.649   \n",
      "\n",
      "   precision  recall  sensitivity  specificity  f1_score  roc_auc  \n",
      "0     83.696  92.771       92.771       85.294      88.0     0.89   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            Best model index calculation  \n",
      "            \n",
      "WWWWWWWWWWWWWWWWWWW recall    Training_No  Model_No Model_Name  \\\n",
      "0            1         1         LR   \n",
      "1            1         2        SVC   \n",
      "2            1         3         NB   \n",
      "3            1         4        KNN   \n",
      "4            1         5         DT   \n",
      "5            1         6         RF   \n",
      "6            1         7         GB   \n",
      "7            1         8        XGB   \n",
      "\n",
      "                                              method  \\\n",
      "0  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
      "1  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
      "2  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
      "3  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
      "4  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
      "5  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
      "6  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
      "7  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
      "\n",
      "                                               model  \\\n",
      "0             LogisticRegression(C=0.5, max_iter=50)   \n",
      "1      SVC(C=0.5, kernel='linear', probability=True)   \n",
      "2                      GaussianNB(var_smoothing=1.0)   \n",
      "3  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
      "4                DecisionTreeClassifier(max_depth=2)   \n",
      "5  (DecisionTreeClassifier(max_depth=2, max_featu...   \n",
      "6  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
      "7  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0        {'C': 0.5, 'max_iter': 50, 'penalty': 'l2'}         91.76   \n",
      "1  {'C': 0.5, 'kernel': 'linear', 'probability': ...         91.76   \n",
      "2                             {'var_smoothing': 1.0}         91.76   \n",
      "3         {'metric': 'manhattan', 'n_neighbors': 15}         92.94   \n",
      "4              {'criterion': 'gini', 'max_depth': 2}         91.76   \n",
      "5  {'criterion': 'gini', 'max_depth': 2, 'n_estim...         92.94   \n",
      "6  {'learning_rate': 0.01, 'max_depth': 2, 'n_est...         94.12   \n",
      "7                      {'eta': 0.01, 'max_depth': 2}         91.76   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[87, 15], [8, 75]]    87.568     83.333  90.361       90.361       85.294   \n",
      "1  [[86, 16], [7, 76]]    87.568     82.609  91.566       91.566       84.314   \n",
      "2  [[71, 31], [7, 76]]    79.459     71.028  91.566       91.566       69.608   \n",
      "3  [[85, 17], [8, 75]]    86.486     81.522  90.361       90.361       83.333   \n",
      "4  [[87, 15], [4, 79]]    89.730     84.043  95.181       95.181       85.294   \n",
      "5  [[88, 14], [6, 77]]    89.189     84.615  92.771       92.771       86.275   \n",
      "6  [[89, 13], [4, 79]]    90.811     85.870  95.181       95.181       87.255   \n",
      "7  [[87, 15], [6, 77]]    88.649     83.696  92.771       92.771       85.294   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    86.705    0.878  \n",
      "1    86.857    0.879  \n",
      "2    80.000    0.806  \n",
      "3    85.714    0.868  \n",
      "4    89.266    0.902  \n",
      "5    88.506    0.895  \n",
      "6    90.286    0.912  \n",
      "7    88.000    0.890  \n",
      "\n",
      "            ***************************************************************************************************\n",
      "            TRAINING 1 END... \n",
      "            \n",
      "train- tri, tmp_train_ids, tmp_val_ids, tr_dat, val_dat 1 [ 59 100 136 174  69  10  13  49  75  26 147 113  92 178  54   1  74  85\n",
      " 128 171  99 119  62 142  81 166   6  44 145 104 135 156 169  95 133 124\n",
      "  70  60 137 105 162  33  72 122   9 180  68 102 123 127 149  51 112 103\n",
      "  34  39 150  64  86 143   3 157  40  97  25  16 158  11  96  46  45  80\n",
      "  15 167 134 140 126 170  58 146 115 154 184  37  21 125  82 129  77  20\n",
      "  12  18  29  22 175  94  84  57  61 106 159 121  91  48 131  78  19  14\n",
      " 179  41 101  90  27 120 132 139 177  23  89  65 168 161  83 111 163 176\n",
      " 155  55 173 160  79  73  71  38   4 116  43 110 165 181  47 141  76  88\n",
      " 144  56  36   5 108  93 148   8 164 183 130  28 114 152  52  17  63  32\n",
      "  31 117 172  35  50  24  87  53 153   0 138  98 107   7  67  66 118   2\n",
      " 182  30 151  42 109] [ 59 100 136 174  69  10  13  49  75  26 147 113  92 178  54   1  74  85\n",
      " 128 171  99 119  62 142  81 166   6  44 145 104 135 156 169  95 133 124\n",
      "  70  60 137 105 162  33  72 122   9 180  68 102 123 127 149  51 112 103\n",
      "  34  39 150  64  86 143   3 157  40  97  25  16 158  11  96  46  45  80\n",
      "  15 167 134 140 126 170  58 146 115 154 184  37  21 125  82 129  77  20\n",
      "  12  18  29  22 175  94  84  57  61 106 159 121  91  48 131  78  19  14\n",
      " 179  41 101  90  27 120 132 139 177  23  89  65 168 161  83 111 163 176\n",
      " 155  55 173 160  79  73  71  38   4 116  43 110 165 181  47 141  76  88\n",
      " 144  56  36   5 108  93 148   8 164 183 130  28 114 152  52  17  63  32\n",
      "  31 117 172  35  50  24  87  53 153   0 138  98 107   7  67  66 118   2\n",
      " 182  30 151  42 109] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "            *** MODEL TRAINING AND VALIDATION PHASE FOR TEST 4 \n",
      "            TRAINING 2 START... XXXXX \n",
      "            ***************************************************************************************************\n",
      "            Validation=> 185 ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            Training=> 185 ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] \n",
      "            \n",
      "\n",
      "            *** ML MODEL FOR TEST:4, TRAINING:2 AND MODEL: LR \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: logistic_regression_parameters\n",
      "All parameters: ['penalty', 'solver', 'max_iter', 'C'], [['l2', 'elasticnet'], [50, 100, 130, 150, 170, 200], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]], [0, 2, 3]\n",
      "Parameters:  {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: LR - {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 84 candidates, totalling 420 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[87 15]\n",
      " [ 8 75]]\n",
      "[[75  8 15 87]\n",
      " [87 15  8 75]]\n",
      "[[162  23]\n",
      " [ 23 162]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.56756756756758\n",
      " Precision = [0.83333333 0.91578947]\n",
      " Recall = [0.90361446 0.85294118]\n",
      " F1 score = [0.86705202 0.88324873]\n",
      " AUC score = 87.82778171509568\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.56756756756758\n",
      " Precision = [0.91578947 0.83333333]\n",
      " Recall = [0.85294118 0.90361446]\n",
      " F1 score = [0.88324873 0.86705202]\n",
      " AUC score = 87.82778171509568\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[87 15]\n",
      " [ 8 75]]\n",
      " Accuracy (acc): 87.568\n",
      " Precision (prc): 83.333\n",
      " Recall (rec): 90.361\n",
      " Sensitivity (sns): 90.361\n",
      " Specificity (spc): 85.294\n",
      " F1 Score (f1s): 86.705\n",
      " ROC AUC (AUC): 0.878\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: LogisticRegression(C=0.5, max_iter=50) \n",
      "        Best parameters of the model: {'C': 0.5, 'max_iter': 50, 'penalty': 'l2'} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
      "\n",
      "                                    model  \\\n",
      "0  LogisticRegression(C=0.5, max_iter=50)   \n",
      "\n",
      "                              model_parameters  model_scores  \\\n",
      "0  {'C': 0.5, 'max_iter': 50, 'penalty': 'l2'}         91.76   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[87, 15], [8, 75]]    87.568     83.333  90.361       90.361       85.294   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    86.705    0.878   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:4, TRAINING:2 AND MODEL: SVC \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: support_vector_classifier_parameters\n",
      "All parameters: ['probability', 'C', 'kernel', 'gamma', 'degree', 'class_weightdict'], [[True], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], ['linear', 'rbf', 'poly']], [0, 1, 2]\n",
      "Parameters:  {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "210 fits failed out of a total of 420.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.84558824        nan 0.84558824        nan 0.84558824        nan\n",
      " 0.84558824        nan 0.84558824        nan 0.84558824        nan\n",
      " 0.88161765        nan 0.88161765        nan 0.88161765        nan\n",
      " 0.88161765        nan 0.88161765        nan 0.88161765        nan\n",
      " 0.91764706        nan 0.91764706        nan 0.91764706        nan\n",
      " 0.91764706        nan 0.91764706        nan 0.91764706        nan\n",
      " 0.91764706        nan 0.91764706        nan 0.91764706        nan\n",
      " 0.91764706        nan 0.91764706        nan 0.91764706        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.85857078        nan 0.85857078        nan 0.85857078        nan\n",
      " 0.85857078        nan 0.85857078        nan 0.85857078        nan\n",
      " 0.86761646        nan 0.86761646        nan 0.86761646        nan\n",
      " 0.86761646        nan 0.86761646        nan 0.86761646        nan\n",
      " 0.89167797        nan 0.89167797        nan 0.89167797        nan\n",
      " 0.89167797        nan 0.89167797        nan 0.89167797        nan\n",
      " 0.90375396        nan 0.90375396        nan 0.90375396        nan\n",
      " 0.90375396        nan 0.90375396        nan 0.90375396        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.85626498        nan 0.85626498        nan 0.85626498        nan\n",
      " 0.85626498        nan 0.85626498        nan 0.85626498        nan\n",
      " 0.86420792        nan 0.86420792        nan 0.86420792        nan\n",
      " 0.86420792        nan 0.86420792        nan 0.86420792        nan\n",
      " 0.88901186        nan 0.88901186        nan 0.88901186        nan\n",
      " 0.88901186        nan 0.88901186        nan 0.88901186        nan\n",
      " 0.88901186        nan 0.88901186        nan 0.88901186        nan\n",
      " 0.88901186        nan 0.88901186        nan 0.88901186        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.84148472        nan 0.84148472        nan 0.84148472        nan\n",
      " 0.84148472        nan 0.84148472        nan 0.84148472        nan\n",
      " 0.8377791         nan 0.8377791         nan 0.8377791         nan\n",
      " 0.8377791         nan 0.8377791         nan 0.8377791         nan\n",
      " 0.85244716        nan 0.85244716        nan 0.85244716        nan\n",
      " 0.85244716        nan 0.85244716        nan 0.85244716        nan\n",
      " 0.86296361        nan 0.86296361        nan 0.86296361        nan\n",
      " 0.86296361        nan 0.86296361        nan 0.86296361        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: SVC - {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[86 16]\n",
      " [ 7 76]]\n",
      "[[76  7 16 86]\n",
      " [86 16  7 76]]\n",
      "[[162  23]\n",
      " [ 23 162]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.56756756756758\n",
      " Precision = [0.82608696 0.92473118]\n",
      " Recall = [0.91566265 0.84313725]\n",
      " F1 score = [0.86857143 0.88205128]\n",
      " AUC score = 87.93999527521854\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.56756756756758\n",
      " Precision = [0.92473118 0.82608696]\n",
      " Recall = [0.84313725 0.91566265]\n",
      " F1 score = [0.88205128 0.86857143]\n",
      " AUC score = 87.93999527521852\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[86 16]\n",
      " [ 7 76]]\n",
      " Accuracy (acc): 87.568\n",
      " Precision (prc): 82.609\n",
      " Recall (rec): 91.566\n",
      " Sensitivity (sns): 91.566\n",
      " Specificity (spc): 84.314\n",
      " F1 Score (f1s): 86.857\n",
      " ROC AUC (AUC): 0.879\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: SVC(C=0.5, kernel='linear', probability=True) \n",
      "        Best parameters of the model: {'C': 0.5, 'kernel': 'linear', 'probability': True} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
      "\n",
      "                                           model  \\\n",
      "0  SVC(C=0.5, kernel='linear', probability=True)   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'C': 0.5, 'kernel': 'linear', 'probability': ...         91.76   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[86, 16], [7, 76]]    87.568     82.609  91.566       91.566       84.314   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    86.857    0.879   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:4, TRAINING:2 AND MODEL: NB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: naive_bayes_parameters\n",
      "All parameters: ['var_smoothing'], [[1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]], [0]\n",
      "Parameters:  {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: NB - {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[71 31]\n",
      " [ 7 76]]\n",
      "[[76  7 31 71]\n",
      " [71 31  7 76]]\n",
      "[[147  38]\n",
      " [ 38 147]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 79.45945945945945\n",
      " Precision = [0.71028037 0.91025641]\n",
      " Recall = [0.91566265 0.69607843]\n",
      " F1 score = [0.8        0.78888889]\n",
      " AUC score = 80.58705409874793\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 79.45945945945945\n",
      " Precision = [0.91025641 0.71028037]\n",
      " Recall = [0.69607843 0.91566265]\n",
      " F1 score = [0.78888889 0.8       ]\n",
      " AUC score = 80.58705409874793\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[71 31]\n",
      " [ 7 76]]\n",
      " Accuracy (acc): 79.459\n",
      " Precision (prc): 71.028\n",
      " Recall (rec): 91.566\n",
      " Sensitivity (sns): 91.566\n",
      " Specificity (spc): 69.608\n",
      " F1 Score (f1s): 80.0\n",
      " ROC AUC (AUC): 0.806\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GaussianNB(var_smoothing=1.0) \n",
      "        Best parameters of the model: {'var_smoothing': 1.0} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
      "\n",
      "                           model        model_parameters  model_scores  \\\n",
      "0  GaussianNB(var_smoothing=1.0)  {'var_smoothing': 1.0}         91.76   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[71, 31], [7, 76]]    79.459     71.028  91.566       91.566       69.608   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0      80.0    0.806   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:4, TRAINING:2 AND MODEL: KNN \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: k_nearest_neighbors_parameters\n",
      "All parameters: ['n_neighbors', 'p', 'metric', 'n_splits'], [[2, 3, 5, 10, 15, 25, 35], ['manhattan', 'minkowski', 'euclidean']], [0, 2]\n",
      "Parameters:  {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: KNN - {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[85 17]\n",
      " [ 8 75]]\n",
      "[[75  8 17 85]\n",
      " [85 17  8 75]]\n",
      "[[160  25]\n",
      " [ 25 160]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 86.48648648648648\n",
      " Precision = [0.81521739 0.91397849]\n",
      " Recall = [0.90361446 0.83333333]\n",
      " F1 score = [0.85714286 0.87179487]\n",
      " AUC score = 86.84738955823295\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 86.48648648648648\n",
      " Precision = [0.91397849 0.81521739]\n",
      " Recall = [0.83333333 0.90361446]\n",
      " F1 score = [0.87179487 0.85714286]\n",
      " AUC score = 86.84738955823295\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[85 17]\n",
      " [ 8 75]]\n",
      " Accuracy (acc): 86.486\n",
      " Precision (prc): 81.522\n",
      " Recall (rec): 90.361\n",
      " Sensitivity (sns): 90.361\n",
      " Specificity (spc): 83.333\n",
      " F1 Score (f1s): 85.714\n",
      " ROC AUC (AUC): 0.868\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: KNeighborsClassifier(metric='manhattan', n_neighbors=15) \n",
      "        Best parameters of the model: {'metric': 'manhattan', 'n_neighbors': 15} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
      "\n",
      "                                               model  \\\n",
      "0  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
      "\n",
      "                             model_parameters  model_scores  \\\n",
      "0  {'metric': 'manhattan', 'n_neighbors': 15}         92.94   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[85, 17], [8, 75]]    86.486     81.522  90.361       90.361       83.333   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    85.714    0.868   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:4, TRAINING:2 AND MODEL: DT \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: decision_tree_parameters\n",
      "All parameters: ['max_depth', 'criterion', 'splitter', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: DT - {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[87 15]\n",
      " [ 4 79]]\n",
      "[[79  4 15 87]\n",
      " [87 15  4 79]]\n",
      "[[166  19]\n",
      " [ 19 166]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 89.72972972972974\n",
      " Precision = [0.84042553 0.95604396]\n",
      " Recall = [0.95180723 0.85294118]\n",
      " F1 score = [0.89265537 0.9015544 ]\n",
      " AUC score = 90.23742026931254\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 89.72972972972974\n",
      " Precision = [0.95604396 0.84042553]\n",
      " Recall = [0.85294118 0.95180723]\n",
      " F1 score = [0.9015544  0.89265537]\n",
      " AUC score = 90.23742026931254\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[87 15]\n",
      " [ 4 79]]\n",
      " Accuracy (acc): 89.73\n",
      " Precision (prc): 84.043\n",
      " Recall (rec): 95.181\n",
      " Sensitivity (sns): 95.181\n",
      " Specificity (spc): 85.294\n",
      " F1 Score (f1s): 89.266\n",
      " ROC AUC (AUC): 0.902\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: DecisionTreeClassifier(max_depth=2) \n",
      "        Best parameters of the model: {'criterion': 'gini', 'max_depth': 2} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
      "\n",
      "                                 model                       model_parameters  \\\n",
      "0  DecisionTreeClassifier(max_depth=2)  {'criterion': 'gini', 'max_depth': 2}   \n",
      "\n",
      "   model_scores     confusion_matrix  accuracy  precision  recall  \\\n",
      "0         91.76  [[87, 15], [4, 79]]     89.73     84.043  95.181   \n",
      "\n",
      "   sensitivity  specificity  f1_score  roc_auc  \n",
      "0       95.181       85.294    89.266    0.902   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:4, TRAINING:2 AND MODEL: RF \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: random_forest_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'criterion', 'splitter', 'max_features', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.91764706 0.73529412 0.79558824 0.82058824 0.79558824 0.82132353\n",
      " 0.76029412 0.77352941 0.75955882 0.90588235 0.77058824 0.77205882\n",
      " 0.76102941 0.77279412 0.81985294 0.76102941 0.79705882 0.78455882\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.95183175 0.87073722 0.99402985 1.         1.         1.\n",
      " 1.         1.         1.         0.93971054 0.87073722 0.97593849\n",
      " 1.         1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.87592444 0.77536327 0.79582851 0.8119086  0.79246925 0.80966587\n",
      " 0.76156863 0.78670297 0.757786   0.85290799 0.78376503 0.76663592\n",
      " 0.74440047 0.75825855 0.78238476 0.75918021 0.78364807 0.77233064\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.89447679 0.90432202 0.99102233 1.         1.         1.\n",
      " 1.         1.         1.         0.88811243 0.89671258 0.98322164\n",
      " 1.         1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: RF - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[87 15]\n",
      " [ 7 76]]\n",
      "[[76  7 15 87]\n",
      " [87 15  7 76]]\n",
      "[[163  22]\n",
      " [ 22 163]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 88.10810810810811\n",
      " Precision = [0.83516484 0.92553191]\n",
      " Recall = [0.91566265 0.85294118]\n",
      " F1 score = [0.87356322 0.8877551 ]\n",
      " AUC score = 88.4301913536499\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 88.10810810810811\n",
      " Precision = [0.92553191 0.83516484]\n",
      " Recall = [0.85294118 0.91566265]\n",
      " F1 score = [0.8877551  0.87356322]\n",
      " AUC score = 88.43019135364987\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[87 15]\n",
      " [ 7 76]]\n",
      " Accuracy (acc): 88.108\n",
      " Precision (prc): 83.516\n",
      " Recall (rec): 91.566\n",
      " Sensitivity (sns): 91.566\n",
      " Specificity (spc): 85.294\n",
      " F1 Score (f1s): 87.356\n",
      " ROC AUC (AUC): 0.884\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: RandomForestClassifier(max_depth=2, n_estimators=30) \n",
      "        Best parameters of the model: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 30} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
      "\n",
      "                                               model  \\\n",
      "0  (DecisionTreeClassifier(max_depth=2, max_featu...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'criterion': 'gini', 'max_depth': 2, 'n_estim...         92.94   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[87, 15], [7, 76]]    88.108     83.516  91.566       91.566       85.294   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    87.356    0.884   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:4, TRAINING:2 AND MODEL: GB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: gradient_boosting_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'learning_rate', 'max_features', 'loss', 'min_samples_split', 'min_samples_leaf'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.5, 1.0]], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "405 fits failed out of a total of 1215.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 467, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.89264706 0.89411765 0.91764706 0.91764706 0.91764706 0.92941176\n",
      " 0.91764706 0.91764706 0.91764706 0.91691176 0.90514706 0.91764706\n",
      " 0.91764706 0.91764706 0.91764706 0.91764706 0.91764706 0.91764706\n",
      " 0.86985294 0.79632353 0.90514706 0.86764706 0.91764706 0.90514706\n",
      " 0.90514706 0.90514706 0.90514706 0.80955882 0.88088235 0.85882353\n",
      " 0.88014706 0.89264706 0.90514706 0.89264706 0.89264706 0.89264706\n",
      " 0.81764706 0.84411765 0.84338235 0.88161765 0.89264706 0.89338235\n",
      " 0.89264706 0.90514706 0.90514706 0.79338235 0.81911765 0.86985294\n",
      " 0.88161765 0.89264706 0.89264706 0.89264706 0.90514706 0.90514706\n",
      " 0.83308824 0.83382353 0.85514706 0.86764706 0.86911765 0.89338235\n",
      " 0.89338235 0.90514706 0.89264706 0.85735294 0.82132353 0.84485294\n",
      " 0.90514706 0.89264706 0.89264706 0.89264706 0.89264706 0.90514706\n",
      " 0.86691176 0.82058824 0.84485294 0.88088235 0.89264706 0.88014706\n",
      " 0.88088235 0.89264706 0.90514706 0.86911765 0.85808824 0.90588235\n",
      " 0.89338235 0.92941176 0.91764706 0.92941176 0.91764706 0.91764706\n",
      " 0.87058824 0.88161765 0.91764706 0.91764706 0.91764706 0.91764706\n",
      " 0.91764706 0.91764706 0.91764706 0.91691176 0.80808824 0.89264706\n",
      " 0.88161765 0.90514706 0.90514706 0.90514706 0.90514706 0.90514706\n",
      " 0.82058824 0.83455882 0.86838235 0.88088235 0.90514706 0.88161765\n",
      " 0.88161765 0.91764706 0.89264706 0.82058824 0.84411765 0.83161765\n",
      " 0.88088235 0.86911765 0.90514706 0.90514706 0.89338235 0.89264706\n",
      " 0.82279412 0.80808824 0.79779412 0.85661765 0.88088235 0.89264706\n",
      " 0.89338235 0.90514706 0.89264706 0.83088235 0.83308824 0.84411765\n",
      " 0.89338235 0.86911765 0.90441176 0.89264706 0.89264706 0.90514706\n",
      " 0.80808824 0.86911765 0.82132353 0.88161765 0.86911765 0.88088235\n",
      " 0.89338235 0.89264706 0.89338235 0.85661765 0.88161765 0.88161765\n",
      " 0.86911765 0.89411765 0.88088235 0.88088235 0.89264706 0.88088235\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.92469471 0.91573948 0.91564903 0.91569426 0.92180009 0.92483039\n",
      " 0.92781547 0.92180009 0.92478517 0.9458616  0.93989145 0.94269561\n",
      " 0.9458616  0.95789236 0.9458616  0.9458616  0.94581637 0.93378562\n",
      " 0.95481682 0.93975577 0.98796924 0.9880597  0.99398462 0.99095432\n",
      " 0.99701493 1.         1.         0.95789236 0.98186341 0.99095432\n",
      " 0.9819991  1.         0.99701493 1.         1.         1.\n",
      " 0.97896879 0.96680235 0.97901402 1.         0.9969697  1.\n",
      " 1.         1.         1.         0.96693804 0.97598372 0.97896879\n",
      " 0.9969697  0.9969697  1.         1.         1.         1.\n",
      " 0.97598372 0.98484848 0.97892356 0.99398462 0.99701493 0.9969697\n",
      " 1.         1.         1.         0.9547716  0.98190864 0.98190864\n",
      " 0.99095432 1.         0.9969697  0.9969697  1.         1.\n",
      " 0.96381728 0.98792402 0.9969697  1.         0.9969697  0.99393939\n",
      " 1.         1.         1.         0.89479873 0.90067843 0.92175486\n",
      " 0.92777024 0.92478517 0.93075531 0.92781547 0.92478517 0.92478517\n",
      " 0.92166441 0.9278607  0.94287653 0.9458616  0.95486205 0.93984622\n",
      " 0.9458616  0.9458616  0.93989145 0.95183175 0.97887834 0.99104478\n",
      " 0.98484848 0.99398462 0.99393939 0.99701493 0.99701493 1.\n",
      " 0.95472637 0.97584803 0.9850294  1.         1.         0.99701493\n",
      " 1.         1.         1.         0.97286296 0.99099955 0.99398462\n",
      " 0.9969697  0.99701493 0.99701493 0.99701493 1.         1.\n",
      " 0.96381728 0.9727725  0.99099955 0.99099955 0.9969697  1.\n",
      " 1.         1.         1.         0.97290819 0.9819991  0.98493894\n",
      " 0.99398462 1.         0.99393939 1.         0.9969697  1.\n",
      " 0.96996834 0.98489371 0.98489371 0.9969697  0.99398462 1.\n",
      " 1.         1.         1.         0.97589326 0.99095432 0.99095432\n",
      " 0.99398462 1.         0.99701493 1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.86965702 0.87077224 0.89507246 0.88901186 0.89507246 0.89667001\n",
      " 0.88329757 0.89507246 0.88901186 0.87287549 0.86637716 0.89368864\n",
      " 0.89507246 0.89507246 0.89507246 0.89507246 0.88901186 0.89507246\n",
      " 0.84451543 0.79309283 0.88862085 0.86650104 0.88935818 0.88862085\n",
      " 0.88862085 0.88257246 0.88862085 0.8172059  0.85458456 0.86058971\n",
      " 0.87563403 0.87000334 0.88862085 0.88173913 0.87602484 0.87602484\n",
      " 0.82702858 0.83320972 0.85239867 0.86869577 0.87571763 0.88256024\n",
      " 0.87571763 0.88257246 0.88862085 0.77873633 0.82355731 0.86062743\n",
      " 0.87353066 0.87012623 0.88173913 0.88173913 0.87685818 0.88257246\n",
      " 0.8334565  0.8287033  0.85930191 0.85538992 0.86321763 0.88256024\n",
      " 0.87651186 0.87998227 0.88173913 0.85676601 0.8423232  0.84733234\n",
      " 0.88862085 0.87343465 0.87914894 0.88173913 0.87571763 0.88862085\n",
      " 0.8621071  0.83383232 0.84894992 0.86965702 0.87428571 0.8658156\n",
      " 0.86395292 0.88173913 0.88257246 0.85541169 0.84928222 0.88282828\n",
      " 0.88243728 0.89632369 0.89507246 0.90238429 0.89507246 0.89507246\n",
      " 0.84267725 0.84756336 0.88642166 0.89507246 0.88642166 0.89507246\n",
      " 0.89507246 0.89507246 0.88901186 0.87877318 0.79414286 0.86460651\n",
      " 0.87007246 0.87146135 0.87426798 0.88257246 0.88257246 0.88257246\n",
      " 0.82986312 0.81698287 0.85336022 0.87308833 0.88862085 0.87043903\n",
      " 0.87612085 0.88642166 0.87571763 0.82821131 0.8288878  0.84579125\n",
      " 0.86508117 0.86321763 0.88257246 0.88257246 0.87080776 0.88173913\n",
      " 0.80925901 0.82076237 0.81271102 0.84763378 0.86965702 0.87571763\n",
      " 0.88256024 0.88862085 0.88173913 0.82244396 0.83914894 0.85400104\n",
      " 0.87651186 0.86379085 0.88984519 0.87571763 0.87571763 0.88257246\n",
      " 0.8062276  0.85170874 0.84080476 0.87006024 0.86923913 0.86965702\n",
      " 0.87651186 0.87914894 0.87080776 0.84037437 0.85490651 0.86401186\n",
      " 0.85820972 0.87143939 0.86793926 0.86965702 0.87312743 0.86965702\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.87020266 0.88151089 0.87776408 0.87634976 0.87860622 0.88384332\n",
      " 0.88183973 0.88223029 0.88259337 0.89227018 0.89662732 0.89582647\n",
      " 0.90103491 0.90369634 0.90115654 0.89740085 0.89865188 0.89111231\n",
      " 0.9421915  0.93449936 0.95538893 0.96644111 0.96808866 0.959656\n",
      " 0.96547883 0.96690095 0.9697952  0.94938685 0.97189583 0.98807834\n",
      " 0.98349203 0.99407358 0.99551117 0.99701476 1.         1.\n",
      " 0.97748798 0.97125041 0.98482432 1.         0.99847328 1.\n",
      " 1.         1.         1.         0.97266572 0.97732644 0.98484508\n",
      " 0.9969918  0.99696952 0.99849624 1.         1.         1.\n",
      " 0.96336257 0.97604257 0.989266   0.99546576 0.99849624 0.99847328\n",
      " 1.         1.         1.         0.96203555 0.98341002 0.98938081\n",
      " 0.9879898  1.         0.99847328 0.99847328 1.         1.\n",
      " 0.95964641 0.98648555 0.9969918  0.99553328 0.9969918  0.99692308\n",
      " 1.         1.         1.         0.86630352 0.86716685 0.88096399\n",
      " 0.88541273 0.8838627  0.88698088 0.88424574 0.88143731 0.88263893\n",
      " 0.88946613 0.88791719 0.89821861 0.89868127 0.90082515 0.89304965\n",
      " 0.89492016 0.89622131 0.89180265 0.93268589 0.95611956 0.95689395\n",
      " 0.96071981 0.9585054  0.9542763  0.95588029 0.95997285 0.97111335\n",
      " 0.95640091 0.96872828 0.98213195 0.99405196 0.98960719 0.99256999\n",
      " 1.         0.99851852 1.         0.96714843 0.97507095 0.99398496\n",
      " 0.9969697  0.99849624 0.99849624 0.99849624 1.         1.\n",
      " 0.96235607 0.97866474 0.9939615  0.99396218 0.99847328 0.99851852\n",
      " 1.         1.         1.         0.95888844 0.97757575 0.98794478\n",
      " 0.99400673 0.99849624 0.99694656 1.         0.99847328 1.\n",
      " 0.9599287  0.97761398 0.98933436 0.99398445 0.99696952 1.\n",
      " 0.99849624 1.         1.         0.96145515 0.97792433 0.99245652\n",
      " 0.99402835 1.         0.99849624 1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: GB - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 405 candidates, totalling 2025 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[89 13]\n",
      " [ 4 79]]\n",
      "[[79  4 13 89]\n",
      " [89 13  4 79]]\n",
      "[[168  17]\n",
      " [ 17 168]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 90.81081081081082\n",
      " Precision = [0.85869565 0.95698925]\n",
      " Recall = [0.95180723 0.87254902]\n",
      " F1 score = [0.90285714 0.91282051]\n",
      " AUC score = 91.21781242617529\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 90.81081081081082\n",
      " Precision = [0.95698925 0.85869565]\n",
      " Recall = [0.87254902 0.95180723]\n",
      " F1 score = [0.91282051 0.90285714]\n",
      " AUC score = 91.21781242617529\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[89 13]\n",
      " [ 4 79]]\n",
      " Accuracy (acc): 90.811\n",
      " Precision (prc): 85.87\n",
      " Recall (rec): 95.181\n",
      " Sensitivity (sns): 95.181\n",
      " Specificity (spc): 87.255\n",
      " F1 Score (f1s): 90.286\n",
      " ROC AUC (AUC): 0.912\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=75) \n",
      "        Best parameters of the model: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 75} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
      "\n",
      "                                               model  \\\n",
      "0  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'learning_rate': 0.01, 'max_depth': 2, 'n_est...         94.12   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[89, 13], [4, 79]]    90.811      85.87  95.181       95.181       87.255   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    90.286    0.912   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:4, TRAINING:2 AND MODEL: XGB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: xtreme_gradient_boosting_parameters\n",
      "All parameters: ['max_depth', 'eta', 'max_leaves'], [[2, 3, 6, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco1', 'narco2', 'narco3', 'narco5', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm9', 'plm10', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241']\n",
      "\n",
      "        From training? True, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: XGB - {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "\n",
      "        From training? False, Data shape: (185, 18), Indices: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (185, 14), Target shape: (185,), Metadata: (185, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[87 15]\n",
      " [ 6 77]]\n",
      "[[77  6 15 87]\n",
      " [87 15  6 77]]\n",
      "[[164  21]\n",
      " [ 21 164]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 88.64864864864866\n",
      " Precision = [0.83695652 0.93548387]\n",
      " Recall = [0.92771084 0.85294118]\n",
      " F1 score = [0.88       0.89230769]\n",
      " AUC score = 89.03260099220411\n",
      " Support = [ 83 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 88.64864864864866\n",
      " Precision = [0.93548387 0.83695652]\n",
      " Recall = [0.85294118 0.92771084]\n",
      " F1 score = [0.89230769 0.88      ]\n",
      " AUC score = 89.03260099220411\n",
      " Support = [102  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[87 15]\n",
      " [ 6 77]]\n",
      " Accuracy (acc): 88.649\n",
      " Precision (prc): 83.696\n",
      " Recall (rec): 92.771\n",
      " Sensitivity (sns): 92.771\n",
      " Specificity (spc): 85.294\n",
      " F1 Score (f1s): 88.0\n",
      " ROC AUC (AUC): 0.89\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False, eta=0.01,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.00999999978, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=2, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, ...) \n",
      "        Best parameters of the model: {'eta': 0.01, 'max_depth': 2} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
      "\n",
      "                                               model  \\\n",
      "0  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
      "\n",
      "                model_parameters  model_scores     confusion_matrix  accuracy  \\\n",
      "0  {'eta': 0.01, 'max_depth': 2}         91.76  [[87, 15], [6, 77]]    88.649   \n",
      "\n",
      "   precision  recall  sensitivity  specificity  f1_score  roc_auc  \n",
      "0     83.696  92.771       92.771       85.294      88.0     0.89   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            Best model index calculation  \n",
      "            \n",
      "\n",
      "            ***************************************************************************************************\n",
      "            TRAINING 2 END... \n",
      "            \n",
      "\n",
      "        ### MODEL EVALUATION PHASE \n",
      "        EVALUATION 4 START... XXXXX \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "        From training? False, Data shape: (45, 18), Indices: [5, 18, 19, 20, 21, 62, 29, 55, 56, 57, 58, 59, 60, 61, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 77, 78, 93, 94, 95, 96, 118, 119, 120, 121, 122]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (45, 14), Target shape: (45,), Metadata: (45, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "[[24  1]\n",
      " [ 4 16]]\n",
      "[[16  4  1 24]\n",
      " [24  1  4 16]]\n",
      "[[40  5]\n",
      " [ 5 40]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 88.88888888888889\n",
      " Precision = [0.94117647 0.85714286]\n",
      " Recall = [0.8  0.96]\n",
      " F1 score = [0.86486486 0.90566038]\n",
      " AUC score = 88.0\n",
      " Support = [20 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 88.88888888888889\n",
      " Precision = [0.85714286 0.94117647]\n",
      " Recall = [0.96 0.8 ]\n",
      " F1 score = [0.90566038 0.86486486]\n",
      " AUC score = 88.0\n",
      " Support = [25 20]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[24  1]\n",
      " [ 4 16]]\n",
      " Accuracy (acc): 88.889\n",
      " Precision (prc): 94.118\n",
      " Recall (rec): 80.0\n",
      " Sensitivity (sns): 80.0\n",
      " Specificity (spc): 96.0\n",
      " F1 Score (f1s): 86.486\n",
      " ROC AUC (AUC): 0.88\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1]\n",
      "[[23  2]\n",
      " [ 4 16]]\n",
      "[[16  4  2 23]\n",
      " [23  2  4 16]]\n",
      "[[39  6]\n",
      " [ 6 39]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 86.66666666666667\n",
      " Precision = [0.88888889 0.85185185]\n",
      " Recall = [0.8  0.92]\n",
      " F1 score = [0.84210526 0.88461538]\n",
      " AUC score = 86.0\n",
      " Support = [20 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 86.66666666666667\n",
      " Precision = [0.85185185 0.88888889]\n",
      " Recall = [0.92 0.8 ]\n",
      " F1 score = [0.88461538 0.84210526]\n",
      " AUC score = 86.00000000000001\n",
      " Support = [25 20]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[23  2]\n",
      " [ 4 16]]\n",
      " Accuracy (acc): 86.667\n",
      " Precision (prc): 88.889\n",
      " Recall (rec): 80.0\n",
      " Sensitivity (sns): 80.0\n",
      " Specificity (spc): 92.0\n",
      " F1 Score (f1s): 84.211\n",
      " ROC AUC (AUC): 0.86\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1]\n",
      "[[22  3]\n",
      " [ 4 16]]\n",
      "[[16  4  3 22]\n",
      " [22  3  4 16]]\n",
      "[[38  7]\n",
      " [ 7 38]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 84.44444444444444\n",
      " Precision = [0.84210526 0.84615385]\n",
      " Recall = [0.8  0.88]\n",
      " F1 score = [0.82051282 0.8627451 ]\n",
      " AUC score = 84.0\n",
      " Support = [20 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 84.44444444444444\n",
      " Precision = [0.84615385 0.84210526]\n",
      " Recall = [0.88 0.8 ]\n",
      " F1 score = [0.8627451  0.82051282]\n",
      " AUC score = 84.00000000000001\n",
      " Support = [25 20]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[22  3]\n",
      " [ 4 16]]\n",
      " Accuracy (acc): 84.444\n",
      " Precision (prc): 84.211\n",
      " Recall (rec): 80.0\n",
      " Sensitivity (sns): 80.0\n",
      " Specificity (spc): 88.0\n",
      " F1 Score (f1s): 82.051\n",
      " ROC AUC (AUC): 0.84\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1]\n",
      "[[23  2]\n",
      " [ 4 16]]\n",
      "[[16  4  2 23]\n",
      " [23  2  4 16]]\n",
      "[[39  6]\n",
      " [ 6 39]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 86.66666666666667\n",
      " Precision = [0.88888889 0.85185185]\n",
      " Recall = [0.8  0.92]\n",
      " F1 score = [0.84210526 0.88461538]\n",
      " AUC score = 86.0\n",
      " Support = [20 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 86.66666666666667\n",
      " Precision = [0.85185185 0.88888889]\n",
      " Recall = [0.92 0.8 ]\n",
      " F1 score = [0.88461538 0.84210526]\n",
      " AUC score = 86.00000000000001\n",
      " Support = [25 20]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[23  2]\n",
      " [ 4 16]]\n",
      " Accuracy (acc): 86.667\n",
      " Precision (prc): 88.889\n",
      " Recall (rec): 80.0\n",
      " Sensitivity (sns): 80.0\n",
      " Specificity (spc): 92.0\n",
      " F1 Score (f1s): 84.211\n",
      " ROC AUC (AUC): 0.86\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "[[22  3]\n",
      " [ 2 18]]\n",
      "[[18  2  3 22]\n",
      " [22  3  2 18]]\n",
      "[[40  5]\n",
      " [ 5 40]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 88.88888888888889\n",
      " Precision = [0.85714286 0.91666667]\n",
      " Recall = [0.9  0.88]\n",
      " F1 score = [0.87804878 0.89795918]\n",
      " AUC score = 89.0\n",
      " Support = [20 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 88.88888888888889\n",
      " Precision = [0.91666667 0.85714286]\n",
      " Recall = [0.88 0.9 ]\n",
      " F1 score = [0.89795918 0.87804878]\n",
      " AUC score = 89.0\n",
      " Support = [25 20]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[22  3]\n",
      " [ 2 18]]\n",
      " Accuracy (acc): 88.889\n",
      " Precision (prc): 85.714\n",
      " Recall (rec): 90.0\n",
      " Sensitivity (sns): 90.0\n",
      " Specificity (spc): 88.0\n",
      " F1 Score (f1s): 87.805\n",
      " ROC AUC (AUC): 0.89\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "[[22  3]\n",
      " [ 3 17]]\n",
      "[[17  3  3 22]\n",
      " [22  3  3 17]]\n",
      "[[39  6]\n",
      " [ 6 39]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 86.66666666666667\n",
      " Precision = [0.85 0.88]\n",
      " Recall = [0.85 0.88]\n",
      " F1 score = [0.85 0.88]\n",
      " AUC score = 86.5\n",
      " Support = [20 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 86.66666666666667\n",
      " Precision = [0.88 0.85]\n",
      " Recall = [0.88 0.85]\n",
      " F1 score = [0.88 0.85]\n",
      " AUC score = 86.50000000000001\n",
      " Support = [25 20]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[22  3]\n",
      " [ 3 17]]\n",
      " Accuracy (acc): 86.667\n",
      " Precision (prc): 85.0\n",
      " Recall (rec): 85.0\n",
      " Sensitivity (sns): 85.0\n",
      " Specificity (spc): 88.0\n",
      " F1 Score (f1s): 85.0\n",
      " ROC AUC (AUC): 0.865\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "[[22  3]\n",
      " [ 3 17]]\n",
      "[[17  3  3 22]\n",
      " [22  3  3 17]]\n",
      "[[39  6]\n",
      " [ 6 39]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 86.66666666666667\n",
      " Precision = [0.85 0.88]\n",
      " Recall = [0.85 0.88]\n",
      " F1 score = [0.85 0.88]\n",
      " AUC score = 86.5\n",
      " Support = [20 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 86.66666666666667\n",
      " Precision = [0.88 0.85]\n",
      " Recall = [0.88 0.85]\n",
      " F1 score = [0.88 0.85]\n",
      " AUC score = 86.50000000000001\n",
      " Support = [25 20]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[22  3]\n",
      " [ 3 17]]\n",
      " Accuracy (acc): 86.667\n",
      " Precision (prc): 85.0\n",
      " Recall (rec): 85.0\n",
      " Sensitivity (sns): 85.0\n",
      " Specificity (spc): 88.0\n",
      " F1 Score (f1s): 85.0\n",
      " ROC AUC (AUC): 0.865\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "[[22  3]\n",
      " [ 3 17]]\n",
      "[[17  3  3 22]\n",
      " [22  3  3 17]]\n",
      "[[39  6]\n",
      " [ 6 39]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 86.66666666666667\n",
      " Precision = [0.85 0.88]\n",
      " Recall = [0.85 0.88]\n",
      " F1 score = [0.85 0.88]\n",
      " AUC score = 86.5\n",
      " Support = [20 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 86.66666666666667\n",
      " Precision = [0.88 0.85]\n",
      " Recall = [0.88 0.85]\n",
      " F1 score = [0.88 0.85]\n",
      " AUC score = 86.50000000000001\n",
      " Support = [25 20]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[22  3]\n",
      " [ 3 17]]\n",
      " Accuracy (acc): 86.667\n",
      " Precision (prc): 85.0\n",
      " Recall (rec): 85.0\n",
      " Sensitivity (sns): 85.0\n",
      " Specificity (spc): 88.0\n",
      " F1 Score (f1s): 85.0\n",
      " ROC AUC (AUC): 0.865\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "            ===================================================================================================\n",
      "            TEST 4 END...\n",
      "            \n",
      "test- tsi, train_all_ids, test_ids, tr_all_dat, ts_dat 4 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [22, 23, 24, 25, 30, 63, 64, 65, 66, 67, 68, 69, 70, 79, 80, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 97, 98, 99, 100, 123, 124, 125, 126, 127] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco5', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm9', 'plm10', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109']\n",
      "\n",
      "            ### MODEL TEST PHASE \n",
      "            TEST 5 START... XXXXX \n",
      "            ===================================================================================================\n",
      "            Test=> 44 ['ins1108', 'ins1109', 'ins1110', 'ins1111', 'narco5', 'nfle33', 'nfle34', 'nfle35', 'nfle36', 'nfle37', 'nfle38', 'nfle39', 'nfle40', 'plm9', 'plm10', 'ST7041', 'ST7051', 'ST7061', 'ST7071', 'ST7081', 'ST7091', 'ST7101', 'ST7111', 'ST7121', 'ST7131', 'ST7141', 'ST7151', 'ST7161', 'ST7171', 'ST7181', 'ST7191', 'ST7201', 'ST7211', 'ST7221', 'ST7241', 'rbd17', 'rbd18', 'rbd19', 'rbd20', 'n1105', 'n1106', 'n1107', 'n1108', 'n1109'] \n",
      "            Training (Including Validation)=> 186 ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] \n",
      "            \n",
      "Random 5 percentage splitting testing...\n",
      "train- tri, tmp_train_ids, tmp_val_ids, tr_dat, val_dat 0 [151  73 120  68  33  95 128 139 144 124 138  20 106  40 173  28  12  84\n",
      "  11   2  13  75  61  77  23  15 142 171 184 130  57 140 182  26  83  72\n",
      " 160  54 115  25  59 148  62 119 180 107  36 183  60  76  91 150 109  94\n",
      " 153 117  99  58  29  96  30  67 162  21   7  87 108 179 146 166 110  92\n",
      " 111 102 185  22 169   6  51 155 143   9 125   5 132 137  90  86  49  93\n",
      " 161 145  48  27 172  66 123  65 170  80 101  44 176  35   3 105  43  88\n",
      "  37 121 113  81  47 112 165 131 156 127 175   8 163  24 152  19  70 136\n",
      "  38 114  74 149 126  17 118  89  79 157 103  18 168 177 178 158  64  55\n",
      "  78  16  53  10  85   4 135 122  14  50 141 181  45 154 133  52  34 174\n",
      "  98  31 129 100   1  71  42 164 159 134  82  41  97  46  56   0 147  39\n",
      "  69 104 116  63 167  32] [151  73 120  68  33  95 128 139 144 124 138  20 106  40 173  28  12  84\n",
      "  11   2  13  75  61  77  23  15 142 171 184 130  57 140 182  26  83  72\n",
      " 160  54 115  25  59 148  62 119 180 107  36 183  60  76  91 150 109  94\n",
      " 153 117  99  58  29  96  30  67 162  21   7  87 108 179 146 166 110  92\n",
      " 111 102 185  22 169   6  51 155 143   9 125   5 132 137  90  86  49  93\n",
      " 161 145  48  27 172  66 123  65 170  80 101  44 176  35   3 105  43  88\n",
      "  37 121 113  81  47 112 165 131 156 127 175   8 163  24 152  19  70 136\n",
      "  38 114  74 149 126  17 118  89  79 157 103  18 168 177 178 158  64  55\n",
      "  78  16  53  10  85   4 135 122  14  50 141 181  45 154 133  52  34 174\n",
      "  98  31 129 100   1  71  42 164 159 134  82  41  97  46  56   0 147  39\n",
      "  69 104 116  63 167  32] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "            *** MODEL TRAINING AND VALIDATION PHASE FOR TEST 5 \n",
      "            TRAINING 1 START... XXXXX \n",
      "            ***************************************************************************************************\n",
      "            Validation=> 186 ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] \n",
      "            Training=> 186 ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] \n",
      "            \n",
      "\n",
      "            *** ML MODEL FOR TEST:5, TRAINING:1 AND MODEL: LR \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: logistic_regression_parameters\n",
      "All parameters: ['penalty', 'solver', 'max_iter', 'C'], [['l2', 'elasticnet'], [50, 100, 130, 150, 170, 200], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]], [0, 2, 3]\n",
      "Parameters:  {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "        From training? True, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\"\n",
      "            Selected features: ['W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "            \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: LR - {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 84 candidates, totalling 420 fits\n",
      "\n",
      "        From training? False, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[85 17]\n",
      " [ 7 77]]\n",
      "[[77  7 17 85]\n",
      " [85 17  7 77]]\n",
      "[[162  24]\n",
      " [ 24 162]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.09677419354838\n",
      " Precision = [0.81914894 0.92391304]\n",
      " Recall = [0.91666667 0.83333333]\n",
      " F1 score = [0.86516854 0.87628866]\n",
      " AUC score = 87.5\n",
      " Support = [ 84 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.09677419354838\n",
      " Precision = [0.92391304 0.81914894]\n",
      " Recall = [0.83333333 0.91666667]\n",
      " F1 score = [0.87628866 0.86516854]\n",
      " AUC score = 87.49999999999999\n",
      " Support = [102  84]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[85 17]\n",
      " [ 7 77]]\n",
      " Accuracy (acc): 87.097\n",
      " Precision (prc): 81.915\n",
      " Recall (rec): 91.667\n",
      " Sensitivity (sns): 91.667\n",
      " Specificity (spc): 83.333\n",
      " F1 Score (f1s): 86.517\n",
      " ROC AUC (AUC): 0.875\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: LogisticRegression(max_iter=50) \n",
      "        Best parameters of the model: {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
      "\n",
      "                             model  \\\n",
      "0  LogisticRegression(max_iter=50)   \n",
      "\n",
      "                              model_parameters  model_scores  \\\n",
      "0  {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'}         91.76   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[85, 17], [7, 77]]    87.097     81.915  91.667       91.667       83.333   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    86.517    0.875   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:5, TRAINING:1 AND MODEL: SVC \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: support_vector_classifier_parameters\n",
      "All parameters: ['probability', 'C', 'kernel', 'gamma', 'degree', 'class_weightdict'], [[True], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], ['linear', 'rbf', 'poly']], [0, 1, 2]\n",
      "Parameters:  {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "        From training? True, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "210 fits failed out of a total of 420.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.88235294        nan 0.88235294        nan 0.88235294        nan\n",
      " 0.88235294        nan 0.88235294        nan 0.88235294        nan\n",
      " 0.88235294        nan 0.88235294        nan 0.88235294        nan\n",
      " 0.88235294        nan 0.88235294        nan 0.88235294        nan\n",
      " 0.90588235        nan 0.90588235        nan 0.90588235        nan\n",
      " 0.90588235        nan 0.90588235        nan 0.90588235        nan\n",
      " 0.91764706        nan 0.91764706        nan 0.91764706        nan\n",
      " 0.91764706        nan 0.91764706        nan 0.91764706        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.8691396         nan 0.8691396         nan 0.8691396         nan\n",
      " 0.8691396         nan 0.8691396         nan 0.8691396         nan\n",
      " 0.8691396         nan 0.8691396         nan 0.8691396         nan\n",
      " 0.8691396         nan 0.8691396         nan 0.8691396         nan\n",
      " 0.88696225        nan 0.88696225        nan 0.88696225        nan\n",
      " 0.88696225        nan 0.88696225        nan 0.88696225        nan\n",
      " 0.90776997        nan 0.90776997        nan 0.90776997        nan\n",
      " 0.90776997        nan 0.90776997        nan 0.90776997        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.86698287        nan 0.86698287        nan 0.86698287        nan\n",
      " 0.86698287        nan 0.86698287        nan 0.86698287        nan\n",
      " 0.86182158        nan 0.86182158        nan 0.86182158        nan\n",
      " 0.86182158        nan 0.86182158        nan 0.86182158        nan\n",
      " 0.87122708        nan 0.87122708        nan 0.87122708        nan\n",
      " 0.87122708        nan 0.87122708        nan 0.87122708        nan\n",
      " 0.87764706        nan 0.87764706        nan 0.87764706        nan\n",
      " 0.87764706        nan 0.87764706        nan 0.87764706        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.83724435        nan 0.83724435        nan 0.83724435        nan\n",
      " 0.83724435        nan 0.83724435        nan 0.83724435        nan\n",
      " 0.8361056         nan 0.8361056         nan 0.8361056         nan\n",
      " 0.8361056         nan 0.8361056         nan 0.8361056         nan\n",
      " 0.84860836        nan 0.84860836        nan 0.84860836        nan\n",
      " 0.84860836        nan 0.84860836        nan 0.84860836        nan\n",
      " 0.85793826        nan 0.85793826        nan 0.85793826        nan\n",
      " 0.85793826        nan 0.85793826        nan 0.85793826        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: SVC - {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[83 19]\n",
      " [ 5 79]]\n",
      "[[79  5 19 83]\n",
      " [83 19  5 79]]\n",
      "[[162  24]\n",
      " [ 24 162]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.09677419354838\n",
      " Precision = [0.80612245 0.94318182]\n",
      " Recall = [0.94047619 0.81372549]\n",
      " F1 score = [0.86813187 0.87368421]\n",
      " AUC score = 87.71008403361344\n",
      " Support = [ 84 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.09677419354838\n",
      " Precision = [0.94318182 0.80612245]\n",
      " Recall = [0.81372549 0.94047619]\n",
      " F1 score = [0.87368421 0.86813187]\n",
      " AUC score = 87.71008403361344\n",
      " Support = [102  84]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[83 19]\n",
      " [ 5 79]]\n",
      " Accuracy (acc): 87.097\n",
      " Precision (prc): 80.612\n",
      " Recall (rec): 94.048\n",
      " Sensitivity (sns): 94.048\n",
      " Specificity (spc): 81.373\n",
      " F1 Score (f1s): 86.813\n",
      " ROC AUC (AUC): 0.877\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: SVC(C=0.5, kernel='linear', probability=True) \n",
      "        Best parameters of the model: {'C': 0.5, 'kernel': 'linear', 'probability': True} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
      "\n",
      "                                           model  \\\n",
      "0  SVC(C=0.5, kernel='linear', probability=True)   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'C': 0.5, 'kernel': 'linear', 'probability': ...         92.94   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[83, 19], [5, 79]]    87.097     80.612  94.048       94.048       81.373   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    86.813    0.877   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:5, TRAINING:1 AND MODEL: NB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: naive_bayes_parameters\n",
      "All parameters: ['var_smoothing'], [[1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]], [0]\n",
      "Parameters:  {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "        From training? True, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: NB - {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "\n",
      "        From training? False, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[84 18]\n",
      " [ 5 79]]\n",
      "[[79  5 18 84]\n",
      " [84 18  5 79]]\n",
      "[[163  23]\n",
      " [ 23 163]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.63440860215054\n",
      " Precision = [0.81443299 0.94382022]\n",
      " Recall = [0.94047619 0.82352941]\n",
      " F1 score = [0.87292818 0.87958115]\n",
      " AUC score = 88.20028011204482\n",
      " Support = [ 84 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.63440860215054\n",
      " Precision = [0.94382022 0.81443299]\n",
      " Recall = [0.82352941 0.94047619]\n",
      " F1 score = [0.87958115 0.87292818]\n",
      " AUC score = 88.20028011204482\n",
      " Support = [102  84]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[84 18]\n",
      " [ 5 79]]\n",
      " Accuracy (acc): 87.634\n",
      " Precision (prc): 81.443\n",
      " Recall (rec): 94.048\n",
      " Sensitivity (sns): 94.048\n",
      " Specificity (spc): 82.353\n",
      " F1 Score (f1s): 87.293\n",
      " ROC AUC (AUC): 0.882\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GaussianNB(var_smoothing=0.005623413251903491) \n",
      "        Best parameters of the model: {'var_smoothing': 0.005623413251903491} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
      "\n",
      "                                            model  \\\n",
      "0  GaussianNB(var_smoothing=0.005623413251903491)   \n",
      "\n",
      "                          model_parameters  model_scores     confusion_matrix  \\\n",
      "0  {'var_smoothing': 0.005623413251903491}         92.94  [[84, 18], [5, 79]]   \n",
      "\n",
      "   accuracy  precision  recall  sensitivity  specificity  f1_score  roc_auc  \n",
      "0    87.634     81.443  94.048       94.048       82.353    87.293    0.882   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:5, TRAINING:1 AND MODEL: KNN \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: k_nearest_neighbors_parameters\n",
      "All parameters: ['n_neighbors', 'p', 'metric', 'n_splits'], [[2, 3, 5, 10, 15, 25, 35], ['manhattan', 'minkowski', 'euclidean']], [0, 2]\n",
      "Parameters:  {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "        From training? True, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: KNN - {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[82 20]\n",
      " [ 7 77]]\n",
      "[[77  7 20 82]\n",
      " [82 20  7 77]]\n",
      "[[159  27]\n",
      " [ 27 159]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 85.48387096774194\n",
      " Precision = [0.79381443 0.92134831]\n",
      " Recall = [0.91666667 0.80392157]\n",
      " F1 score = [0.85082873 0.85863874]\n",
      " AUC score = 86.02941176470587\n",
      " Support = [ 84 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 85.48387096774194\n",
      " Precision = [0.92134831 0.79381443]\n",
      " Recall = [0.80392157 0.91666667]\n",
      " F1 score = [0.85863874 0.85082873]\n",
      " AUC score = 86.02941176470587\n",
      " Support = [102  84]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[82 20]\n",
      " [ 7 77]]\n",
      " Accuracy (acc): 85.484\n",
      " Precision (prc): 79.381\n",
      " Recall (rec): 91.667\n",
      " Sensitivity (sns): 91.667\n",
      " Specificity (spc): 80.392\n",
      " F1 Score (f1s): 85.083\n",
      " ROC AUC (AUC): 0.86\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: KNeighborsClassifier(metric='manhattan', n_neighbors=25) \n",
      "        Best parameters of the model: {'metric': 'manhattan', 'n_neighbors': 25} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
      "\n",
      "                                               model  \\\n",
      "0  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
      "\n",
      "                             model_parameters  model_scores  \\\n",
      "0  {'metric': 'manhattan', 'n_neighbors': 25}         92.94   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[82, 20], [7, 77]]    85.484     79.381  91.667       91.667       80.392   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    85.083     0.86   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:5, TRAINING:1 AND MODEL: DT \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: decision_tree_parameters\n",
      "All parameters: ['max_depth', 'criterion', 'splitter', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "        From training? True, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: DT - {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "        From training? False, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[87 15]\n",
      " [ 6 78]]\n",
      "[[78  6 15 87]\n",
      " [87 15  6 78]]\n",
      "[[165  21]\n",
      " [ 21 165]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 88.70967741935483\n",
      " Precision = [0.83870968 0.93548387]\n",
      " Recall = [0.92857143 0.85294118]\n",
      " F1 score = [0.88135593 0.89230769]\n",
      " AUC score = 89.07563025210085\n",
      " Support = [ 84 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 88.70967741935483\n",
      " Precision = [0.93548387 0.83870968]\n",
      " Recall = [0.85294118 0.92857143]\n",
      " F1 score = [0.89230769 0.88135593]\n",
      " AUC score = 89.07563025210084\n",
      " Support = [102  84]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[87 15]\n",
      " [ 6 78]]\n",
      " Accuracy (acc): 88.71\n",
      " Precision (prc): 83.871\n",
      " Recall (rec): 92.857\n",
      " Sensitivity (sns): 92.857\n",
      " Specificity (spc): 85.294\n",
      " F1 Score (f1s): 88.136\n",
      " ROC AUC (AUC): 0.891\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: DecisionTreeClassifier(max_depth=2) \n",
      "        Best parameters of the model: {'criterion': 'gini', 'max_depth': 2} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
      "\n",
      "                                 model                       model_parameters  \\\n",
      "0  DecisionTreeClassifier(max_depth=2)  {'criterion': 'gini', 'max_depth': 2}   \n",
      "\n",
      "   model_scores     confusion_matrix  accuracy  precision  recall  \\\n",
      "0         89.41  [[87, 15], [6, 78]]     88.71     83.871  92.857   \n",
      "\n",
      "   sensitivity  specificity  f1_score  roc_auc  \n",
      "0       92.857       85.294    88.136    0.891   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:5, TRAINING:1 AND MODEL: RF \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: random_forest_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'criterion', 'splitter', 'max_features', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "        From training? True, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.89411765 0.84705882 0.84558824 0.85808824 0.84558824 0.88235294\n",
      " 0.84632353 0.82205882 0.83382353 0.88235294 0.82352941 0.81029412\n",
      " 0.81176471 0.81102941 0.7875     0.79852941 0.82279412 0.82279412\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.94332748 0.98208955 0.96733977 1.         1.         1.\n",
      " 1.         1.         1.         0.94034241 0.98208955 0.96733977\n",
      " 1.         1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.86108173 0.83898663 0.85006888 0.8569506  0.8477551  0.87121831\n",
      " 0.84361727 0.8352551  0.8416945  0.85270734 0.80421675 0.80822678\n",
      " 0.80554232 0.81771505 0.7927564  0.80100538 0.8206999  0.80994721\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.88608771 0.92576979 0.96143767 0.99854015 1.         1.\n",
      " 1.         1.         1.         0.88455514 0.91553906 0.96145289\n",
      " 1.         1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: RF - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "\n",
      "        From training? False, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[94  8]\n",
      " [ 1 83]]\n",
      "[[83  1  8 94]\n",
      " [94  8  1 83]]\n",
      "[[177   9]\n",
      " [  9 177]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 95.16129032258065\n",
      " Precision = [0.91208791 0.98947368]\n",
      " Recall = [0.98809524 0.92156863]\n",
      " F1 score = [0.94857143 0.95431472]\n",
      " AUC score = 95.48319327731092\n",
      " Support = [ 84 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 95.16129032258065\n",
      " Precision = [0.98947368 0.91208791]\n",
      " Recall = [0.92156863 0.98809524]\n",
      " F1 score = [0.95431472 0.94857143]\n",
      " AUC score = 95.48319327731093\n",
      " Support = [102  84]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[94  8]\n",
      " [ 1 83]]\n",
      " Accuracy (acc): 95.161\n",
      " Precision (prc): 91.209\n",
      " Recall (rec): 98.81\n",
      " Sensitivity (sns): 98.81\n",
      " Specificity (spc): 92.157\n",
      " F1 Score (f1s): 94.857\n",
      " ROC AUC (AUC): 0.955\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: RandomForestClassifier(max_depth=5, n_estimators=5) \n",
      "        Best parameters of the model: {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 5} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
      "\n",
      "                                               model  \\\n",
      "0  (DecisionTreeClassifier(max_depth=5, max_featu...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'criterion': 'gini', 'max_depth': 5, 'n_estim...         92.87   \n",
      "\n",
      "     confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[94, 8], [1, 83]]    95.161     91.209   98.81        98.81       92.157   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    94.857    0.955   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:5, TRAINING:1 AND MODEL: GB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: gradient_boosting_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'learning_rate', 'max_features', 'loss', 'min_samples_split', 'min_samples_leaf'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.5, 1.0]], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "        From training? True, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "405 fits failed out of a total of 1215.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 467, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.84705882 0.89411765 0.90588235 0.89411765 0.91764706 0.91764706\n",
      " 0.90588235 0.91764706 0.91764706 0.87058824 0.86911765 0.90588235\n",
      " 0.90588235 0.90588235 0.91764706 0.90588235 0.91764706 0.91764706\n",
      " 0.85808824 0.92867647 0.90588235 0.89264706 0.88235294 0.89411765\n",
      " 0.89411765 0.89411765 0.90588235 0.81176471 0.78823529 0.84632353\n",
      " 0.88235294 0.88161765 0.89411765 0.87058824 0.89411765 0.90588235\n",
      " 0.81029412 0.83455882 0.83529412 0.87058824 0.88161765 0.89411765\n",
      " 0.89411765 0.89411765 0.89411765 0.76323529 0.85808824 0.84705882\n",
      " 0.85882353 0.88235294 0.88235294 0.88235294 0.90588235 0.90588235\n",
      " 0.75147059 0.79779412 0.83529412 0.85882353 0.87058824 0.88235294\n",
      " 0.89411765 0.90588235 0.90588235 0.76323529 0.84632353 0.82132353\n",
      " 0.87058824 0.89411765 0.87058824 0.89411765 0.89411765 0.90588235\n",
      " 0.71397059 0.79852941 0.79852941 0.88161765 0.90588235 0.89411765\n",
      " 0.90588235 0.88235294 0.89411765 0.88235294 0.88235294 0.91764706\n",
      " 0.91764706 0.91764706 0.91764706 0.90588235 0.90588235 0.91764706\n",
      " 0.84632353 0.85882353 0.89411765 0.90588235 0.90588235 0.91764706\n",
      " 0.91764706 0.91764706 0.91764706 0.88161765 0.87058824 0.89411765\n",
      " 0.90588235 0.91764706 0.90588235 0.90588235 0.91764706 0.90588235\n",
      " 0.77573529 0.84558824 0.83529412 0.88235294 0.85882353 0.89411765\n",
      " 0.90588235 0.89411765 0.89411765 0.76470588 0.82352941 0.85808824\n",
      " 0.89411765 0.84705882 0.90588235 0.89411765 0.90588235 0.90588235\n",
      " 0.81176471 0.84705882 0.8        0.87058824 0.87058824 0.84705882\n",
      " 0.88235294 0.89411765 0.90588235 0.82279412 0.77573529 0.82279412\n",
      " 0.90588235 0.89411765 0.87058824 0.88235294 0.89411765 0.89411765\n",
      " 0.81029412 0.88235294 0.7875     0.85882353 0.88235294 0.88235294\n",
      " 0.88235294 0.90588235 0.88235294 0.83529412 0.85735294 0.82352941\n",
      " 0.85882353 0.87058824 0.85882353 0.89411765 0.88235294 0.90588235\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.91957858 0.93160667 0.91079895 0.9286216  0.93160667 0.93748903\n",
      " 0.9286216  0.9286216  0.92568042 0.93151888 0.94345917 0.9404302\n",
      " 0.93748903 0.9523705  0.94644425 0.94648815 0.96132572 0.94947322\n",
      " 0.96123793 0.97028095 0.98511853 0.99407375 0.99108867 0.99402985\n",
      " 0.99407375 0.99705882 1.         0.96725198 0.9761633  0.97322212\n",
      " 1.         0.99701493 0.99701493 1.         1.         1.\n",
      " 0.96128183 0.98208955 0.98507463 0.99705882 1.         1.\n",
      " 1.         1.         1.         0.95531168 0.97919227 0.9881036\n",
      " 0.99701493 0.99402985 1.         0.99701493 1.         1.\n",
      " 0.94956102 0.9762511  0.99402985 0.99701493 1.         1.\n",
      " 1.         1.         1.         0.95544337 0.96720808 0.9880597\n",
      " 0.99701493 1.         1.         1.         1.         1.\n",
      " 0.94951712 0.98213345 0.98217735 1.         1.         1.\n",
      " 1.         1.         1.         0.93450395 0.92563652 0.940518\n",
      " 0.92559263 0.93160667 0.9166813  0.9166813  0.9286216  0.9286216\n",
      " 0.93744513 0.93748903 0.94345917 0.95838455 0.94938543 0.94942932\n",
      " 0.94354697 0.95539947 0.94350307 0.96119403 0.96725198 0.9880597\n",
      " 0.9880597  0.9881036  0.99701493 0.99104478 0.99705882 0.99407375\n",
      " 0.9524144  0.97330992 0.99104478 0.99701493 1.         1.\n",
      " 1.         1.         1.         0.9761633  0.98507463 0.9881036\n",
      " 0.99705882 1.         1.         1.         1.         1.\n",
      " 0.96145742 0.98507463 0.98507463 0.98507463 1.         0.99701493\n",
      " 1.         1.         1.         0.96725198 0.9880597  0.97919227\n",
      " 0.99701493 0.99402985 1.         1.         1.         1.\n",
      " 0.95250219 0.98213345 0.99108867 0.99701493 1.         1.\n",
      " 1.         1.         1.         0.9762511  0.99104478 0.9880597\n",
      " 0.99701493 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.85108225 0.87385057 0.87666667 0.87615554 0.88551724 0.88551724\n",
      " 0.88190561 0.88551724 0.88796622 0.84851709 0.85539598 0.87710369\n",
      " 0.88190561 0.87945664 0.88796622 0.87710369 0.88551724 0.88551724\n",
      " 0.80967532 0.88665763 0.87710369 0.86136078 0.86460369 0.8706643\n",
      " 0.8706643  0.8706643  0.87484125 0.81385348 0.79043562 0.83677659\n",
      " 0.86016418 0.86232903 0.86840186 0.85548641 0.8706643  0.87710369\n",
      " 0.81416897 0.83616932 0.83131245 0.85774886 0.86421269 0.86872931\n",
      " 0.86272004 0.86840186 0.86840186 0.76322489 0.83247956 0.84293508\n",
      " 0.84978992 0.85892188 0.86695664 0.86695664 0.87710369 0.87484125\n",
      " 0.7681085  0.81888408 0.83428571 0.85292517 0.85622931 0.86695664\n",
      " 0.87301724 0.87710369 0.87945664 0.76116756 0.84439896 0.83297275\n",
      " 0.85248248 0.86840186 0.85396687 0.8706643  0.86878064 0.87710369\n",
      " 0.76169833 0.80865798 0.81192204 0.84825258 0.87710369 0.87339603\n",
      " 0.87484125 0.86228992 0.86840186 0.84321426 0.8601018  0.88551724\n",
      " 0.88551724 0.88796622 0.88796622 0.88190561 0.87945664 0.88796622\n",
      " 0.82370519 0.82867647 0.87546622 0.87980296 0.87945664 0.88551724\n",
      " 0.88796622 0.88796622 0.88796622 0.85043088 0.84926652 0.87264286\n",
      " 0.88121212 0.8831643  0.87710369 0.87710369 0.88551724 0.87710369\n",
      " 0.77820636 0.83115355 0.83843318 0.85854309 0.85130946 0.86840186\n",
      " 0.87710369 0.86840186 0.86878064 0.7515989  0.83143046 0.85213064\n",
      " 0.86691712 0.84173519 0.87945664 0.86840186 0.87139959 0.87484125\n",
      " 0.80224562 0.82997048 0.80879331 0.86051724 0.85649763 0.84173519\n",
      " 0.85896104 0.8706643  0.87710369 0.80058576 0.79700993 0.82917164\n",
      " 0.87139959 0.86872931 0.86051724 0.86460369 0.86498248 0.8706643\n",
      " 0.80656612 0.83959696 0.80104839 0.83788462 0.86051724 0.8581643\n",
      " 0.86154702 0.87484125 0.86695664 0.76542484 0.84436311 0.82603448\n",
      " 0.83137255 0.85396687 0.85130946 0.8706643  0.86234125 0.87710369\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.86996609 0.87867078 0.86876795 0.87562541 0.87984743 0.88062791\n",
      " 0.87713164 0.87594962 0.87668658 0.88318006 0.89568068 0.89528541\n",
      " 0.89255831 0.90286632 0.89122765 0.89362975 0.89661258 0.89519066\n",
      " 0.92988965 0.93369795 0.95181382 0.94815326 0.95886991 0.95630073\n",
      " 0.96310682 0.96592552 0.96189146 0.95758828 0.96909496 0.97903052\n",
      " 0.99126156 0.99259211 0.99261389 0.99413849 0.99411765 0.99416027\n",
      " 0.95705687 0.97772913 0.98951378 0.99557734 0.99854015 0.99851852\n",
      " 0.99851852 1.         1.         0.96522852 0.97777207 0.99250297\n",
      " 0.99265682 0.99553263 0.99854015 0.99849624 1.         1.\n",
      " 0.96067604 0.97633563 0.99113097 0.98969451 0.99851852 0.99705866\n",
      " 1.         1.         1.         0.96105427 0.97568565 0.98816667\n",
      " 0.99849624 0.99705882 0.99851852 1.         1.         1.\n",
      " 0.95512497 0.97788302 0.98502807 0.99411765 0.99705882 1.\n",
      " 1.         1.         1.         0.87325434 0.88145401 0.87991702\n",
      " 0.87444491 0.8786923  0.86856581 0.87202026 0.87475754 0.87710515\n",
      " 0.88990318 0.887559   0.89570674 0.89984676 0.89164853 0.89528937\n",
      " 0.88615373 0.89346087 0.88492712 0.91160707 0.93326509 0.93358961\n",
      " 0.94654856 0.94926931 0.94986479 0.94683547 0.95638084 0.95093639\n",
      " 0.94841307 0.96045093 0.97518109 0.98681663 0.99409586 0.98971662\n",
      " 0.99416027 0.99124041 0.99559897 0.97339155 0.98217667 0.98958491\n",
      " 0.99409586 0.99559897 0.99854015 0.99851852 1.         1.\n",
      " 0.96138646 0.97404645 0.98509559 0.98951773 0.99557718 0.99849624\n",
      " 0.99851852 1.         1.         0.96441355 0.98228378 0.98794594\n",
      " 0.99411621 0.995511   1.         1.         1.         1.\n",
      " 0.95535309 0.97494361 0.98967223 0.99701476 1.         1.\n",
      " 1.         1.         1.         0.96659386 0.98370289 0.98954003\n",
      " 0.99117599 0.99854015 1.         1.         0.99851852 1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: GB - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 405 candidates, totalling 2025 fits\n",
      "\n",
      "        From training? False, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[88 14]\n",
      " [ 2 82]]\n",
      "[[82  2 14 88]\n",
      " [88 14  2 82]]\n",
      "[[170  16]\n",
      " [ 16 170]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 91.39784946236558\n",
      " Precision = [0.85416667 0.97777778]\n",
      " Recall = [0.97619048 0.8627451 ]\n",
      " F1 score = [0.91111111 0.91666667]\n",
      " AUC score = 91.94677871148458\n",
      " Support = [ 84 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 91.39784946236558\n",
      " Precision = [0.97777778 0.85416667]\n",
      " Recall = [0.8627451  0.97619048]\n",
      " F1 score = [0.91666667 0.91111111]\n",
      " AUC score = 91.9467787114846\n",
      " Support = [102  84]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[88 14]\n",
      " [ 2 82]]\n",
      " Accuracy (acc): 91.398\n",
      " Precision (prc): 85.417\n",
      " Recall (rec): 97.619\n",
      " Sensitivity (sns): 97.619\n",
      " Specificity (spc): 86.275\n",
      " F1 Score (f1s): 91.111\n",
      " ROC AUC (AUC): 0.919\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GradientBoostingClassifier(learning_rate=0.05, max_depth=2, n_estimators=50) \n",
      "        Best parameters of the model: {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 50} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
      "\n",
      "                                               model  \\\n",
      "0  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'learning_rate': 0.05, 'max_depth': 2, 'n_est...         94.12   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[88, 14], [2, 82]]    91.398     85.417  97.619       97.619       86.275   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    91.111    0.919   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:5, TRAINING:1 AND MODEL: XGB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: xtreme_gradient_boosting_parameters\n",
      "All parameters: ['max_depth', 'eta', 'max_leaves'], [[2, 3, 6, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "        From training? True, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: XGB - {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "\n",
      "        From training? False, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[89 13]\n",
      " [ 1 83]]\n",
      "[[83  1 13 89]\n",
      " [89 13  1 83]]\n",
      "[[172  14]\n",
      " [ 14 172]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 92.47311827956989\n",
      " Precision = [0.86458333 0.98888889]\n",
      " Recall = [0.98809524 0.87254902]\n",
      " F1 score = [0.92222222 0.92708333]\n",
      " AUC score = 93.03221288515407\n",
      " Support = [ 84 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 92.47311827956989\n",
      " Precision = [0.98888889 0.86458333]\n",
      " Recall = [0.87254902 0.98809524]\n",
      " F1 score = [0.92708333 0.92222222]\n",
      " AUC score = 93.03221288515407\n",
      " Support = [102  84]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[89 13]\n",
      " [ 1 83]]\n",
      " Accuracy (acc): 92.473\n",
      " Precision (prc): 86.458\n",
      " Recall (rec): 98.81\n",
      " Sensitivity (sns): 98.81\n",
      " Specificity (spc): 87.255\n",
      " F1 Score (f1s): 92.222\n",
      " ROC AUC (AUC): 0.93\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False, eta=0.05,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.0500000007, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=2, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, ...) \n",
      "        Best parameters of the model: {'eta': 0.05, 'max_depth': 2} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
      "\n",
      "                                               model  \\\n",
      "0  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
      "\n",
      "                model_parameters  model_scores     confusion_matrix  accuracy  \\\n",
      "0  {'eta': 0.05, 'max_depth': 2}         90.59  [[89, 13], [1, 83]]    92.473   \n",
      "\n",
      "   precision  recall  sensitivity  specificity  f1_score  roc_auc  \n",
      "0     86.458   98.81        98.81       87.255    92.222     0.93   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            Best model index calculation  \n",
      "            \n",
      "WWWWWWWWWWWWWWWWWWW recall    Training_No  Model_No Model_Name  \\\n",
      "0            1         1         LR   \n",
      "1            1         2        SVC   \n",
      "2            1         3         NB   \n",
      "3            1         4        KNN   \n",
      "4            1         5         DT   \n",
      "5            1         6         RF   \n",
      "6            1         7         GB   \n",
      "7            1         8        XGB   \n",
      "\n",
      "                                              method  \\\n",
      "0  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
      "1  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
      "2  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
      "3  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
      "4  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
      "5  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
      "6  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
      "7  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
      "\n",
      "                                               model  \\\n",
      "0                    LogisticRegression(max_iter=50)   \n",
      "1      SVC(C=0.5, kernel='linear', probability=True)   \n",
      "2     GaussianNB(var_smoothing=0.005623413251903491)   \n",
      "3  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
      "4                DecisionTreeClassifier(max_depth=2)   \n",
      "5  (DecisionTreeClassifier(max_depth=5, max_featu...   \n",
      "6  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
      "7  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0        {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'}         91.76   \n",
      "1  {'C': 0.5, 'kernel': 'linear', 'probability': ...         92.94   \n",
      "2            {'var_smoothing': 0.005623413251903491}         92.94   \n",
      "3         {'metric': 'manhattan', 'n_neighbors': 25}         92.94   \n",
      "4              {'criterion': 'gini', 'max_depth': 2}         89.41   \n",
      "5  {'criterion': 'gini', 'max_depth': 5, 'n_estim...         92.87   \n",
      "6  {'learning_rate': 0.05, 'max_depth': 2, 'n_est...         94.12   \n",
      "7                      {'eta': 0.05, 'max_depth': 2}         90.59   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[85, 17], [7, 77]]    87.097     81.915  91.667       91.667       83.333   \n",
      "1  [[83, 19], [5, 79]]    87.097     80.612  94.048       94.048       81.373   \n",
      "2  [[84, 18], [5, 79]]    87.634     81.443  94.048       94.048       82.353   \n",
      "3  [[82, 20], [7, 77]]    85.484     79.381  91.667       91.667       80.392   \n",
      "4  [[87, 15], [6, 78]]    88.710     83.871  92.857       92.857       85.294   \n",
      "5   [[94, 8], [1, 83]]    95.161     91.209  98.810       98.810       92.157   \n",
      "6  [[88, 14], [2, 82]]    91.398     85.417  97.619       97.619       86.275   \n",
      "7  [[89, 13], [1, 83]]    92.473     86.458  98.810       98.810       87.255   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    86.517    0.875  \n",
      "1    86.813    0.877  \n",
      "2    87.293    0.882  \n",
      "3    85.083    0.860  \n",
      "4    88.136    0.891  \n",
      "5    94.857    0.955  \n",
      "6    91.111    0.919  \n",
      "7    92.222    0.930  \n",
      "\n",
      "            ***************************************************************************************************\n",
      "            TRAINING 1 END... \n",
      "            \n",
      "train- tri, tmp_train_ids, tmp_val_ids, tr_dat, val_dat 1 [155 120 129  27  59 151 158  89 169  97  93  57  96  17  50  25 144 175\n",
      " 122  73 127  77  51 152 118  13  37 161 149 145   0 174 110 180  83  98\n",
      " 139  20  15  30 131 164 177  43 178  69 179  12 147  10 100 119 182  55\n",
      "   4 102 130  61 168 124 142   1  74  36  35 153 115  88  90  56   8 123\n",
      "  22  23   3 171   9  40  54 104  95 135 107 116 141 173  81  33 181  86\n",
      "  32  41 117 105 157 166  72   7 114  19 109  48  34 183  24 163 165 143\n",
      " 150  14 108   5  79  94  58 132 101  16  31 125  42  71  18  99   2 184\n",
      "  38 159 137 172 128  60 170  29 112  44  68  65  75 154 126  63  39 156\n",
      " 138 148 136  53  82  76  26  87 140  46  67 111 106 176  47  80  52  11\n",
      "  91 162  64  21 121  84  70   6 134  78 160 133  92  45  85  66 113 146\n",
      "  49 167 103  62  28 185] [155 120 129  27  59 151 158  89 169  97  93  57  96  17  50  25 144 175\n",
      " 122  73 127  77  51 152 118  13  37 161 149 145   0 174 110 180  83  98\n",
      " 139  20  15  30 131 164 177  43 178  69 179  12 147  10 100 119 182  55\n",
      "   4 102 130  61 168 124 142   1  74  36  35 153 115  88  90  56   8 123\n",
      "  22  23   3 171   9  40  54 104  95 135 107 116 141 173  81  33 181  86\n",
      "  32  41 117 105 157 166  72   7 114  19 109  48  34 183  24 163 165 143\n",
      " 150  14 108   5  79  94  58 132 101  16  31 125  42  71  18  99   2 184\n",
      "  38 159 137 172 128  60 170  29 112  44  68  65  75 154 126  63  39 156\n",
      " 138 148 136  53  82  76  26  87 140  46  67 111 106 176  47  80  52  11\n",
      "  91 162  64  21 121  84  70   6 134  78 160 133  92  45  85  66 113 146\n",
      "  49 167 103  62  28 185] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "            *** MODEL TRAINING AND VALIDATION PHASE FOR TEST 5 \n",
      "            TRAINING 2 START... XXXXX \n",
      "            ***************************************************************************************************\n",
      "            Validation=> 186 ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] \n",
      "            Training=> 186 ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] \n",
      "            \n",
      "\n",
      "            *** ML MODEL FOR TEST:5, TRAINING:2 AND MODEL: LR \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: logistic_regression_parameters\n",
      "All parameters: ['penalty', 'solver', 'max_iter', 'C'], [['l2', 'elasticnet'], [50, 100, 130, 150, 170, 200], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]], [0, 2, 3]\n",
      "Parameters:  {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "        From training? True, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: LR - {'penalty': ['l2', 'elasticnet'], 'max_iter': [50, 100, 130, 150, 170, 200], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 84 candidates, totalling 420 fits\n",
      "\n",
      "        From training? False, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[85 17]\n",
      " [ 7 77]]\n",
      "[[77  7 17 85]\n",
      " [85 17  7 77]]\n",
      "[[162  24]\n",
      " [ 24 162]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.09677419354838\n",
      " Precision = [0.81914894 0.92391304]\n",
      " Recall = [0.91666667 0.83333333]\n",
      " F1 score = [0.86516854 0.87628866]\n",
      " AUC score = 87.5\n",
      " Support = [ 84 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.09677419354838\n",
      " Precision = [0.92391304 0.81914894]\n",
      " Recall = [0.83333333 0.91666667]\n",
      " F1 score = [0.87628866 0.86516854]\n",
      " AUC score = 87.49999999999999\n",
      " Support = [102  84]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[85 17]\n",
      " [ 7 77]]\n",
      " Accuracy (acc): 87.097\n",
      " Precision (prc): 81.915\n",
      " Recall (rec): 91.667\n",
      " Sensitivity (sns): 91.667\n",
      " Specificity (spc): 83.333\n",
      " F1 Score (f1s): 86.517\n",
      " ROC AUC (AUC): 0.875\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_iter': [50, 100, 130, 150, 170, 200],\n",
      "                         'penalty': ['l2', 'elasticnet']},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: LogisticRegression(max_iter=50) \n",
      "        Best parameters of the model: {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
      "\n",
      "                             model  \\\n",
      "0  LogisticRegression(max_iter=50)   \n",
      "\n",
      "                              model_parameters  model_scores  \\\n",
      "0  {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'}         91.76   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[85, 17], [7, 77]]    87.097     81.915  91.667       91.667       83.333   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    86.517    0.875   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:5, TRAINING:2 AND MODEL: SVC \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: support_vector_classifier_parameters\n",
      "All parameters: ['probability', 'C', 'kernel', 'gamma', 'degree', 'class_weightdict'], [[True], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], ['linear', 'rbf', 'poly']], [0, 1, 2]\n",
      "Parameters:  {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "        From training? True, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "210 fits failed out of a total of 420.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.88235294        nan 0.88235294        nan 0.88235294        nan\n",
      " 0.88235294        nan 0.88235294        nan 0.88235294        nan\n",
      " 0.88235294        nan 0.88235294        nan 0.88235294        nan\n",
      " 0.88235294        nan 0.88235294        nan 0.88235294        nan\n",
      " 0.90588235        nan 0.90588235        nan 0.90588235        nan\n",
      " 0.90588235        nan 0.90588235        nan 0.90588235        nan\n",
      " 0.91764706        nan 0.91764706        nan 0.91764706        nan\n",
      " 0.91764706        nan 0.91764706        nan 0.91764706        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.8691396         nan 0.8691396         nan 0.8691396         nan\n",
      " 0.8691396         nan 0.8691396         nan 0.8691396         nan\n",
      " 0.8691396         nan 0.8691396         nan 0.8691396         nan\n",
      " 0.8691396         nan 0.8691396         nan 0.8691396         nan\n",
      " 0.88696225        nan 0.88696225        nan 0.88696225        nan\n",
      " 0.88696225        nan 0.88696225        nan 0.88696225        nan\n",
      " 0.90776997        nan 0.90776997        nan 0.90776997        nan\n",
      " 0.90776997        nan 0.90776997        nan 0.90776997        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.86698287        nan 0.86698287        nan 0.86698287        nan\n",
      " 0.86698287        nan 0.86698287        nan 0.86698287        nan\n",
      " 0.86182158        nan 0.86182158        nan 0.86182158        nan\n",
      " 0.86182158        nan 0.86182158        nan 0.86182158        nan\n",
      " 0.87122708        nan 0.87122708        nan 0.87122708        nan\n",
      " 0.87122708        nan 0.87122708        nan 0.87122708        nan\n",
      " 0.87764706        nan 0.87764706        nan 0.87764706        nan\n",
      " 0.87764706        nan 0.87764706        nan 0.87764706        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.                nan 0.                nan 0.                nan\n",
      " 0.83724435        nan 0.83724435        nan 0.83724435        nan\n",
      " 0.83724435        nan 0.83724435        nan 0.83724435        nan\n",
      " 0.8361056         nan 0.8361056         nan 0.8361056         nan\n",
      " 0.8361056         nan 0.8361056         nan 0.8361056         nan\n",
      " 0.84860836        nan 0.84860836        nan 0.84860836        nan\n",
      " 0.84860836        nan 0.84860836        nan 0.84860836        nan\n",
      " 0.85793826        nan 0.85793826        nan 0.85793826        nan\n",
      " 0.85793826        nan 0.85793826        nan 0.85793826        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: SVC - {'probability': [True], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf', 'poly']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[83 19]\n",
      " [ 5 79]]\n",
      "[[79  5 19 83]\n",
      " [83 19  5 79]]\n",
      "[[162  24]\n",
      " [ 24 162]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.09677419354838\n",
      " Precision = [0.80612245 0.94318182]\n",
      " Recall = [0.94047619 0.81372549]\n",
      " F1 score = [0.86813187 0.87368421]\n",
      " AUC score = 87.71008403361344\n",
      " Support = [ 84 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.09677419354838\n",
      " Precision = [0.94318182 0.80612245]\n",
      " Recall = [0.81372549 0.94047619]\n",
      " F1 score = [0.87368421 0.86813187]\n",
      " AUC score = 87.71008403361344\n",
      " Support = [102  84]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[83 19]\n",
      " [ 5 79]]\n",
      " Accuracy (acc): 87.097\n",
      " Precision (prc): 80.612\n",
      " Recall (rec): 94.048\n",
      " Sensitivity (sns): 94.048\n",
      " Specificity (spc): 81.373\n",
      " F1 Score (f1s): 86.813\n",
      " ROC AUC (AUC): 0.877\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
      "             param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'kernel': ['linear', 'rbf', 'poly'],\n",
      "                         'probability': [True]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: SVC(C=0.5, kernel='linear', probability=True) \n",
      "        Best parameters of the model: {'C': 0.5, 'kernel': 'linear', 'probability': True} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
      "\n",
      "                                           model  \\\n",
      "0  SVC(C=0.5, kernel='linear', probability=True)   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'C': 0.5, 'kernel': 'linear', 'probability': ...         92.94   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[83, 19], [5, 79]]    87.097     80.612  94.048       94.048       81.373   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    86.813    0.877   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:5, TRAINING:2 AND MODEL: NB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: naive_bayes_parameters\n",
      "All parameters: ['var_smoothing'], [[1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]], [0]\n",
      "Parameters:  {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "        From training? True, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: NB - {'var_smoothing': [1.0, 0.005623413251903491, 3.1622776601683795e-05, 1.7782794100389227e-07, 1e-09]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "\n",
      "        From training? False, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[84 18]\n",
      " [ 5 79]]\n",
      "[[79  5 18 84]\n",
      " [84 18  5 79]]\n",
      "[[163  23]\n",
      " [ 23 163]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 87.63440860215054\n",
      " Precision = [0.81443299 0.94382022]\n",
      " Recall = [0.94047619 0.82352941]\n",
      " F1 score = [0.87292818 0.87958115]\n",
      " AUC score = 88.20028011204482\n",
      " Support = [ 84 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 87.63440860215054\n",
      " Precision = [0.94382022 0.81443299]\n",
      " Recall = [0.82352941 0.94047619]\n",
      " F1 score = [0.87958115 0.87292818]\n",
      " AUC score = 88.20028011204482\n",
      " Support = [102  84]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[84 18]\n",
      " [ 5 79]]\n",
      " Accuracy (acc): 87.634\n",
      " Precision (prc): 81.443\n",
      " Recall (rec): 94.048\n",
      " Sensitivity (sns): 94.048\n",
      " Specificity (spc): 82.353\n",
      " F1 Score (f1s): 87.293\n",
      " ROC AUC (AUC): 0.882\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
      "             param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
      "                                           3.1622776601683795e-05,\n",
      "                                           1.7782794100389227e-07, 1e-09]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GaussianNB(var_smoothing=0.005623413251903491) \n",
      "        Best parameters of the model: {'var_smoothing': 0.005623413251903491} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
      "\n",
      "                                            model  \\\n",
      "0  GaussianNB(var_smoothing=0.005623413251903491)   \n",
      "\n",
      "                          model_parameters  model_scores     confusion_matrix  \\\n",
      "0  {'var_smoothing': 0.005623413251903491}         92.94  [[84, 18], [5, 79]]   \n",
      "\n",
      "   accuracy  precision  recall  sensitivity  specificity  f1_score  roc_auc  \n",
      "0    87.634     81.443  94.048       94.048       82.353    87.293    0.882   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:5, TRAINING:2 AND MODEL: KNN \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: k_nearest_neighbors_parameters\n",
      "All parameters: ['n_neighbors', 'p', 'metric', 'n_splits'], [[2, 3, 5, 10, 15, 25, 35], ['manhattan', 'minkowski', 'euclidean']], [0, 2]\n",
      "Parameters:  {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "        From training? True, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: KNN - {'n_neighbors': [2, 3, 5, 10, 15, 25, 35], 'metric': ['manhattan', 'minkowski', 'euclidean']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "        From training? False, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[82 20]\n",
      " [ 7 77]]\n",
      "[[77  7 20 82]\n",
      " [82 20  7 77]]\n",
      "[[159  27]\n",
      " [ 27 159]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 85.48387096774194\n",
      " Precision = [0.79381443 0.92134831]\n",
      " Recall = [0.91666667 0.80392157]\n",
      " F1 score = [0.85082873 0.85863874]\n",
      " AUC score = 86.02941176470587\n",
      " Support = [ 84 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 85.48387096774194\n",
      " Precision = [0.92134831 0.79381443]\n",
      " Recall = [0.80392157 0.91666667]\n",
      " F1 score = [0.85863874 0.85082873]\n",
      " AUC score = 86.02941176470587\n",
      " Support = [102  84]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[82 20]\n",
      " [ 7 77]]\n",
      " Accuracy (acc): 85.484\n",
      " Precision (prc): 79.381\n",
      " Recall (rec): 91.667\n",
      " Sensitivity (sns): 91.667\n",
      " Specificity (spc): 80.392\n",
      " F1 Score (f1s): 85.083\n",
      " ROC AUC (AUC): 0.86\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
      "             param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
      "                         'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: KNeighborsClassifier(metric='manhattan', n_neighbors=25) \n",
      "        Best parameters of the model: {'metric': 'manhattan', 'n_neighbors': 25} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
      "\n",
      "                                               model  \\\n",
      "0  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
      "\n",
      "                             model_parameters  model_scores  \\\n",
      "0  {'metric': 'manhattan', 'n_neighbors': 25}         92.94   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[82, 20], [7, 77]]    85.484     79.381  91.667       91.667       80.392   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    85.083     0.86   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:5, TRAINING:2 AND MODEL: DT \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: decision_tree_parameters\n",
      "All parameters: ['max_depth', 'criterion', 'splitter', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "        From training? True, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: DT - {'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "        From training? False, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[87 15]\n",
      " [ 6 78]]\n",
      "[[78  6 15 87]\n",
      " [87 15  6 78]]\n",
      "[[165  21]\n",
      " [ 21 165]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 88.70967741935483\n",
      " Precision = [0.83870968 0.93548387]\n",
      " Recall = [0.92857143 0.85294118]\n",
      " F1 score = [0.88135593 0.89230769]\n",
      " AUC score = 89.07563025210085\n",
      " Support = [ 84 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 88.70967741935483\n",
      " Precision = [0.93548387 0.83870968]\n",
      " Recall = [0.85294118 0.92857143]\n",
      " F1 score = [0.89230769 0.88135593]\n",
      " AUC score = 89.07563025210084\n",
      " Support = [102  84]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[87 15]\n",
      " [ 6 78]]\n",
      " Accuracy (acc): 88.71\n",
      " Precision (prc): 83.871\n",
      " Recall (rec): 92.857\n",
      " Sensitivity (sns): 92.857\n",
      " Specificity (spc): 85.294\n",
      " F1 Score (f1s): 88.136\n",
      " ROC AUC (AUC): 0.891\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: DecisionTreeClassifier(max_depth=2) \n",
      "        Best parameters of the model: {'criterion': 'gini', 'max_depth': 2} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
      "\n",
      "                                 model                       model_parameters  \\\n",
      "0  DecisionTreeClassifier(max_depth=2)  {'criterion': 'gini', 'max_depth': 2}   \n",
      "\n",
      "   model_scores     confusion_matrix  accuracy  precision  recall  \\\n",
      "0         89.41  [[87, 15], [6, 78]]     88.71     83.871  92.857   \n",
      "\n",
      "   sensitivity  specificity  f1_score  roc_auc  \n",
      "0       92.857       85.294    88.136    0.891   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:5, TRAINING:2 AND MODEL: RF \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: random_forest_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'criterion', 'splitter', 'max_features', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "        From training? True, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.89411765 0.85882353 0.84558824 0.86985294 0.86985294 0.86985294\n",
      " 0.84558824 0.86985294 0.86985294 0.88235294 0.82352941 0.81029412\n",
      " 0.82279412 0.81102941 0.79926471 0.83529412 0.82279412 0.83455882\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.94332748 0.98208955 0.96733977 1.         1.         1.\n",
      " 1.         1.         1.         0.94034241 0.98208955 0.96733977\n",
      " 0.99705882 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.86108173 0.85209373 0.85006888 0.86301121 0.86301121 0.86301121\n",
      " 0.85006888 0.86301121 0.86301121 0.85270734 0.80421675 0.80822678\n",
      " 0.82536657 0.80159071 0.79961123 0.82106549 0.80994721 0.8176085\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.88608771 0.92576979 0.96143767 0.99854015 1.         1.\n",
      " 1.         1.         1.         0.88455514 0.91553906 0.96145289\n",
      " 0.99703704 1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: RF - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'criterion': ['gini', 'entropy', 'log_loss']} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "\n",
      "        From training? False, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[86 16]\n",
      " [ 5 79]]\n",
      "[[79  5 16 86]\n",
      " [86 16  5 79]]\n",
      "[[165  21]\n",
      " [ 21 165]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 88.70967741935483\n",
      " Precision = [0.83157895 0.94505495]\n",
      " Recall = [0.94047619 0.84313725]\n",
      " F1 score = [0.88268156 0.89119171]\n",
      " AUC score = 89.18067226890757\n",
      " Support = [ 84 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 88.70967741935483\n",
      " Precision = [0.94505495 0.83157895]\n",
      " Recall = [0.84313725 0.94047619]\n",
      " F1 score = [0.89119171 0.88268156]\n",
      " AUC score = 89.18067226890756\n",
      " Support = [102  84]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[86 16]\n",
      " [ 5 79]]\n",
      " Accuracy (acc): 88.71\n",
      " Precision (prc): 83.158\n",
      " Recall (rec): 94.048\n",
      " Sensitivity (sns): 94.048\n",
      " Specificity (spc): 84.314\n",
      " F1 Score (f1s): 88.268\n",
      " ROC AUC (AUC): 0.892\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
      "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: RandomForestClassifier(max_depth=2, n_estimators=3) \n",
      "        Best parameters of the model: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 3} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
      "\n",
      "                                               model  \\\n",
      "0  (DecisionTreeClassifier(max_depth=2, max_featu...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'criterion': 'gini', 'max_depth': 2, 'n_estim...         92.94   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[86, 16], [5, 79]]     88.71     83.158  94.048       94.048       84.314   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    88.268    0.892   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:5, TRAINING:2 AND MODEL: GB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: gradient_boosting_parameters\n",
      "All parameters: ['n_estimators', 'max_depth', 'learning_rate', 'max_features', 'loss', 'min_samples_split', 'min_samples_leaf'], [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.5, 1.0]], [0, 1, 2]\n",
      "Parameters:  {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "        From training? True, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "405 fits failed out of a total of 1215.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 467, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.92941176 0.90588235 0.90588235 0.88235294 0.91764706 0.89411765\n",
      " 0.91764706 0.91764706 0.91764706 0.83529412 0.83529412 0.87058824\n",
      " 0.88235294 0.91764706 0.91764706 0.90588235 0.90588235 0.91764706\n",
      " 0.81176471 0.81029412 0.90588235 0.89411765 0.89411765 0.90588235\n",
      " 0.90588235 0.89411765 0.91764706 0.83308824 0.81029412 0.85882353\n",
      " 0.83455882 0.90588235 0.89411765 0.88235294 0.90588235 0.90588235\n",
      " 0.78823529 0.81029412 0.87058824 0.87058824 0.89411765 0.89411765\n",
      " 0.88235294 0.90588235 0.90588235 0.80882353 0.68014706 0.79852941\n",
      " 0.89338235 0.89411765 0.87058824 0.90588235 0.90588235 0.90588235\n",
      " 0.80882353 0.74926471 0.77573529 0.88235294 0.87058824 0.85882353\n",
      " 0.90588235 0.89411765 0.89411765 0.79852941 0.81176471 0.7875\n",
      " 0.90588235 0.86985294 0.90588235 0.90588235 0.89411765 0.89411765\n",
      " 0.74044118 0.84558824 0.77352941 0.88235294 0.87058824 0.87058824\n",
      " 0.89411765 0.89411765 0.88235294 0.86911765 0.89411765 0.90588235\n",
      " 0.91764706 0.90588235 0.88235294 0.90588235 0.91764706 0.91764706\n",
      " 0.82352941 0.83529412 0.91764706 0.90588235 0.90588235 0.91764706\n",
      " 0.91764706 0.91764706 0.90588235 0.84632353 0.85588235 0.88235294\n",
      " 0.89411765 0.88235294 0.89411765 0.90588235 0.90588235 0.90588235\n",
      " 0.70441176 0.86985294 0.85882353 0.89411765 0.89411765 0.90588235\n",
      " 0.89411765 0.90588235 0.90588235 0.69044118 0.83455882 0.84632353\n",
      " 0.89411765 0.84705882 0.88235294 0.89411765 0.89411765 0.90588235\n",
      " 0.81176471 0.75147059 0.78676471 0.90588235 0.87058824 0.88235294\n",
      " 0.90588235 0.89411765 0.89411765 0.79852941 0.82205882 0.79852941\n",
      " 0.83529412 0.87058824 0.87058824 0.90588235 0.90588235 0.90588235\n",
      " 0.84632353 0.83529412 0.77573529 0.88235294 0.90588235 0.88235294\n",
      " 0.88235294 0.88235294 0.89411765 0.83529412 0.82352941 0.79926471\n",
      " 0.88235294 0.88235294 0.90588235 0.90588235 0.90588235 0.89411765\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.91374012 0.90184372 0.90785777 0.90790167 0.9167252  0.9166813\n",
      " 0.91966637 0.91966637 0.9286216  0.94644425 0.92269535 0.94644425\n",
      " 0.9404302  0.95535558 0.95544337 0.94341528 0.94934153 0.9523266\n",
      " 0.92875329 0.95834065 0.98217735 0.9881036  0.99108867 0.99108867\n",
      " 0.99701493 0.99701493 1.         0.94925373 0.99108867 0.98516242\n",
      " 0.99402985 0.99701493 1.         1.         1.         1.\n",
      " 0.93151888 0.9761633  0.98213345 1.         0.99701493 1.\n",
      " 1.         1.         1.         0.97330992 0.97910448 0.99104478\n",
      " 0.99701493 0.99701493 1.         0.99701493 1.         1.\n",
      " 0.97023705 0.97028095 0.98511853 0.99701493 1.         1.\n",
      " 1.         1.         1.         0.97317823 0.97914838 0.98516242\n",
      " 1.         0.99701493 0.99402985 1.         1.         1.\n",
      " 0.96716418 0.9761633  0.976295   1.         1.         1.\n",
      " 1.         1.         1.         0.89003512 0.8928446  0.9284899\n",
      " 0.9286655  0.9167252  0.9166374  0.9286216  0.91966637 0.9286216\n",
      " 0.92265145 0.93454785 0.964223   0.94938543 0.9405619  0.93156277\n",
      " 0.94350307 0.94938543 0.94938543 0.94934153 0.9643547  0.98516242\n",
      " 0.98516242 0.9881036  0.99701493 1.         1.         1.\n",
      " 0.94060579 0.98511853 0.99407375 0.99705882 1.         1.\n",
      " 1.         1.         1.         0.95834065 0.98507463 0.98213345\n",
      " 0.99402985 1.         1.         1.         1.         1.\n",
      " 0.9643108  0.97014925 0.97919227 1.         1.         1.\n",
      " 1.         1.         1.         0.97019315 0.98213345 0.98516242\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.95531168 0.98507463 0.97919227 0.99701493 1.         1.\n",
      " 1.         1.         1.         0.97322212 0.98208955 0.98213345\n",
      " 0.99402985 0.99701493 1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.85142472 0.87722171 0.87945664 0.86403433 0.88796622 0.87546622\n",
      " 0.88796622 0.88796622 0.88796622 0.83393939 0.82371212 0.8661233\n",
      " 0.86778997 0.8831643  0.88551724 0.87945664 0.87945664 0.88551724\n",
      " 0.81109409 0.81749085 0.87417312 0.8697844  0.87374235 0.87710369\n",
      " 0.87945664 0.86840186 0.8831643  0.82551425 0.82060703 0.84917834\n",
      " 0.83956151 0.863759   0.8706643  0.86695664 0.87710369 0.87945664\n",
      " 0.78512814 0.81125665 0.85534941 0.85622931 0.87241979 0.87584501\n",
      " 0.86380946 0.87484125 0.87710369 0.8099867  0.68617806 0.82499816\n",
      " 0.86976634 0.86840186 0.85396687 0.87484125 0.87945664 0.87710369\n",
      " 0.79859593 0.75985114 0.80054711 0.85434565 0.85622931 0.84904702\n",
      " 0.87710369 0.8706643  0.8706643  0.79195365 0.79220048 0.80477879\n",
      " 0.87279857 0.85357586 0.87484125 0.87710369 0.8706643  0.8706643\n",
      " 0.76276482 0.84352978 0.8049989  0.8661624  0.85622931 0.85317263\n",
      " 0.86840186 0.86840186 0.86380946 0.82601313 0.86733871 0.88637681\n",
      " 0.88225194 0.87945664 0.87009493 0.88190561 0.88551724 0.88796622\n",
      " 0.81388342 0.82278299 0.87945664 0.87945664 0.88190561 0.88551724\n",
      " 0.88796622 0.88551724 0.88190561 0.83247696 0.85333419 0.86940561\n",
      " 0.86514706 0.85936995 0.87546622 0.86913715 0.87484125 0.87484125\n",
      " 0.7347913  0.84013784 0.84948148 0.86302521 0.87339603 0.87139959\n",
      " 0.87301724 0.87484125 0.86913715 0.73368099 0.82597124 0.84633836\n",
      " 0.86234125 0.84399763 0.86556495 0.87584501 0.86733542 0.87710369\n",
      " 0.81377542 0.76163358 0.80501298 0.87945664 0.8601018  0.8661624\n",
      " 0.87945664 0.87301724 0.86840186 0.7810286  0.80775391 0.81938556\n",
      " 0.83629331 0.85548641 0.85396687 0.88190561 0.87710369 0.87484125\n",
      " 0.82618438 0.82791789 0.80193353 0.85431378 0.87139959 0.8569646\n",
      " 0.86460369 0.86460369 0.86840186 0.82805747 0.82379331 0.81735791\n",
      " 0.85896104 0.86460369 0.86844489 0.87484125 0.87710369 0.87301724\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.85605138 0.86600629 0.8680206  0.8658555  0.87193601 0.87168643\n",
      " 0.87246396 0.87249335 0.87594962 0.88848908 0.88595781 0.89124351\n",
      " 0.88916767 0.89590417 0.89470029 0.88843505 0.89391625 0.89790547\n",
      " 0.90747242 0.93262521 0.9382465  0.95465408 0.95339712 0.96003572\n",
      " 0.95366141 0.96437128 0.95912078 0.93948017 0.9682964  0.9736129\n",
      " 0.9853977  0.98827555 0.99121941 0.99119762 0.99411749 0.99705866\n",
      " 0.94393457 0.96911581 0.98803593 0.99705866 0.99407342 0.99854015\n",
      " 1.         1.         1.         0.96623933 0.9707041  0.99250314\n",
      " 0.99411621 0.99559769 1.         0.99849624 1.         1.\n",
      " 0.95901025 0.97035871 0.98653063 0.99701493 0.99411749 1.\n",
      " 1.         1.         1.         0.96907418 0.96934782 0.98956296\n",
      " 0.99413896 0.99555491 0.99699248 0.99851852 1.         1.\n",
      " 0.96836864 0.97477007 0.98498485 0.99705866 1.         0.99851852\n",
      " 1.         1.         1.         0.85049212 0.86011938 0.87709325\n",
      " 0.87597829 0.87297622 0.87176026 0.87827373 0.87246396 0.8771182\n",
      " 0.88467564 0.89360559 0.89925951 0.89882928 0.89056793 0.88339106\n",
      " 0.88850113 0.8903653  0.89165841 0.92513724 0.91627506 0.94240955\n",
      " 0.94777004 0.94933172 0.94718279 0.94884492 0.95391566 0.95675664\n",
      " 0.94060609 0.97800854 0.98969402 0.97968792 0.9926791  0.98975892\n",
      " 0.98398315 0.99265779 0.99557734 0.96540808 0.97364529 0.98801348\n",
      " 0.99261262 0.99854015 1.         1.         1.         1.\n",
      " 0.9656008  0.97158402 0.98651105 0.99411749 0.99851852 1.\n",
      " 1.         1.         1.         0.95616632 0.97916395 0.9881031\n",
      " 0.99705866 0.99703704 1.         1.         1.         1.\n",
      " 0.94690867 0.97800872 0.98794594 0.99115403 1.         0.99854015\n",
      " 1.         1.         1.         0.97618336 0.9763562  0.986532\n",
      " 0.99405131 0.99703639 1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: GB - {'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100], 'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 405 candidates, totalling 2025 fits\n",
      "\n",
      "        From training? False, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[88 14]\n",
      " [ 2 82]]\n",
      "[[82  2 14 88]\n",
      " [88 14  2 82]]\n",
      "[[170  16]\n",
      " [ 16 170]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 91.39784946236558\n",
      " Precision = [0.85416667 0.97777778]\n",
      " Recall = [0.97619048 0.8627451 ]\n",
      " F1 score = [0.91111111 0.91666667]\n",
      " AUC score = 91.94677871148458\n",
      " Support = [ 84 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 91.39784946236558\n",
      " Precision = [0.97777778 0.85416667]\n",
      " Recall = [0.8627451  0.97619048]\n",
      " F1 score = [0.91666667 0.91111111]\n",
      " AUC score = 91.9467787114846\n",
      " Support = [102  84]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[88 14]\n",
      " [ 2 82]]\n",
      " Accuracy (acc): 91.398\n",
      " Precision (prc): 85.417\n",
      " Recall (rec): 97.619\n",
      " Sensitivity (sns): 97.619\n",
      " Specificity (spc): 86.275\n",
      " F1 Score (f1s): 91.111\n",
      " ROC AUC (AUC): 0.919\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
      "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
      "                         'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: GradientBoostingClassifier(learning_rate=0.05, max_depth=2, n_estimators=50) \n",
      "        Best parameters of the model: {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 50} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
      "\n",
      "                                               model  \\\n",
      "0  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
      "\n",
      "                                    model_parameters  model_scores  \\\n",
      "0  {'learning_rate': 0.05, 'max_depth': 2, 'n_est...         94.12   \n",
      "\n",
      "      confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
      "0  [[88, 14], [2, 82]]    91.398     85.417  97.619       97.619       86.275   \n",
      "\n",
      "   f1_score  roc_auc  \n",
      "0    91.111    0.919   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            *** ML MODEL FOR TEST:5, TRAINING:2 AND MODEL: XGB \n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            \n",
      "Calling method: xtreme_gradient_boosting_parameters\n",
      "All parameters: ['max_depth', 'eta', 'max_leaves'], [[2, 3, 6, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]], [0, 1]\n",
      "Parameters:  {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]}\n",
      "run_model_gridSearch- val_ids, train_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "run_model_gridSearch- val_dat, tr_dat ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021'] ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sdb4', 'ins1', 'ins2', 'ins3', 'ins4', 'ins5', 'ins6', 'ins7', 'ins8', 'ins9', 'ins1101', 'ins1102', 'ins1103', 'ins1104', 'ins1105', 'ins1106', 'ins1107', 'narco1', 'narco2', 'narco3', 'narco4', 'nfle1', 'nfle2', 'nfle3', 'nfle4', 'nfle5', 'nfle6', 'nfle7', 'nfle8', 'nfle9', 'nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15', 'nfle16', 'nfle17', 'nfle18', 'nfle19', 'nfle20', 'nfle21', 'nfle22', 'nfle23', 'nfle24', 'nfle25', 'nfle26', 'nfle27', 'nfle28', 'nfle29', 'nfle30', 'nfle31', 'nfle32', 'plm1', 'plm2', 'plm3', 'plm4', 'plm5', 'plm6', 'plm7', 'plm8', 'rbd1', 'rbd2', 'rbd3', 'rbd4', 'rbd5', 'rbd6', 'rbd7', 'rbd8', 'rbd9', 'rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd14', 'rbd15', 'rbd16', 'rbd21', 'rbd22', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', 'n15', 'n16', 'n1101', 'n1102', 'n1103', 'n1104', 'n1110', 'n1111', 'SC4001', 'SC4011', 'SC4021', 'SC4031', 'SC4041', 'SC4051', 'SC4061', 'SC4071', 'SC4081', 'SC4091', 'SC4101', 'SC4111', 'SC4121', 'SC4131', 'SC4141', 'SC4151', 'SC4161', 'SC4171', 'SC4181', 'SC4191', 'SC4201', 'SC4211', 'SC4221', 'SC4231', 'SC4241', 'SC4251', 'SC4261', 'SC4271', 'SC4281', 'SC4291', 'SC4301', 'SC4311', 'SC4321', 'SC4331', 'SC4341', 'SC4351', 'SC4362', 'SC4371', 'SC4381', 'SC4401', 'SC4411', 'SC4421', 'SC4431', 'SC4441', 'SC4451', 'SC4461', 'SC4471', 'SC4481', 'SC4491', 'SC4501', 'SC4511', 'SC4522', 'SC4531', 'SC4541', 'SC4551', 'SC4561', 'SC4571', 'SC4581', 'SC4591', 'SC4601', 'SC4611', 'SC4621', 'SC4631', 'SC4641', 'SC4651', 'SC4661', 'SC4671', 'SC4701', 'SC4711', 'SC4721', 'SC4731', 'SC4741', 'SC4751', 'SC4761', 'SC4771', 'SC4801', 'SC4811', 'SC4821', 'ST7011', 'ST7021']\n",
      "\n",
      "        From training? True, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "\n",
      "        Here to calculate feature importance using AUC\n",
      "        \n",
      "\n",
      "===========================================\n",
      "\n",
      "\n",
      "        GridSearch: XGB - {'max_depth': [2, 3, 6, 10, 15, 20, 25, 30], 'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]} \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "Refitting the model with best parameter ['recall', 'f1'] == recall\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "\n",
      "        From training? False, Data shape: (186, 18), Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (186, 14), Target shape: (186,), Metadata: (186, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[89 13]\n",
      " [ 1 83]]\n",
      "[[83  1 13 89]\n",
      " [89 13  1 83]]\n",
      "[[172  14]\n",
      " [ 14 172]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 92.47311827956989\n",
      " Precision = [0.86458333 0.98888889]\n",
      " Recall = [0.98809524 0.87254902]\n",
      " F1 score = [0.92222222 0.92708333]\n",
      " AUC score = 93.03221288515407\n",
      " Support = [ 84 102]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 92.47311827956989\n",
      " Precision = [0.98888889 0.86458333]\n",
      " Recall = [0.87254902 0.98809524]\n",
      " F1 score = [0.92708333 0.92222222]\n",
      " AUC score = 93.03221288515407\n",
      " Support = [102  84]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[89 13]\n",
      " [ 1 83]]\n",
      " Accuracy (acc): 92.473\n",
      " Precision (prc): 86.458\n",
      " Recall (rec): 98.81\n",
      " Sensitivity (sns): 98.81\n",
      " Specificity (spc): 87.255\n",
      " F1 Score (f1s): 92.222\n",
      " ROC AUC (AUC): 0.93\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "        Best model (GriveSearchCV): GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best model: GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_bin=None,\n",
      "                                     max_ca...\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, reg_alpha=None,\n",
      "                                     reg_lambda=None, ...),\n",
      "             n_jobs=50,\n",
      "             param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
      "                         'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
      "             refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
      "             verbose=2) \n",
      "        Best estimator of the model: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False, eta=0.05,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.0500000007, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=2, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, ...) \n",
      "        Best parameters of the model: {'eta': 0.05, 'max_depth': 2} \n",
      "        Best model scores:                                               method  \\\n",
      "0  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
      "\n",
      "                                               model  \\\n",
      "0  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
      "\n",
      "                model_parameters  model_scores     confusion_matrix  accuracy  \\\n",
      "0  {'eta': 0.05, 'max_depth': 2}         90.59  [[89, 13], [1, 83]]    92.473   \n",
      "\n",
      "   precision  recall  sensitivity  specificity  f1_score  roc_auc  \n",
      "0     86.458   98.81        98.81       87.255    92.222     0.93   \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "            Best model index calculation  \n",
      "            \n",
      "\n",
      "            ***************************************************************************************************\n",
      "            TRAINING 2 END... \n",
      "            \n",
      "\n",
      "        ### MODEL EVALUATION PHASE \n",
      "        EVALUATION 5 START... XXXXX \n",
      "        ---------------------------------------------------------------------------------------------------\n",
      "        \n",
      "\n",
      "        From training? False, Data shape: (44, 18), Indices: [22, 23, 24, 25, 30, 63, 64, 65, 66, 67, 68, 69, 70, 79, 80, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 97, 98, 99, 100, 123, 124, 125, 126, 127]\n",
      "        All Columns: ['Dataset', 'Category', 'Subject_Name', 'Class', 'W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n",
      "        \n",
      "\n",
      "        Feature shape: (44, 14), Target shape: (44,), Metadata: (44, 3) \n",
      "        \n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[[24  1]\n",
      " [ 4 15]]\n",
      "[[15  4  1 24]\n",
      " [24  1  4 15]]\n",
      "[[39  5]\n",
      " [ 5 39]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 88.63636363636364\n",
      " Precision = [0.9375     0.85714286]\n",
      " Recall = [0.78947368 0.96      ]\n",
      " F1 score = [0.85714286 0.90566038]\n",
      " AUC score = 87.47368421052632\n",
      " Support = [19 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 88.63636363636364\n",
      " Precision = [0.85714286 0.9375    ]\n",
      " Recall = [0.96       0.78947368]\n",
      " F1 score = [0.90566038 0.85714286]\n",
      " AUC score = 87.47368421052632\n",
      " Support = [25 19]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[24  1]\n",
      " [ 4 15]]\n",
      " Accuracy (acc): 88.636\n",
      " Precision (prc): 93.75\n",
      " Recall (rec): 78.947\n",
      " Sensitivity (sns): 78.947\n",
      " Specificity (spc): 96.0\n",
      " F1 Score (f1s): 85.714\n",
      " ROC AUC (AUC): 0.875\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[[23  2]\n",
      " [ 4 15]]\n",
      "[[15  4  2 23]\n",
      " [23  2  4 15]]\n",
      "[[38  6]\n",
      " [ 6 38]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 86.36363636363636\n",
      " Precision = [0.88235294 0.85185185]\n",
      " Recall = [0.78947368 0.92      ]\n",
      " F1 score = [0.83333333 0.88461538]\n",
      " AUC score = 85.47368421052633\n",
      " Support = [19 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 86.36363636363636\n",
      " Precision = [0.85185185 0.88235294]\n",
      " Recall = [0.92       0.78947368]\n",
      " F1 score = [0.88461538 0.83333333]\n",
      " AUC score = 85.47368421052633\n",
      " Support = [25 19]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[23  2]\n",
      " [ 4 15]]\n",
      " Accuracy (acc): 86.364\n",
      " Precision (prc): 88.235\n",
      " Recall (rec): 78.947\n",
      " Sensitivity (sns): 78.947\n",
      " Specificity (spc): 92.0\n",
      " F1 Score (f1s): 83.333\n",
      " ROC AUC (AUC): 0.855\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0]\n",
      "[[22  3]\n",
      " [ 4 15]]\n",
      "[[15  4  3 22]\n",
      " [22  3  4 15]]\n",
      "[[37  7]\n",
      " [ 7 37]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 84.0909090909091\n",
      " Precision = [0.83333333 0.84615385]\n",
      " Recall = [0.78947368 0.88      ]\n",
      " F1 score = [0.81081081 0.8627451 ]\n",
      " AUC score = 83.47368421052632\n",
      " Support = [19 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 84.0909090909091\n",
      " Precision = [0.84615385 0.83333333]\n",
      " Recall = [0.88       0.78947368]\n",
      " F1 score = [0.8627451  0.81081081]\n",
      " AUC score = 83.47368421052633\n",
      " Support = [25 19]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[22  3]\n",
      " [ 4 15]]\n",
      " Accuracy (acc): 84.091\n",
      " Precision (prc): 83.333\n",
      " Recall (rec): 78.947\n",
      " Sensitivity (sns): 78.947\n",
      " Specificity (spc): 88.0\n",
      " F1 Score (f1s): 81.081\n",
      " ROC AUC (AUC): 0.835\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[[22  3]\n",
      " [ 4 15]]\n",
      "[[15  4  3 22]\n",
      " [22  3  4 15]]\n",
      "[[37  7]\n",
      " [ 7 37]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 84.0909090909091\n",
      " Precision = [0.83333333 0.84615385]\n",
      " Recall = [0.78947368 0.88      ]\n",
      " F1 score = [0.81081081 0.8627451 ]\n",
      " AUC score = 83.47368421052632\n",
      " Support = [19 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 84.0909090909091\n",
      " Precision = [0.84615385 0.83333333]\n",
      " Recall = [0.88       0.78947368]\n",
      " F1 score = [0.8627451  0.81081081]\n",
      " AUC score = 83.47368421052633\n",
      " Support = [25 19]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[22  3]\n",
      " [ 4 15]]\n",
      " Accuracy (acc): 84.091\n",
      " Precision (prc): 83.333\n",
      " Recall (rec): 78.947\n",
      " Sensitivity (sns): 78.947\n",
      " Specificity (spc): 88.0\n",
      " F1 Score (f1s): 81.081\n",
      " ROC AUC (AUC): 0.835\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n",
      "[[25  0]\n",
      " [ 6 13]]\n",
      "[[13  6  0 25]\n",
      " [25  0  6 13]]\n",
      "[[38  6]\n",
      " [ 6 38]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 86.36363636363636\n",
      " Precision = [1.         0.80645161]\n",
      " Recall = [0.68421053 1.        ]\n",
      " F1 score = [0.8125     0.89285714]\n",
      " AUC score = 84.21052631578947\n",
      " Support = [19 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 86.36363636363636\n",
      " Precision = [0.80645161 1.        ]\n",
      " Recall = [1.         0.68421053]\n",
      " F1 score = [0.89285714 0.8125    ]\n",
      " AUC score = 84.21052631578947\n",
      " Support = [25 19]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[25  0]\n",
      " [ 6 13]]\n",
      " Accuracy (acc): 86.364\n",
      " Precision (prc): 100.0\n",
      " Recall (rec): 68.421\n",
      " Sensitivity (sns): 68.421\n",
      " Specificity (spc): 100.0\n",
      " F1 Score (f1s): 81.25\n",
      " ROC AUC (AUC): 0.842\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0]\n",
      "[[23  2]\n",
      " [ 3 16]]\n",
      "[[16  3  2 23]\n",
      " [23  2  3 16]]\n",
      "[[39  5]\n",
      " [ 5 39]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 88.63636363636364\n",
      " Precision = [0.88888889 0.88461538]\n",
      " Recall = [0.84210526 0.92      ]\n",
      " F1 score = [0.86486486 0.90196078]\n",
      " AUC score = 88.10526315789473\n",
      " Support = [19 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 88.63636363636364\n",
      " Precision = [0.88461538 0.88888889]\n",
      " Recall = [0.92       0.84210526]\n",
      " F1 score = [0.90196078 0.86486486]\n",
      " AUC score = 88.10526315789473\n",
      " Support = [25 19]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[23  2]\n",
      " [ 3 16]]\n",
      " Accuracy (acc): 88.636\n",
      " Precision (prc): 88.889\n",
      " Recall (rec): 84.211\n",
      " Sensitivity (sns): 84.211\n",
      " Specificity (spc): 92.0\n",
      " F1 Score (f1s): 86.486\n",
      " ROC AUC (AUC): 0.881\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n",
      "[[25  0]\n",
      " [ 6 13]]\n",
      "[[13  6  0 25]\n",
      " [25  0  6 13]]\n",
      "[[38  6]\n",
      " [ 6 38]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 86.36363636363636\n",
      " Precision = [1.         0.80645161]\n",
      " Recall = [0.68421053 1.        ]\n",
      " F1 score = [0.8125     0.89285714]\n",
      " AUC score = 84.21052631578947\n",
      " Support = [19 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 86.36363636363636\n",
      " Precision = [0.80645161 1.        ]\n",
      " Recall = [1.         0.68421053]\n",
      " F1 score = [0.89285714 0.8125    ]\n",
      " AUC score = 84.21052631578947\n",
      " Support = [25 19]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[25  0]\n",
      " [ 6 13]]\n",
      " Accuracy (acc): 86.364\n",
      " Precision (prc): 100.0\n",
      " Recall (rec): 68.421\n",
      " Sensitivity (sns): 68.421\n",
      " Specificity (spc): 100.0\n",
      " F1 Score (f1s): 81.25\n",
      " ROC AUC (AUC): 0.842\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[[25  0]\n",
      " [ 3 16]]\n",
      "[[16  3  0 25]\n",
      " [25  0  3 16]]\n",
      "[[41  3]\n",
      " [ 3 41]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 93.18181818181817\n",
      " Precision = [1.         0.89285714]\n",
      " Recall = [0.84210526 1.        ]\n",
      " F1 score = [0.91428571 0.94339623]\n",
      " AUC score = 92.10526315789474\n",
      " Support = [19 25]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 93.18181818181817\n",
      " Precision = [0.89285714 1.        ]\n",
      " Recall = [1.         0.84210526]\n",
      " F1 score = [0.94339623 0.91428571]\n",
      " AUC score = 92.10526315789474\n",
      " Support = [25 19]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[25  0]\n",
      " [ 3 16]]\n",
      " Accuracy (acc): 93.182\n",
      " Precision (prc): 100.0\n",
      " Recall (rec): 84.211\n",
      " Sensitivity (sns): 84.211\n",
      " Specificity (spc): 100.0\n",
      " F1 Score (f1s): 91.429\n",
      " ROC AUC (AUC): 0.921\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "\n",
      "            ===================================================================================================\n",
      "            TEST 5 END...\n",
      "            \n",
      "Directory successfully created at path: ./Results//_Classification/ML11002//Models/\n",
      "Start saving model to file...\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_1_1.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_1_2.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_1_3.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_1_4.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_1_5.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_1_6.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_1_7.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_1_8.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_2_1.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_2_2.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_2_3.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_2_4.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_2_5.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_2_6.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_2_7.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_2_8.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_3_1.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_3_2.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_3_3.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_3_4.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_3_5.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_3_6.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_3_7.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_3_8.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_4_1.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_4_2.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_4_3.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_4_4.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_4_5.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_4_6.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_4_7.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_4_8.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_5_1.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_5_2.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_5_3.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_5_4.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_5_5.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_5_6.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_5_7.dat\n",
      "\n",
      "Test Models is written to the file: ./Results//_Classification/ML11002//Models/ts_model_5_8.dat\n",
      "\n",
      "Finish saving model to file...\n",
      "Start saving model to file...\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_1_1_1.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_1_1_2.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_1_1_3.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_1_1_4.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_1_1_5.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_1_1_6.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_1_1_7.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_1_1_8.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_1_2_1.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_1_2_2.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_1_2_3.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_1_2_4.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_1_2_5.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_1_2_6.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_1_2_7.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_1_2_8.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_2_1_1.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_2_1_2.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_2_1_3.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_2_1_4.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_2_1_5.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_2_1_6.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_2_1_7.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_2_1_8.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_2_2_1.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_2_2_2.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_2_2_3.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_2_2_4.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_2_2_5.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_2_2_6.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_2_2_7.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_2_2_8.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_3_1_1.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_3_1_2.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_3_1_3.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_3_1_4.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_3_1_5.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_3_1_6.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_3_1_7.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_3_1_8.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_3_2_1.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_3_2_2.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_3_2_3.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_3_2_4.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_3_2_5.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_3_2_6.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_3_2_7.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_3_2_8.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_4_1_1.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_4_1_2.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_4_1_3.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_4_1_4.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_4_1_5.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_4_1_6.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_4_1_7.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_4_1_8.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_4_2_1.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_4_2_2.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_4_2_3.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_4_2_4.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_4_2_5.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_4_2_6.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_4_2_7.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_4_2_8.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_5_1_1.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_5_1_2.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_5_1_3.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_5_1_4.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_5_1_5.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_5_1_6.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_5_1_7.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_5_1_8.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_5_2_1.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_5_2_2.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_5_2_3.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_5_2_4.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_5_2_5.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_5_2_6.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_5_2_7.dat\n",
      "\n",
      "Training Models is written to the file: ./Results//_Classification/ML11002//Models/tr_model_5_2_8.dat\n",
      "\n",
      "Finish saving model to file...\n",
      "Start saving model to file...\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_1_1.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_1_2.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_1_3.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_1_4.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_1_5.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_1_6.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_1_7.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_1_8.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_2_1.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_2_2.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_2_3.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_2_4.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_2_5.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_2_6.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_2_7.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_2_8.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_3_1.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_3_2.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_3_3.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_3_4.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_3_5.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_3_6.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_3_7.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_3_8.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_4_1.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_4_2.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_4_3.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_4_4.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_4_5.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_4_6.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_4_7.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_4_8.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_5_1.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_5_2.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_5_3.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_5_4.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_5_5.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_5_6.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_5_7.dat\n",
      "\n",
      "Best Training Models is written to the file: ./Results//_Classification/ML11002//Models/best_tr_model_5_8.dat\n",
      "\n",
      "Finish saving model to file...\n",
      "ML11002 ['ML0011', 'ML0012', 'ML0013', 'ML10001', 'ML10002', 'ML10003', 'ML10004', 'ML10005', 'ML10006', 'ML10007', 'ML10008', 'ML1001', 'ML1002', 'ML1003', 'ML1004', 'ML1005', 'ML1006', 'ML1007', 'ML1008', 'ML1009', 'ML1010', 'ML11001', 'ML1101', 'ML1102', 'ML1103', 'ML1201', 'ML1202', 'ML1203', 'ML13001', 'ML13002', 'ML13003', 'ML13004', 'ML1501', 'ML1502', 'ML1503', 'ML2001', 'ML2002', 'ML2003', 'ML2501', 'ML2502', 'ML2503', 'ML3001', 'ML3002', 'ML3003', 'ML3004', 'ML3005', 'ML3006', 'ML3007', 'ML3008', 'ML3009', 'ML3010', 'ML3011', 'ML3012', 'ML3013', 'ML3014', 'ML3015', 'ML3016', 'ML3017', 'ML3018', 'ML3019', 'ML3020', 'ML3021', 'ML3022', 'ML3023', 'ML3101', 'ML3102', 'ML3103', 'ML3111', 'ML3112', 'ML3113', 'ML3151', 'ML3152', 'ML3153', 'ML3201', 'ML3202', 'ML3203', 'ML3211', 'ML3212', 'ML3213', 'ML3251', 'ML3252', 'ML3253', 'ML4001', 'ML4002', 'ML4003', 'ML4004', 'ML4005', 'ML4006', 'ML4007', 'ML4008', 'ML4009', 'ML4010', 'ML4011', 'ML4012', 'ML4013', 'ML4014', 'ML4015', 'ML4016', 'ML4017', 'ML4018', 'ML4019', 'ML4020', 'ML4021', 'ML4022', 'ML6001', 'ML6002', 'ML6003', 'ML6004', 'ML6005', 'ML6006', 'ML6007', 'ML6008', 'ML6009']\n",
      "Data is successfully inserted...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_name</th>\n",
       "      <th>stp_from</th>\n",
       "      <th>exp_description</th>\n",
       "      <th>datasets</th>\n",
       "      <th>feature_selection</th>\n",
       "      <th>special_consideration</th>\n",
       "      <th>classification_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ML0011</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>2 datasets- CAP_Sleep, Sleep_EDFX</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, subject balanced over the fol...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ML0012</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>2 datasets- CAP_Sleep, Sleep_EDFX</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, subject balanced over the fol...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ML0013</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>2 datasets- CAP_Sleep, Sleep_EDFX</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, subject balanced over the fol...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ML10001</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>2 datasets- CAP and EDFX</td>\n",
       "      <td>pre selection P-value&lt;0.05</td>\n",
       "      <td>Remove all zero transition and W-&gt;W stage, sub...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ML10002</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>2 datasets- CAP and EDFX</td>\n",
       "      <td>pre selection AUC&gt;0.5</td>\n",
       "      <td>Remove all zero transition and W-&gt;W stage, sub...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>ML6005</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>3 datasets- All</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, AUC&gt;0.5, subject balanced ove...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>ML6006</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>3 datasets- All</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, AUC&gt;0.6, subject balanced ove...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>ML6007</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>3 datasets- All</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, pval&lt;=0.05, subject balanced ...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>ML6008</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>3 datasets- All</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, AUC&gt;0.5, subject balanced ove...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>ML6009</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>3 datasets- All</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, AUC&gt;0.6, subject balanced ove...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    exp_name stp_from                                    exp_description  \\\n",
       "0     ML0011     Same  Data classification using sleep transition mat...   \n",
       "1     ML0012     Same  Data classification using sleep transition mat...   \n",
       "2     ML0013     Same  Data classification using sleep transition mat...   \n",
       "3    ML10001     Same  Data classification using sleep transition mat...   \n",
       "4    ML10002     Same  Data classification using sleep transition mat...   \n",
       "..       ...      ...                                                ...   \n",
       "109   ML6005     Same  Data classification using sleep transition mat...   \n",
       "110   ML6006     Same  Data classification using sleep transition mat...   \n",
       "111   ML6007     Same  Data classification using sleep transition mat...   \n",
       "112   ML6008     Same  Data classification using sleep transition mat...   \n",
       "113   ML6009     Same  Data classification using sleep transition mat...   \n",
       "\n",
       "                              datasets           feature_selection  \\\n",
       "0    2 datasets- CAP_Sleep, Sleep_EDFX        No feature selection   \n",
       "1    2 datasets- CAP_Sleep, Sleep_EDFX        No feature selection   \n",
       "2    2 datasets- CAP_Sleep, Sleep_EDFX        No feature selection   \n",
       "3             2 datasets- CAP and EDFX  pre selection P-value<0.05   \n",
       "4             2 datasets- CAP and EDFX       pre selection AUC>0.5   \n",
       "..                                 ...                         ...   \n",
       "109                    3 datasets- All        No feature selection   \n",
       "110                    3 datasets- All        No feature selection   \n",
       "111                    3 datasets- All        No feature selection   \n",
       "112                    3 datasets- All        No feature selection   \n",
       "113                    3 datasets- All        No feature selection   \n",
       "\n",
       "                                 special_consideration  \\\n",
       "0    Remove all zero, subject balanced over the fol...   \n",
       "1    Remove all zero, subject balanced over the fol...   \n",
       "2    Remove all zero, subject balanced over the fol...   \n",
       "3    Remove all zero transition and W->W stage, sub...   \n",
       "4    Remove all zero transition and W->W stage, sub...   \n",
       "..                                                 ...   \n",
       "109  Remove all zero, AUC>0.5, subject balanced ove...   \n",
       "110  Remove all zero, AUC>0.6, subject balanced ove...   \n",
       "111  Remove all zero, pval<=0.05, subject balanced ...   \n",
       "112  Remove all zero, AUC>0.5, subject balanced ove...   \n",
       "113  Remove all zero, AUC>0.6, subject balanced ove...   \n",
       "\n",
       "                              classification_type  \n",
       "0    Binary classification- Healthy vs disordered  \n",
       "1    Binary classification- Healthy vs disordered  \n",
       "2    Binary classification- Healthy vs disordered  \n",
       "3    Binary classification- Healthy vs disordered  \n",
       "4    Binary classification- Healthy vs disordered  \n",
       "..                                            ...  \n",
       "109  Binary classification- Healthy vs disordered  \n",
       "110  Binary classification- Healthy vs disordered  \n",
       "111  Binary classification- Healthy vs disordered  \n",
       "112  Binary classification- Healthy vs disordered  \n",
       "113  Binary classification- Healthy vs disordered  \n",
       "\n",
       "[114 rows x 7 columns]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Set the classifier parameters in the \"HumachLab_ML_CLassifiers\" class file to run with the parameter \n",
    "# best_tr_model, tr_model, tr_model_scores_df, tr_target_and_prediction_df, ts_model, ts_model_scores_df, ts_target_and_prediction_df, ts_fold_info_df, exp_info_df\n",
    "best_tr_model, tr_model, tr_model_scores_df, tr_target_and_prediction_df, ts_model, ts_model_scores_df, ts_target_and_prediction_df, ts_fold_info_df, exp_info_df = classifier_obj.classify(\n",
    "    should_use_params=should_use_params, splitting_crieteria=splitting_crieteria, model_list=model_list, is_validate_models=is_validate_models, \n",
    "    result_save_path=result_save_path, exp_name=exp_name, exp_detail=exp_detail, apply_feature_selection=apply_feature_selection, custom_splitter=custom_splitter) \n",
    "\n",
    "stop_logger(logger) \n",
    "\n",
    "exp_sum_df = modify_experiment_information_summarry(result_directory, dict_dat=exp_detail) \n",
    "exp_sum_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ff410a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1633,
   "id": "83bc3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_tr_model, tr_model, tr_model_scores_df, tr_target_and_prediction_df, ts_model, ts_model_scores_df, ts_target_and_prediction_df, ts_fold_info_df, exp_info_df\n",
    "# all_best_tr_model, all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_ts_model, all_ts_scores_df, all_ts_prediction_df, all_ts_fold_info_df, all_exp_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1645,
   "id": "c12bf92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_model \n",
    "# best_tr_model\n",
    "# tr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len(ts_fold_info_df[:1]['Test'].values[0]), ts_fold_info_df[:1]['Test'].values[0] ) \n",
    "print( len(ts_fold_info_df[:1]['Validation'].values[0]), ts_fold_info_df[:1]['Validation'].values[0] ) \n",
    "print( len(ts_fold_info_df[:1]['Training'].values[0]), ts_fold_info_df[:1]['Training'].values[0] ) \n",
    "21+20+167 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb26d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_fold_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d741c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0117d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "cc5ba0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_No</th>\n",
       "      <th>Model_No</th>\n",
       "      <th>method</th>\n",
       "      <th>model</th>\n",
       "      <th>model_parameters</th>\n",
       "      <th>model_scores</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=5, max_featu...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'n_estim...</td>\n",
       "      <td>90.59</td>\n",
       "      <td>[[0, 6], [0, 22]]</td>\n",
       "      <td>78.571</td>\n",
       "      <td>78.571</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=3, max_featu...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 3, 'n_estim...</td>\n",
       "      <td>91.62</td>\n",
       "      <td>[[0, 6], [0, 22]]</td>\n",
       "      <td>78.571</td>\n",
       "      <td>78.571</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>(DecisionTreeClassifier(criterion='entropy', m...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3, 'n_es...</td>\n",
       "      <td>95.29</td>\n",
       "      <td>[[0, 5], [2, 18]]</td>\n",
       "      <td>72.000</td>\n",
       "      <td>78.261</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.721</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>(DecisionTreeClassifier(criterion='entropy', m...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7, 'n_es...</td>\n",
       "      <td>96.47</td>\n",
       "      <td>[[2, 3], [0, 20]]</td>\n",
       "      <td>88.000</td>\n",
       "      <td>86.957</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>93.023</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>(DecisionTreeClassifier(criterion='entropy', m...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 2, 'n_es...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[1, 4], [0, 19]]</td>\n",
       "      <td>83.333</td>\n",
       "      <td>82.609</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>90.476</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Test_No  Model_No                                             method  \\\n",
       "5         1         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "13        2         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "21        3         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "29        4         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "37        5         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "\n",
       "                                                model  \\\n",
       "5   (DecisionTreeClassifier(max_depth=5, max_featu...   \n",
       "13  (DecisionTreeClassifier(max_depth=3, max_featu...   \n",
       "21  (DecisionTreeClassifier(criterion='entropy', m...   \n",
       "29  (DecisionTreeClassifier(criterion='entropy', m...   \n",
       "37  (DecisionTreeClassifier(criterion='entropy', m...   \n",
       "\n",
       "                                     model_parameters  model_scores  \\\n",
       "5   {'criterion': 'gini', 'max_depth': 5, 'n_estim...         90.59   \n",
       "13  {'criterion': 'gini', 'max_depth': 3, 'n_estim...         91.62   \n",
       "21  {'criterion': 'entropy', 'max_depth': 3, 'n_es...         95.29   \n",
       "29  {'criterion': 'entropy', 'max_depth': 7, 'n_es...         96.47   \n",
       "37  {'criterion': 'entropy', 'max_depth': 2, 'n_es...        100.00   \n",
       "\n",
       "     confusion_matrix  accuracy  precision  recall  sensitivity  specificity  \\\n",
       "5   [[0, 6], [0, 22]]    78.571     78.571   100.0        100.0          0.0   \n",
       "13  [[0, 6], [0, 22]]    78.571     78.571   100.0        100.0          0.0   \n",
       "21  [[0, 5], [2, 18]]    72.000     78.261    90.0         90.0          0.0   \n",
       "29  [[2, 3], [0, 20]]    88.000     86.957   100.0        100.0         40.0   \n",
       "37  [[1, 4], [0, 19]]    83.333     82.609   100.0        100.0         20.0   \n",
       "\n",
       "    f1_score  roc_auc  \n",
       "5     88.000     0.50  \n",
       "13    88.000     0.50  \n",
       "21    83.721     0.45  \n",
       "29    93.023     0.70  \n",
       "37    90.476     0.60  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_model_scores_df[(ts_model_scores_df['Model_No']==6)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "18a359c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_No</th>\n",
       "      <th>Model_No</th>\n",
       "      <th>method</th>\n",
       "      <th>model</th>\n",
       "      <th>model_parameters</th>\n",
       "      <th>model_scores</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=XGB...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'eta': 0.05, 'max_depth': 6}</td>\n",
       "      <td>88.09</td>\n",
       "      <td>[[1, 5], [1, 21]]</td>\n",
       "      <td>78.571</td>\n",
       "      <td>80.769</td>\n",
       "      <td>95.455</td>\n",
       "      <td>95.455</td>\n",
       "      <td>16.667</td>\n",
       "      <td>87.500</td>\n",
       "      <td>0.561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=XGB...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'eta': 0.01, 'max_depth': 2}</td>\n",
       "      <td>90.44</td>\n",
       "      <td>[[1, 5], [0, 22]]</td>\n",
       "      <td>82.143</td>\n",
       "      <td>81.481</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>16.667</td>\n",
       "      <td>89.796</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=XGB...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'eta': 0.01, 'max_depth': 2}</td>\n",
       "      <td>95.29</td>\n",
       "      <td>[[0, 5], [1, 19]]</td>\n",
       "      <td>76.000</td>\n",
       "      <td>79.167</td>\n",
       "      <td>95.000</td>\n",
       "      <td>95.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>86.364</td>\n",
       "      <td>0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=XGB...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'eta': 0.1, 'max_depth': 3}</td>\n",
       "      <td>91.62</td>\n",
       "      <td>[[3, 2], [1, 19]]</td>\n",
       "      <td>88.000</td>\n",
       "      <td>90.476</td>\n",
       "      <td>95.000</td>\n",
       "      <td>95.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>92.683</td>\n",
       "      <td>0.775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=XGB...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'eta': 0.01, 'max_depth': 2}</td>\n",
       "      <td>95.29</td>\n",
       "      <td>[[1, 4], [2, 17]]</td>\n",
       "      <td>75.000</td>\n",
       "      <td>80.952</td>\n",
       "      <td>89.474</td>\n",
       "      <td>89.474</td>\n",
       "      <td>20.000</td>\n",
       "      <td>85.000</td>\n",
       "      <td>0.547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Test_No  Model_No                                             method  \\\n",
       "7         1         8  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
       "15        2         8  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
       "23        3         8  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
       "31        4         8  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
       "39        5         8  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
       "\n",
       "                                                model  \\\n",
       "7   XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "15  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "23  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "31  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "39  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "\n",
       "                 model_parameters  model_scores   confusion_matrix  accuracy  \\\n",
       "7   {'eta': 0.05, 'max_depth': 6}         88.09  [[1, 5], [1, 21]]    78.571   \n",
       "15  {'eta': 0.01, 'max_depth': 2}         90.44  [[1, 5], [0, 22]]    82.143   \n",
       "23  {'eta': 0.01, 'max_depth': 2}         95.29  [[0, 5], [1, 19]]    76.000   \n",
       "31   {'eta': 0.1, 'max_depth': 3}         91.62  [[3, 2], [1, 19]]    88.000   \n",
       "39  {'eta': 0.01, 'max_depth': 2}         95.29  [[1, 4], [2, 17]]    75.000   \n",
       "\n",
       "    precision   recall  sensitivity  specificity  f1_score  roc_auc  \n",
       "7      80.769   95.455       95.455       16.667    87.500    0.561  \n",
       "15     81.481  100.000      100.000       16.667    89.796    0.583  \n",
       "23     79.167   95.000       95.000        0.000    86.364    0.475  \n",
       "31     90.476   95.000       95.000       60.000    92.683    0.775  \n",
       "39     80.952   89.474       89.474       20.000    85.000    0.547  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_model_scores_df[(ts_model_scores_df['Model_No']==8)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "id": "c0c95b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_model_scores_df[(ts_model_scores_df['Model_No']==6)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "id": "54195d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_model_scores_df[(ts_model_scores_df['Model_No']==8)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1 = ts_model[1][6].estimator \n",
    "rf1 = ts_model[1][6] \n",
    "rf1.feature_importances_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c7a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f578d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9db0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model_scores_df['method_name'][-5:].values, ts_model_scores_df['method_name'][-5:].values[0], type(ts_model_scores_df['method_name'][-5:].values[0]) \n",
    "ts_model_scores_df[-10:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11e50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_model_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3c8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_target_and_prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f468482",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_column, class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8f0049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe based on the list as the column values\n",
    "sorter = processed_dataset['Subject_Name'].values.tolist()\n",
    "sorter\n",
    "\n",
    "ts_target_and_prediction_df['Subject_Name'] = ts_target_and_prediction_df['Subject_Name'].astype(\"category\")\n",
    "ts_target_and_prediction_df['Subject_Name'] = ts_target_and_prediction_df['Subject_Name'].cat.set_categories(sorter)\n",
    "ts_target_and_prediction_df\n",
    "ts_target_and_prediction_df2 = ts_target_and_prediction_df.sort_values(['Subject_Name'])\n",
    "ts_target_and_prediction_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec8604",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_target_and_prediction_df2[(ts_target_and_prediction_df2['Subject_Name'].str.match(r'^n\\d')==True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d29df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_target_and_prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe0152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1c83ea4",
   "metadata": {},
   "source": [
    "### Test saved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "id": "57161b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Results/_Classification/ML3002/'"
      ]
     },
     "execution_count": 1053,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exp_name = 'ML3002' \n",
    "# result_save_path = f'./Results/_Classification/{exp_name}/'\n",
    "# result_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "id": "68645e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if logger:\n",
    "#     stop_logger(logger) \n",
    "# util, logger = start_logger(result_save_path, exp_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5d0f2e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Results/_Classification/ML6002/'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result_save_path2 = result_save_path \n",
    "result_save_path2 = f'./Results/_Classification/{exp_name}/'\n",
    "result_save_path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d90ce5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Class'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ebd98480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Object is initialised with the following properties: \n",
      "        ###################################################################################################\n",
      "        Dataset size: (0, 1), Columns: ['Class']\n",
      "        Target class column name: Class\n",
      "        Metadata column names: ['Dataset', 'Category', 'Subject_Name']\n",
      "        Dataset split column on which the training and test sets will be devided: Subject_Name\n",
      "        Is multi-class classification: False\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.HumachLab_ML_CLassifiers at 0x1ca4be7c4c8>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if logger:\n",
    "    stop_logger(logger) \n",
    "util, logger = start_logger('./Results/', '00')\n",
    "\n",
    "classifier_obj2 = HumachLab_ML_CLassifiers(logger=logger, directory=result_save_path2, dataset=pd.DataFrame(columns=[class_name]), class_name=class_name, label_map={}, metadata_column=metadata_column, split_column=split_column, random_state_value=0, split_balance_pattern=[], check_result=True) \n",
    "\n",
    "classifier_obj2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a69ab024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Data is being loaded from: ./Results/_Classification/ML6002/\n",
      "        \n",
      "Start retrieving Test Models model from file...\n",
      "Finish retrieving Test Models model from file...\n",
      "Start retrieving Training Models model from file...\n",
      "Finish retrieving Training Models model from file...\n",
      "Start retrieving Best Training Models model from file...\n",
      "Finish retrieving Best Training Models model from file...\n"
     ]
    }
   ],
   "source": [
    "best_tr_model2, tr_model2, tr_model_scores_df2, tr_target_and_prediction_df2, ts_model2, ts_model_scores_df2, ts_target_and_prediction_df2, ts_fold_info_df2, exp_info_df2  = classifier_obj2.load_results(result_save_path2)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2a084a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_tr_model, tr_model, tr_model_scores_df, tr_target_and_prediction_df, ts_model, ts_model_scores_df, ts_target_and_prediction_df, ts_fold_info_df, exp_info_df = best_tr_model2, tr_model2, tr_model_scores_df2, tr_target_and_prediction_df2, ts_model2, ts_model_scores_df2, ts_target_and_prediction_df2, ts_fold_info_df2, exp_info_df2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "39bb7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_No</th>\n",
       "      <th>Training_No</th>\n",
       "      <th>Model_No</th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>Selected_Features</th>\n",
       "      <th>Test</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Training</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>LR</td>\n",
       "      <td>['W-&gt;W-&gt;W', 'W-&gt;W-&gt;S1', 'W-&gt;W-&gt;S2', 'W-&gt;S1-&gt;W'...</td>\n",
       "      <td>['n1110', 'brux1', 'sdb1', 'ins1', 'ins2', 'in...</td>\n",
       "      <td>['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...</td>\n",
       "      <td>['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>LR</td>\n",
       "      <td>['W-&gt;W-&gt;W', 'W-&gt;W-&gt;S1', 'W-&gt;W-&gt;S2', 'W-&gt;S1-&gt;W'...</td>\n",
       "      <td>['n1110', 'brux1', 'sdb1', 'ins1', 'ins2', 'in...</td>\n",
       "      <td>['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...</td>\n",
       "      <td>['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>LR</td>\n",
       "      <td>['W-&gt;W-&gt;W', 'W-&gt;W-&gt;S1', 'W-&gt;W-&gt;S2', 'W-&gt;S1-&gt;W'...</td>\n",
       "      <td>['n1110', 'brux1', 'sdb1', 'ins1', 'ins2', 'in...</td>\n",
       "      <td>['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...</td>\n",
       "      <td>['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>LR</td>\n",
       "      <td>['W-&gt;W-&gt;W', 'W-&gt;W-&gt;S1', 'W-&gt;W-&gt;S2', 'W-&gt;S1-&gt;W'...</td>\n",
       "      <td>['n1110', 'brux1', 'sdb1', 'ins1', 'ins2', 'in...</td>\n",
       "      <td>['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...</td>\n",
       "      <td>['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>LR</td>\n",
       "      <td>['W-&gt;W-&gt;W', 'W-&gt;W-&gt;S1', 'W-&gt;W-&gt;S2', 'W-&gt;S1-&gt;W'...</td>\n",
       "      <td>['n1110', 'brux1', 'sdb1', 'ins1', 'ins2', 'in...</td>\n",
       "      <td>['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...</td>\n",
       "      <td>['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>XGB</td>\n",
       "      <td>['W-&gt;W-&gt;W', 'W-&gt;W-&gt;S1', 'W-&gt;W-&gt;S2', 'W-&gt;S1-&gt;W'...</td>\n",
       "      <td>['ins1108', 'ins1109', 'ins1110', 'ins1111', '...</td>\n",
       "      <td>['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...</td>\n",
       "      <td>['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>XGB</td>\n",
       "      <td>['W-&gt;W-&gt;W', 'W-&gt;W-&gt;S1', 'W-&gt;W-&gt;S2', 'W-&gt;S1-&gt;W'...</td>\n",
       "      <td>['ins1108', 'ins1109', 'ins1110', 'ins1111', '...</td>\n",
       "      <td>['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...</td>\n",
       "      <td>['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>XGB</td>\n",
       "      <td>['W-&gt;W-&gt;W', 'W-&gt;W-&gt;S1', 'W-&gt;W-&gt;S2', 'W-&gt;S1-&gt;W'...</td>\n",
       "      <td>['ins1108', 'ins1109', 'ins1110', 'ins1111', '...</td>\n",
       "      <td>['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...</td>\n",
       "      <td>['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>XGB</td>\n",
       "      <td>['W-&gt;W-&gt;W', 'W-&gt;W-&gt;S1', 'W-&gt;W-&gt;S2', 'W-&gt;S1-&gt;W'...</td>\n",
       "      <td>['ins1108', 'ins1109', 'ins1110', 'ins1111', '...</td>\n",
       "      <td>['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...</td>\n",
       "      <td>['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>XGB</td>\n",
       "      <td>['W-&gt;W-&gt;W', 'W-&gt;W-&gt;S1', 'W-&gt;W-&gt;S2', 'W-&gt;S1-&gt;W'...</td>\n",
       "      <td>['ins1108', 'ins1109', 'ins1110', 'ins1111', '...</td>\n",
       "      <td>['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...</td>\n",
       "      <td>['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Test_No  Training_No  Model_No Model_Name  \\\n",
       "0          1            1         1         LR   \n",
       "1          1            2         1         LR   \n",
       "2          1            3         1         LR   \n",
       "3          1            4         1         LR   \n",
       "4          1            5         1         LR   \n",
       "..       ...          ...       ...        ...   \n",
       "195        5            1         8        XGB   \n",
       "196        5            2         8        XGB   \n",
       "197        5            3         8        XGB   \n",
       "198        5            4         8        XGB   \n",
       "199        5            5         8        XGB   \n",
       "\n",
       "                                     Selected_Features  \\\n",
       "0    ['W->W->W', 'W->W->S1', 'W->W->S2', 'W->S1->W'...   \n",
       "1    ['W->W->W', 'W->W->S1', 'W->W->S2', 'W->S1->W'...   \n",
       "2    ['W->W->W', 'W->W->S1', 'W->W->S2', 'W->S1->W'...   \n",
       "3    ['W->W->W', 'W->W->S1', 'W->W->S2', 'W->S1->W'...   \n",
       "4    ['W->W->W', 'W->W->S1', 'W->W->S2', 'W->S1->W'...   \n",
       "..                                                 ...   \n",
       "195  ['W->W->W', 'W->W->S1', 'W->W->S2', 'W->S1->W'...   \n",
       "196  ['W->W->W', 'W->W->S1', 'W->W->S2', 'W->S1->W'...   \n",
       "197  ['W->W->W', 'W->W->S1', 'W->W->S2', 'W->S1->W'...   \n",
       "198  ['W->W->W', 'W->W->S1', 'W->W->S2', 'W->S1->W'...   \n",
       "199  ['W->W->W', 'W->W->S1', 'W->W->S2', 'W->S1->W'...   \n",
       "\n",
       "                                                  Test  \\\n",
       "0    ['n1110', 'brux1', 'sdb1', 'ins1', 'ins2', 'in...   \n",
       "1    ['n1110', 'brux1', 'sdb1', 'ins1', 'ins2', 'in...   \n",
       "2    ['n1110', 'brux1', 'sdb1', 'ins1', 'ins2', 'in...   \n",
       "3    ['n1110', 'brux1', 'sdb1', 'ins1', 'ins2', 'in...   \n",
       "4    ['n1110', 'brux1', 'sdb1', 'ins1', 'ins2', 'in...   \n",
       "..                                                 ...   \n",
       "195  ['ins1108', 'ins1109', 'ins1110', 'ins1111', '...   \n",
       "196  ['ins1108', 'ins1109', 'ins1110', 'ins1111', '...   \n",
       "197  ['ins1108', 'ins1109', 'ins1110', 'ins1111', '...   \n",
       "198  ['ins1108', 'ins1109', 'ins1110', 'ins1111', '...   \n",
       "199  ['ins1108', 'ins1109', 'ins1110', 'ins1111', '...   \n",
       "\n",
       "                                            Validation  \\\n",
       "0    ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...   \n",
       "1    ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...   \n",
       "2    ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...   \n",
       "3    ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...   \n",
       "4    ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...   \n",
       "..                                                 ...   \n",
       "195  ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...   \n",
       "196  ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...   \n",
       "197  ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...   \n",
       "198  ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...   \n",
       "199  ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...   \n",
       "\n",
       "                                              Training  \n",
       "0    ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...  \n",
       "1    ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...  \n",
       "2    ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...  \n",
       "3    ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...  \n",
       "4    ['brux2', 'sdb2', 'sdb3', 'sdb4', 'ins5', 'ins...  \n",
       "..                                                 ...  \n",
       "195  ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...  \n",
       "196  ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...  \n",
       "197  ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...  \n",
       "198  ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...  \n",
       "199  ['brux1', 'brux2', 'sdb1', 'sdb2', 'sdb3', 'sd...  \n",
       "\n",
       "[200 rows x 8 columns]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_fold_info_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d576769",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = ts_fold_info_df2[ ts_fold_info_df2['Model_No']==1 ] ['Selected_Features'] \n",
    "tt\n",
    "\n",
    "for t in tt:\n",
    "    print(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9354d98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_No</th>\n",
       "      <th>Model_No</th>\n",
       "      <th>method</th>\n",
       "      <th>model</th>\n",
       "      <th>model_parameters</th>\n",
       "      <th>model_scores</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=LogisticRegressio...</td>\n",
       "      <td>LogisticRegression(C=0.001, max_iter=50)</td>\n",
       "      <td>{'C': 0.001, 'max_iter': 50, 'penalty': 'l2'}</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 6], [0, 22]]</td>\n",
       "      <td>78.571</td>\n",
       "      <td>78.571</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=LogisticRegressio...</td>\n",
       "      <td>LogisticRegression(C=0.001, max_iter=50)</td>\n",
       "      <td>{'C': 0.001, 'max_iter': 50, 'penalty': 'l2'}</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 6], [0, 22]]</td>\n",
       "      <td>78.571</td>\n",
       "      <td>78.571</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=LogisticRegressio...</td>\n",
       "      <td>LogisticRegression(C=0.001, max_iter=50)</td>\n",
       "      <td>{'C': 0.001, 'max_iter': 50, 'penalty': 'l2'}</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 5], [0, 20]]</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.889</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=LogisticRegressio...</td>\n",
       "      <td>LogisticRegression(C=0.001, max_iter=50)</td>\n",
       "      <td>{'C': 0.001, 'max_iter': 50, 'penalty': 'l2'}</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 5], [0, 20]]</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.889</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=LogisticRegressio...</td>\n",
       "      <td>LogisticRegression(C=0.001, max_iter=50)</td>\n",
       "      <td>{'C': 0.001, 'max_iter': 50, 'penalty': 'l2'}</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 5], [0, 19]]</td>\n",
       "      <td>79.167</td>\n",
       "      <td>79.167</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.372</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...</td>\n",
       "      <td>SVC(C=0.001, kernel='linear', probability=True)</td>\n",
       "      <td>{'C': 0.001, 'kernel': 'linear', 'probability'...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 6], [0, 22]]</td>\n",
       "      <td>78.571</td>\n",
       "      <td>78.571</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...</td>\n",
       "      <td>SVC(C=0.001, kernel='linear', probability=True)</td>\n",
       "      <td>{'C': 0.001, 'kernel': 'linear', 'probability'...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 6], [0, 22]]</td>\n",
       "      <td>78.571</td>\n",
       "      <td>78.571</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...</td>\n",
       "      <td>SVC(C=0.001, kernel='linear', probability=True)</td>\n",
       "      <td>{'C': 0.001, 'kernel': 'linear', 'probability'...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 5], [0, 20]]</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.889</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...</td>\n",
       "      <td>SVC(C=0.001, kernel='linear', probability=True)</td>\n",
       "      <td>{'C': 0.001, 'kernel': 'linear', 'probability'...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 5], [0, 20]]</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.889</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...</td>\n",
       "      <td>SVC(C=0.001, kernel='linear', probability=True)</td>\n",
       "      <td>{'C': 0.001, 'kernel': 'linear', 'probability'...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 5], [0, 19]]</td>\n",
       "      <td>79.167</td>\n",
       "      <td>79.167</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.372</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GaussianNB(), n_j...</td>\n",
       "      <td>GaussianNB(var_smoothing=1.0)</td>\n",
       "      <td>{'var_smoothing': 1.0}</td>\n",
       "      <td>85.66</td>\n",
       "      <td>[[1, 5], [0, 22]]</td>\n",
       "      <td>82.143</td>\n",
       "      <td>81.481</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>16.667</td>\n",
       "      <td>89.796</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GaussianNB(), n_j...</td>\n",
       "      <td>GaussianNB(var_smoothing=1.0)</td>\n",
       "      <td>{'var_smoothing': 1.0}</td>\n",
       "      <td>86.84</td>\n",
       "      <td>[[1, 5], [0, 22]]</td>\n",
       "      <td>82.143</td>\n",
       "      <td>81.481</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>16.667</td>\n",
       "      <td>89.796</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GaussianNB(), n_j...</td>\n",
       "      <td>GaussianNB(var_smoothing=1.0)</td>\n",
       "      <td>{'var_smoothing': 1.0}</td>\n",
       "      <td>90.59</td>\n",
       "      <td>[[0, 5], [3, 17]]</td>\n",
       "      <td>68.000</td>\n",
       "      <td>77.273</td>\n",
       "      <td>85.000</td>\n",
       "      <td>85.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>80.952</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GaussianNB(), n_j...</td>\n",
       "      <td>GaussianNB(var_smoothing=1.0)</td>\n",
       "      <td>{'var_smoothing': 1.0}</td>\n",
       "      <td>91.76</td>\n",
       "      <td>[[4, 1], [4, 16]]</td>\n",
       "      <td>80.000</td>\n",
       "      <td>94.118</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>86.486</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GaussianNB(), n_j...</td>\n",
       "      <td>GaussianNB(var_smoothing=1.0)</td>\n",
       "      <td>{'var_smoothing': 1.0}</td>\n",
       "      <td>91.76</td>\n",
       "      <td>[[5, 0], [4, 15]]</td>\n",
       "      <td>83.333</td>\n",
       "      <td>100.000</td>\n",
       "      <td>78.947</td>\n",
       "      <td>78.947</td>\n",
       "      <td>100.000</td>\n",
       "      <td>88.235</td>\n",
       "      <td>0.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=KNeighborsClassif...</td>\n",
       "      <td>KNeighborsClassifier(metric='manhattan', n_nei...</td>\n",
       "      <td>{'metric': 'manhattan', 'n_neighbors': 25}</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 6], [0, 22]]</td>\n",
       "      <td>78.571</td>\n",
       "      <td>78.571</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=KNeighborsClassif...</td>\n",
       "      <td>KNeighborsClassifier(metric='manhattan', n_nei...</td>\n",
       "      <td>{'metric': 'manhattan', 'n_neighbors': 25}</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 6], [0, 22]]</td>\n",
       "      <td>78.571</td>\n",
       "      <td>78.571</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=KNeighborsClassif...</td>\n",
       "      <td>KNeighborsClassifier(metric='manhattan', n_nei...</td>\n",
       "      <td>{'metric': 'manhattan', 'n_neighbors': 35}</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 5], [0, 20]]</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.889</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=KNeighborsClassif...</td>\n",
       "      <td>KNeighborsClassifier(metric='manhattan', n_nei...</td>\n",
       "      <td>{'metric': 'manhattan', 'n_neighbors': 25}</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 5], [0, 20]]</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.889</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=KNeighborsClassif...</td>\n",
       "      <td>KNeighborsClassifier(metric='manhattan', n_nei...</td>\n",
       "      <td>{'metric': 'manhattan', 'n_neighbors': 15}</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 5], [0, 19]]</td>\n",
       "      <td>79.167</td>\n",
       "      <td>79.167</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.372</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=DecisionTreeClass...</td>\n",
       "      <td>DecisionTreeClassifier(criterion='entropy', ma...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 2}</td>\n",
       "      <td>90.44</td>\n",
       "      <td>[[0, 6], [0, 22]]</td>\n",
       "      <td>78.571</td>\n",
       "      <td>78.571</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=DecisionTreeClass...</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=2)</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2}</td>\n",
       "      <td>86.76</td>\n",
       "      <td>[[0, 6], [0, 22]]</td>\n",
       "      <td>78.571</td>\n",
       "      <td>78.571</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=DecisionTreeClass...</td>\n",
       "      <td>DecisionTreeClassifier(criterion='entropy', ma...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3}</td>\n",
       "      <td>94.04</td>\n",
       "      <td>[[0, 5], [2, 18]]</td>\n",
       "      <td>72.000</td>\n",
       "      <td>78.261</td>\n",
       "      <td>90.000</td>\n",
       "      <td>90.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>83.721</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=DecisionTreeClass...</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=2)</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2}</td>\n",
       "      <td>89.34</td>\n",
       "      <td>[[2, 3], [1, 19]]</td>\n",
       "      <td>84.000</td>\n",
       "      <td>86.364</td>\n",
       "      <td>95.000</td>\n",
       "      <td>95.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>90.476</td>\n",
       "      <td>0.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=DecisionTreeClass...</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=2)</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2}</td>\n",
       "      <td>92.87</td>\n",
       "      <td>[[2, 3], [2, 17]]</td>\n",
       "      <td>79.167</td>\n",
       "      <td>85.000</td>\n",
       "      <td>89.474</td>\n",
       "      <td>89.474</td>\n",
       "      <td>40.000</td>\n",
       "      <td>87.179</td>\n",
       "      <td>0.647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>RandomForestClassifier(max_depth=5, n_estimato...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'n_estim...</td>\n",
       "      <td>90.59</td>\n",
       "      <td>[[0, 6], [0, 22]]</td>\n",
       "      <td>78.571</td>\n",
       "      <td>78.571</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>RandomForestClassifier(max_depth=3, n_estimato...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 3, 'n_estim...</td>\n",
       "      <td>91.62</td>\n",
       "      <td>[[0, 6], [0, 22]]</td>\n",
       "      <td>78.571</td>\n",
       "      <td>78.571</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>RandomForestClassifier(criterion='entropy', ma...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3, 'n_es...</td>\n",
       "      <td>95.29</td>\n",
       "      <td>[[0, 5], [2, 18]]</td>\n",
       "      <td>72.000</td>\n",
       "      <td>78.261</td>\n",
       "      <td>90.000</td>\n",
       "      <td>90.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>83.721</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>RandomForestClassifier(criterion='entropy', ma...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7, 'n_es...</td>\n",
       "      <td>96.47</td>\n",
       "      <td>[[2, 3], [0, 20]]</td>\n",
       "      <td>88.000</td>\n",
       "      <td>86.957</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>93.023</td>\n",
       "      <td>0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>RandomForestClassifier(criterion='entropy', ma...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 2, 'n_es...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[1, 4], [0, 19]]</td>\n",
       "      <td>83.333</td>\n",
       "      <td>82.609</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>90.476</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GradientBoostingC...</td>\n",
       "      <td>GradientBoostingClassifier(learning_rate=0.01,...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 2, 'n_est...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 6], [0, 22]]</td>\n",
       "      <td>78.571</td>\n",
       "      <td>78.571</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GradientBoostingC...</td>\n",
       "      <td>GradientBoostingClassifier(learning_rate=0.01,...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 2, 'n_est...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 6], [0, 22]]</td>\n",
       "      <td>78.571</td>\n",
       "      <td>78.571</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GradientBoostingC...</td>\n",
       "      <td>GradientBoostingClassifier(learning_rate=0.01,...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 2, 'n_est...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 5], [0, 20]]</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.889</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GradientBoostingC...</td>\n",
       "      <td>GradientBoostingClassifier(learning_rate=0.01,...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 2, 'n_est...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 5], [0, 20]]</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.889</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GradientBoostingC...</td>\n",
       "      <td>GradientBoostingClassifier(learning_rate=0.01,...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 2, 'n_est...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[[0, 5], [0, 19]]</td>\n",
       "      <td>79.167</td>\n",
       "      <td>79.167</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.372</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=XGB...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'eta': 0.05, 'max_depth': 6}</td>\n",
       "      <td>88.09</td>\n",
       "      <td>[[1, 5], [1, 21]]</td>\n",
       "      <td>78.571</td>\n",
       "      <td>80.769</td>\n",
       "      <td>95.455</td>\n",
       "      <td>95.455</td>\n",
       "      <td>16.667</td>\n",
       "      <td>87.500</td>\n",
       "      <td>0.561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=XGB...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'eta': 0.01, 'max_depth': 2}</td>\n",
       "      <td>90.44</td>\n",
       "      <td>[[1, 5], [0, 22]]</td>\n",
       "      <td>82.143</td>\n",
       "      <td>81.481</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>16.667</td>\n",
       "      <td>89.796</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=XGB...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'eta': 0.01, 'max_depth': 2}</td>\n",
       "      <td>95.29</td>\n",
       "      <td>[[0, 5], [1, 19]]</td>\n",
       "      <td>76.000</td>\n",
       "      <td>79.167</td>\n",
       "      <td>95.000</td>\n",
       "      <td>95.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>86.364</td>\n",
       "      <td>0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=XGB...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'eta': 0.1, 'max_depth': 3}</td>\n",
       "      <td>91.62</td>\n",
       "      <td>[[3, 2], [1, 19]]</td>\n",
       "      <td>88.000</td>\n",
       "      <td>90.476</td>\n",
       "      <td>95.000</td>\n",
       "      <td>95.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>92.683</td>\n",
       "      <td>0.775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=XGB...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'eta': 0.01, 'max_depth': 2}</td>\n",
       "      <td>95.29</td>\n",
       "      <td>[[1, 4], [2, 17]]</td>\n",
       "      <td>75.000</td>\n",
       "      <td>80.952</td>\n",
       "      <td>89.474</td>\n",
       "      <td>89.474</td>\n",
       "      <td>20.000</td>\n",
       "      <td>85.000</td>\n",
       "      <td>0.547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Test_No  Model_No                                             method  \\\n",
       "0         1         1  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
       "1         2         1  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
       "2         3         1  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
       "3         4         1  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
       "4         5         1  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
       "5         1         2  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
       "6         2         2  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
       "7         3         2  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
       "8         4         2  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
       "9         5         2  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
       "10        1         3  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
       "11        2         3  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
       "12        3         3  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
       "13        4         3  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
       "14        5         3  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
       "15        1         4  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
       "16        2         4  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
       "17        3         4  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
       "18        4         4  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
       "19        5         4  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
       "20        1         5  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
       "21        2         5  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
       "22        3         5  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
       "23        4         5  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
       "24        5         5  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
       "25        1         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "26        2         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "27        3         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "28        4         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "29        5         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "30        1         7  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
       "31        2         7  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
       "32        3         7  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
       "33        4         7  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
       "34        5         7  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
       "35        1         8  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
       "36        2         8  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
       "37        3         8  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
       "38        4         8  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
       "39        5         8  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
       "\n",
       "                                                model  \\\n",
       "0            LogisticRegression(C=0.001, max_iter=50)   \n",
       "1            LogisticRegression(C=0.001, max_iter=50)   \n",
       "2            LogisticRegression(C=0.001, max_iter=50)   \n",
       "3            LogisticRegression(C=0.001, max_iter=50)   \n",
       "4            LogisticRegression(C=0.001, max_iter=50)   \n",
       "5     SVC(C=0.001, kernel='linear', probability=True)   \n",
       "6     SVC(C=0.001, kernel='linear', probability=True)   \n",
       "7     SVC(C=0.001, kernel='linear', probability=True)   \n",
       "8     SVC(C=0.001, kernel='linear', probability=True)   \n",
       "9     SVC(C=0.001, kernel='linear', probability=True)   \n",
       "10                      GaussianNB(var_smoothing=1.0)   \n",
       "11                      GaussianNB(var_smoothing=1.0)   \n",
       "12                      GaussianNB(var_smoothing=1.0)   \n",
       "13                      GaussianNB(var_smoothing=1.0)   \n",
       "14                      GaussianNB(var_smoothing=1.0)   \n",
       "15  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
       "16  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
       "17  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
       "18  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
       "19  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
       "20  DecisionTreeClassifier(criterion='entropy', ma...   \n",
       "21                DecisionTreeClassifier(max_depth=2)   \n",
       "22  DecisionTreeClassifier(criterion='entropy', ma...   \n",
       "23                DecisionTreeClassifier(max_depth=2)   \n",
       "24                DecisionTreeClassifier(max_depth=2)   \n",
       "25  RandomForestClassifier(max_depth=5, n_estimato...   \n",
       "26  RandomForestClassifier(max_depth=3, n_estimato...   \n",
       "27  RandomForestClassifier(criterion='entropy', ma...   \n",
       "28  RandomForestClassifier(criterion='entropy', ma...   \n",
       "29  RandomForestClassifier(criterion='entropy', ma...   \n",
       "30  GradientBoostingClassifier(learning_rate=0.01,...   \n",
       "31  GradientBoostingClassifier(learning_rate=0.01,...   \n",
       "32  GradientBoostingClassifier(learning_rate=0.01,...   \n",
       "33  GradientBoostingClassifier(learning_rate=0.01,...   \n",
       "34  GradientBoostingClassifier(learning_rate=0.01,...   \n",
       "35  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "36  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "37  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "38  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "39  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "\n",
       "                                     model_parameters  model_scores  \\\n",
       "0       {'C': 0.001, 'max_iter': 50, 'penalty': 'l2'}        100.00   \n",
       "1       {'C': 0.001, 'max_iter': 50, 'penalty': 'l2'}        100.00   \n",
       "2       {'C': 0.001, 'max_iter': 50, 'penalty': 'l2'}        100.00   \n",
       "3       {'C': 0.001, 'max_iter': 50, 'penalty': 'l2'}        100.00   \n",
       "4       {'C': 0.001, 'max_iter': 50, 'penalty': 'l2'}        100.00   \n",
       "5   {'C': 0.001, 'kernel': 'linear', 'probability'...        100.00   \n",
       "6   {'C': 0.001, 'kernel': 'linear', 'probability'...        100.00   \n",
       "7   {'C': 0.001, 'kernel': 'linear', 'probability'...        100.00   \n",
       "8   {'C': 0.001, 'kernel': 'linear', 'probability'...        100.00   \n",
       "9   {'C': 0.001, 'kernel': 'linear', 'probability'...        100.00   \n",
       "10                             {'var_smoothing': 1.0}         85.66   \n",
       "11                             {'var_smoothing': 1.0}         86.84   \n",
       "12                             {'var_smoothing': 1.0}         90.59   \n",
       "13                             {'var_smoothing': 1.0}         91.76   \n",
       "14                             {'var_smoothing': 1.0}         91.76   \n",
       "15         {'metric': 'manhattan', 'n_neighbors': 25}        100.00   \n",
       "16         {'metric': 'manhattan', 'n_neighbors': 25}        100.00   \n",
       "17         {'metric': 'manhattan', 'n_neighbors': 35}        100.00   \n",
       "18         {'metric': 'manhattan', 'n_neighbors': 25}        100.00   \n",
       "19         {'metric': 'manhattan', 'n_neighbors': 15}        100.00   \n",
       "20           {'criterion': 'entropy', 'max_depth': 2}         90.44   \n",
       "21              {'criterion': 'gini', 'max_depth': 2}         86.76   \n",
       "22           {'criterion': 'entropy', 'max_depth': 3}         94.04   \n",
       "23              {'criterion': 'gini', 'max_depth': 2}         89.34   \n",
       "24              {'criterion': 'gini', 'max_depth': 2}         92.87   \n",
       "25  {'criterion': 'gini', 'max_depth': 5, 'n_estim...         90.59   \n",
       "26  {'criterion': 'gini', 'max_depth': 3, 'n_estim...         91.62   \n",
       "27  {'criterion': 'entropy', 'max_depth': 3, 'n_es...         95.29   \n",
       "28  {'criterion': 'entropy', 'max_depth': 7, 'n_es...         96.47   \n",
       "29  {'criterion': 'entropy', 'max_depth': 2, 'n_es...        100.00   \n",
       "30  {'learning_rate': 0.01, 'max_depth': 2, 'n_est...        100.00   \n",
       "31  {'learning_rate': 0.01, 'max_depth': 2, 'n_est...        100.00   \n",
       "32  {'learning_rate': 0.01, 'max_depth': 2, 'n_est...        100.00   \n",
       "33  {'learning_rate': 0.01, 'max_depth': 2, 'n_est...        100.00   \n",
       "34  {'learning_rate': 0.01, 'max_depth': 2, 'n_est...        100.00   \n",
       "35                      {'eta': 0.05, 'max_depth': 6}         88.09   \n",
       "36                      {'eta': 0.01, 'max_depth': 2}         90.44   \n",
       "37                      {'eta': 0.01, 'max_depth': 2}         95.29   \n",
       "38                       {'eta': 0.1, 'max_depth': 3}         91.62   \n",
       "39                      {'eta': 0.01, 'max_depth': 2}         95.29   \n",
       "\n",
       "     confusion_matrix  accuracy  precision   recall  sensitivity  specificity  \\\n",
       "0   [[0, 6], [0, 22]]    78.571     78.571  100.000      100.000        0.000   \n",
       "1   [[0, 6], [0, 22]]    78.571     78.571  100.000      100.000        0.000   \n",
       "2   [[0, 5], [0, 20]]    80.000     80.000  100.000      100.000        0.000   \n",
       "3   [[0, 5], [0, 20]]    80.000     80.000  100.000      100.000        0.000   \n",
       "4   [[0, 5], [0, 19]]    79.167     79.167  100.000      100.000        0.000   \n",
       "5   [[0, 6], [0, 22]]    78.571     78.571  100.000      100.000        0.000   \n",
       "6   [[0, 6], [0, 22]]    78.571     78.571  100.000      100.000        0.000   \n",
       "7   [[0, 5], [0, 20]]    80.000     80.000  100.000      100.000        0.000   \n",
       "8   [[0, 5], [0, 20]]    80.000     80.000  100.000      100.000        0.000   \n",
       "9   [[0, 5], [0, 19]]    79.167     79.167  100.000      100.000        0.000   \n",
       "10  [[1, 5], [0, 22]]    82.143     81.481  100.000      100.000       16.667   \n",
       "11  [[1, 5], [0, 22]]    82.143     81.481  100.000      100.000       16.667   \n",
       "12  [[0, 5], [3, 17]]    68.000     77.273   85.000       85.000        0.000   \n",
       "13  [[4, 1], [4, 16]]    80.000     94.118   80.000       80.000       80.000   \n",
       "14  [[5, 0], [4, 15]]    83.333    100.000   78.947       78.947      100.000   \n",
       "15  [[0, 6], [0, 22]]    78.571     78.571  100.000      100.000        0.000   \n",
       "16  [[0, 6], [0, 22]]    78.571     78.571  100.000      100.000        0.000   \n",
       "17  [[0, 5], [0, 20]]    80.000     80.000  100.000      100.000        0.000   \n",
       "18  [[0, 5], [0, 20]]    80.000     80.000  100.000      100.000        0.000   \n",
       "19  [[0, 5], [0, 19]]    79.167     79.167  100.000      100.000        0.000   \n",
       "20  [[0, 6], [0, 22]]    78.571     78.571  100.000      100.000        0.000   \n",
       "21  [[0, 6], [0, 22]]    78.571     78.571  100.000      100.000        0.000   \n",
       "22  [[0, 5], [2, 18]]    72.000     78.261   90.000       90.000        0.000   \n",
       "23  [[2, 3], [1, 19]]    84.000     86.364   95.000       95.000       40.000   \n",
       "24  [[2, 3], [2, 17]]    79.167     85.000   89.474       89.474       40.000   \n",
       "25  [[0, 6], [0, 22]]    78.571     78.571  100.000      100.000        0.000   \n",
       "26  [[0, 6], [0, 22]]    78.571     78.571  100.000      100.000        0.000   \n",
       "27  [[0, 5], [2, 18]]    72.000     78.261   90.000       90.000        0.000   \n",
       "28  [[2, 3], [0, 20]]    88.000     86.957  100.000      100.000       40.000   \n",
       "29  [[1, 4], [0, 19]]    83.333     82.609  100.000      100.000       20.000   \n",
       "30  [[0, 6], [0, 22]]    78.571     78.571  100.000      100.000        0.000   \n",
       "31  [[0, 6], [0, 22]]    78.571     78.571  100.000      100.000        0.000   \n",
       "32  [[0, 5], [0, 20]]    80.000     80.000  100.000      100.000        0.000   \n",
       "33  [[0, 5], [0, 20]]    80.000     80.000  100.000      100.000        0.000   \n",
       "34  [[0, 5], [0, 19]]    79.167     79.167  100.000      100.000        0.000   \n",
       "35  [[1, 5], [1, 21]]    78.571     80.769   95.455       95.455       16.667   \n",
       "36  [[1, 5], [0, 22]]    82.143     81.481  100.000      100.000       16.667   \n",
       "37  [[0, 5], [1, 19]]    76.000     79.167   95.000       95.000        0.000   \n",
       "38  [[3, 2], [1, 19]]    88.000     90.476   95.000       95.000       60.000   \n",
       "39  [[1, 4], [2, 17]]    75.000     80.952   89.474       89.474       20.000   \n",
       "\n",
       "    f1_score  roc_auc  \n",
       "0     88.000    0.500  \n",
       "1     88.000    0.500  \n",
       "2     88.889    0.500  \n",
       "3     88.889    0.500  \n",
       "4     88.372    0.500  \n",
       "5     88.000    0.500  \n",
       "6     88.000    0.500  \n",
       "7     88.889    0.500  \n",
       "8     88.889    0.500  \n",
       "9     88.372    0.500  \n",
       "10    89.796    0.583  \n",
       "11    89.796    0.583  \n",
       "12    80.952    0.425  \n",
       "13    86.486    0.800  \n",
       "14    88.235    0.895  \n",
       "15    88.000    0.500  \n",
       "16    88.000    0.500  \n",
       "17    88.889    0.500  \n",
       "18    88.889    0.500  \n",
       "19    88.372    0.500  \n",
       "20    88.000    0.500  \n",
       "21    88.000    0.500  \n",
       "22    83.721    0.450  \n",
       "23    90.476    0.675  \n",
       "24    87.179    0.647  \n",
       "25    88.000    0.500  \n",
       "26    88.000    0.500  \n",
       "27    83.721    0.450  \n",
       "28    93.023    0.700  \n",
       "29    90.476    0.600  \n",
       "30    88.000    0.500  \n",
       "31    88.000    0.500  \n",
       "32    88.889    0.500  \n",
       "33    88.889    0.500  \n",
       "34    88.372    0.500  \n",
       "35    87.500    0.561  \n",
       "36    89.796    0.583  \n",
       "37    86.364    0.475  \n",
       "38    92.683    0.775  \n",
       "39    85.000    0.547  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_model_scores_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "0834471f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.02888352, 0.01464654, 0.02246696, 0.03424956,\n",
       "       0.03860234, 0.        , 0.01117043, 0.        , 0.04388861,\n",
       "       0.03416731, 0.        , 0.01632673, 0.0932154 , 0.03100788,\n",
       "       0.        , 0.02564235, 0.        , 0.        , 0.02300982,\n",
       "       0.        , 0.028694  , 0.09891637, 0.0081134 , 0.02662944,\n",
       "       0.01282695, 0.        , 0.        , 0.        , 0.01723145,\n",
       "       0.        , 0.        , 0.09260955, 0.        , 0.        ,\n",
       "       0.0764569 , 0.02281511, 0.00252085, 0.        , 0.        ,\n",
       "       0.0131076 , 0.        , 0.00249633, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.06456443, 0.05540885, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05821315,\n",
       "       0.00211816])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_model2[1][6].best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "id": "5c5358f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0.], dtype=float32)"
      ]
     },
     "execution_count": 1084,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_model2[1][8].best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d03105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b2db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_model_scores_df2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bbbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_model_scores_df2[ (tr_model_scores_df2['Model_No']==6) ]['accuracy'].min() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef8da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_model_scores_df['method_name'][-5:].values, ts_model_scores_df['method_name'][-5:].values[0], type(ts_model_scores_df['method_name'][-5:].values[0]) \n",
    "ts_model_scores_df2[-30:] \n",
    "ts_model_scores_df2[ ts_model_scores_df2['Model_No']==1 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5570b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model_scores_df2[ ts_model_scores_df2['Model_No']==6 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c2d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model_scores_df2[ ts_model_scores_df2['Model_No']==7 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076dfb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c7dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d68db",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_ts_model[1][1]), type(all_ts_model[1][1].estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c862e54b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df851bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf42adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec82d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.to_csv('./dataset.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723abf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "69a20cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_tr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2141968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bec533dc",
   "metadata": {},
   "source": [
    "#### Show and save RF Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e08867e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_model = ts_model2\n",
    "# result_save_path = result_save_path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "798f9ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(5, 0), (5, 20)], 6)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitting_crieteria, (splitting_crieteria[0][0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "id": "f8e84dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subject_Name</th>\n",
       "      <th>Class</th>\n",
       "      <th>W-&gt;W</th>\n",
       "      <th>W-&gt;S1</th>\n",
       "      <th>W-&gt;S2</th>\n",
       "      <th>W-&gt;S3</th>\n",
       "      <th>W-&gt;S4</th>\n",
       "      <th>W-&gt;REM</th>\n",
       "      <th>...</th>\n",
       "      <th>S4-&gt;S2</th>\n",
       "      <th>S4-&gt;S3</th>\n",
       "      <th>S4-&gt;S4</th>\n",
       "      <th>S4-&gt;REM</th>\n",
       "      <th>REM-&gt;W</th>\n",
       "      <th>REM-&gt;S1</th>\n",
       "      <th>REM-&gt;S2</th>\n",
       "      <th>REM-&gt;S3</th>\n",
       "      <th>REM-&gt;S4</th>\n",
       "      <th>REM-&gt;REM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.970443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.966480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.984940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.876147</td>\n",
       "      <td>0.123853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.949239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.962617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.948387</td>\n",
       "      <td>0.045161</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.974359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.934272</td>\n",
       "      <td>0.046948</td>\n",
       "      <td>0.018779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.935065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7191</td>\n",
       "      <td>0</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7201</td>\n",
       "      <td>0</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.932836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7211</td>\n",
       "      <td>0</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7221</td>\n",
       "      <td>0</td>\n",
       "      <td>0.818792</td>\n",
       "      <td>0.181208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021645</td>\n",
       "      <td>0.043290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.935065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7241</td>\n",
       "      <td>0</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dataset Category Subject_Name  Class      W->W     W->S1     W->S2  \\\n",
       "0     CAP_Sleep     brux        brux1      1  0.798077  0.192308  0.009615   \n",
       "1     CAP_Sleep     brux        brux2      1  0.824000  0.176000  0.000000   \n",
       "2     CAP_Sleep      sdb         sdb1      1  0.876147  0.123853  0.000000   \n",
       "3     CAP_Sleep      sdb         sdb2      1  0.948387  0.045161  0.006452   \n",
       "4     CAP_Sleep      sdb         sdb3      1  0.934272  0.046948  0.018779   \n",
       "..          ...      ...          ...    ...       ...       ...       ...   \n",
       "203  Sleep_EDFX        n       ST7191      0  0.739130  0.246377  0.014493   \n",
       "204  Sleep_EDFX        n       ST7201      0  0.294118  0.588235  0.058824   \n",
       "205  Sleep_EDFX        n       ST7211      0  0.894118  0.100000  0.000000   \n",
       "206  Sleep_EDFX        n       ST7221      0  0.818792  0.181208  0.000000   \n",
       "207  Sleep_EDFX        n       ST7241      0  0.607143  0.285714  0.035714   \n",
       "\n",
       "     W->S3  W->S4    W->REM  ...    S4->S2    S4->S3    S4->S4  S4->REM  \\\n",
       "0      0.0    0.0  0.000000  ...  0.004926  0.024631  0.970443      0.0   \n",
       "1      0.0    0.0  0.000000  ...  0.000000  0.012048  0.984940      0.0   \n",
       "2      0.0    0.0  0.000000  ...  0.005076  0.030457  0.949239      0.0   \n",
       "3      0.0    0.0  0.000000  ...  0.008475  0.016949  0.966102      0.0   \n",
       "4      0.0    0.0  0.000000  ...  0.025974  0.038961  0.935065      0.0   \n",
       "..     ...    ...       ...  ...       ...       ...       ...      ...   \n",
       "203    0.0    0.0  0.000000  ...  0.013889  0.166667  0.805556      0.0   \n",
       "204    0.0    0.0  0.058824  ...  0.038462  0.615385  0.346154      0.0   \n",
       "205    0.0    0.0  0.005882  ...  0.016949  0.254237  0.728814      0.0   \n",
       "206    0.0    0.0  0.000000  ...  0.000000  0.000000  0.000000      0.0   \n",
       "207    0.0    0.0  0.071429  ...  0.142857  0.571429  0.142857      0.0   \n",
       "\n",
       "       REM->W   REM->S1   REM->S2  REM->S3  REM->S4  REM->REM  \n",
       "0    0.033520  0.000000  0.000000      0.0      0.0  0.966480  \n",
       "1    0.019324  0.004831  0.000000      0.0      0.0  0.975845  \n",
       "2    0.009346  0.000000  0.028037      0.0      0.0  0.962617  \n",
       "3    0.000000  0.000000  0.025641      0.0      0.0  0.974359  \n",
       "4    0.062500  0.000000  0.000000      0.0      0.0  0.937500  \n",
       "..        ...       ...       ...      ...      ...       ...  \n",
       "203  0.000000  0.020000  0.015000      0.0      0.0  0.965000  \n",
       "204  0.044776  0.007463  0.014925      0.0      0.0  0.932836  \n",
       "205  0.025641  0.000000  0.019231      0.0      0.0  0.955128  \n",
       "206  0.021645  0.043290  0.000000      0.0      0.0  0.935065  \n",
       "207  0.016878  0.016878  0.008439      0.0      0.0  0.957806  \n",
       "\n",
       "[208 rows x 40 columns]"
      ]
     },
     "execution_count": 1138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ts_model \n",
    "processed_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f33d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_model[3][6].feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1f4407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_model[1][6].best_estimator_.feature_names = ['hi', 'hello', 5, 1.5] \n",
    "ts_model[1][6].feature_names, ts_model[1][6].best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "63c6e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lll in range(1, 6):\n",
    "#     print(lll, ts_model[lll][6].best_estimator_, len(ts_model[lll][6].feature_names), ts_model[lll][6].best_estimator_.feature_importances_.shape[0], ts_model[lll][6].feature_names, ts_model[lll][6].best_estimator_.feature_importances_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6f3502a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_feat_importance_df = pd.DataFrame() \n",
    "\n",
    "# for feat_ss in range(1, (splitting_crieteria[0][0]+1)):\n",
    "# # #     print('---> ', feat_ss, rf_feat_importance_df)\n",
    "# #     rf_feat_importance_df[f'Feature-{feat_ss}'] = ts_model[feat_ss][6].feature_names  \n",
    "# # #     print(feat_ss, ts_model[fld_ss][6].best_estimator_.feature_importances_)\n",
    "# #     rf_feat_importance_df[f'Fold-{feat_ss}'] = ts_model[feat_ss][6].best_estimator_.feature_importances_ \n",
    "#     tdf = pd.DataFrame({f'Feature-{feat_ss}': ts_model[feat_ss][6].feature_names,\n",
    "#                        f'Fold-{feat_ss}': ts_model[feat_ss][6].best_estimator_.feature_importances_ }) \n",
    "#     rf_feat_importance_df = pd.concat([rf_feat_importance_df, tdf], axis=1) \n",
    "    \n",
    "# rf_feat_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "id": "f9f53dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['W->S1', 'W->S2', 'W->S3', 'W->S4', 'W->REM', 'S1->W', 'S1->S1',\n",
       "       'S1->S2', 'S1->S3', 'S1->S4', 'S1->REM', 'S2->W', 'S2->S1',\n",
       "       'S2->S2', 'S2->S3', 'S2->S4', 'S2->REM', 'S3->W', 'S3->S1',\n",
       "       'S3->S2', 'S3->S3', 'S3->S4', 'S3->REM', 'S4->W', 'S4->S1',\n",
       "       'S4->S2', 'S4->S3', 'S4->S4', 'S4->REM', 'REM->W', 'REM->S1',\n",
       "       'REM->S2', 'REM->S3', 'REM->REM'], dtype=object)"
      ]
     },
     "execution_count": 1120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model id # not just RF but also other model \n",
    "mod_id = 8 #6-RF, 8-xgboost \n",
    "\n",
    "rf_feat_importance_df = pd.DataFrame() \n",
    "all_fts = processed_dataset.columns.values[4:]\n",
    "rf_feat_importance_df[f'Feature'] = all_fts \n",
    "rf_feat_importance_df[f'Selected'] = [0 for _ in all_fts] \n",
    "# rf_feat_importance_df.set_index([f'Feature'])\n",
    "\n",
    "for feat_ss in range(1, (splitting_crieteria[0][0]+1)):\n",
    "#     print('---> ', feat_ss, rf_feat_importance_df)\n",
    "    fts = ts_model[feat_ss][mod_id].feature_names \n",
    "    modl = ts_model[feat_ss][mod_id].best_estimator_ \n",
    "    print(feat_ss, fts, type(modl), modl, modl.feature_importances_)\n",
    "#     print(modl.predict(processed_dataset[all_fts].iloc[:3, :]))\n",
    "    fts_imp = modl.feature_importances_ \n",
    "#     ### for xgboost\n",
    "#     feature_important = modl.get_booster().get_score(importance_type=\"weight\") #weight, gain\n",
    "#     fts_imp = list(feature_important.values())\n",
    "    print(f'Importance: {fts_imp} || {len(fts)==len(fts_imp)}')\n",
    "    imp_lst = [fts_imp[fts.index(f)] if (f in fts and fts.index(f)>=0) else None for f in all_fts] # [MM[LL.index(f)] for f in LL if f in NN] \n",
    "    for f in fts: # Change the status of the selected features if this feature has been used in the experiment of not \n",
    "#         rf_feat_importance_df[(rf_feat_importance_df[f'Feature']==f)][f'Selected'] = 1 \n",
    "        rf_feat_importance_df.loc[rf_feat_importance_df[f'Feature']==f, f'Selected'] = 1\n",
    "#         print(f, rf_feat_importance_df[(rf_feat_importance_df[f'Feature']==f)][f'Selected']) \n",
    "    rf_feat_importance_df[f'Fold-{feat_ss}'] = imp_lst \n",
    "    \n",
    "rf_feat_importance_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "id": "071042d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_feat_importance_df[(rf_feat_importance_df[f'Selected']=='REM->S3')]#[f'Selected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "id": "7aeb5a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 34==23+11, np.nan, np.NaN, \n",
    "# rf_feat_importance_df[(rf_feat_importance_df['Fold-1']==np.nan)]\n",
    "\n",
    "# rf_feat_importance_df['Fold-1'][2]==np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "id": "555d024f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Results/_Classification/ML13003/',\n",
       " './Results/_Classification/ML13003/rf_feat_importance.csv')"
      ]
     },
     "execution_count": 1033,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_save_path, f'{result_save_path}rf_feat_importance.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "id": "1967420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_feat_importance_df.to_csv(f'{result_save_path}rf_feat_importance.csv', index=False)  \n",
    "# rf_feat_importance_df.to_csv(f'{result_save_path}gb_feat_importance.csv', index=False)  \n",
    "rf_feat_importance_df.to_csv(f'{result_save_path}xgb_feat_importance.csv', index=False)  \n",
    "# rf_feat_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "id": "43dcab05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Selected</th>\n",
       "      <th>Fold-1</th>\n",
       "      <th>Fold-2</th>\n",
       "      <th>Fold-3</th>\n",
       "      <th>Fold-4</th>\n",
       "      <th>Fold-5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Class</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W-&gt;S4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>W-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009430</td>\n",
       "      <td>0.018044</td>\n",
       "      <td>0.102245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S1-&gt;W</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S1-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S1-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S1-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S1-&gt;S4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>S1-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>S2-&gt;W</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>S2-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>S2-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>S2-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>S2-&gt;S4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>S2-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>S3-&gt;W</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>S3-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>S3-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.177598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>S3-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>S3-&gt;S4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111985</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>S3-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>S4-&gt;W</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>S4-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>S4-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>S4-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.069736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>S4-&gt;S4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.235916</td>\n",
       "      <td>0.375931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>S4-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>REM-&gt;W</td>\n",
       "      <td>1</td>\n",
       "      <td>0.085545</td>\n",
       "      <td>0.180445</td>\n",
       "      <td>0.079176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>REM-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>REM-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>REM-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>REM-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature  Selected    Fold-1    Fold-2    Fold-3    Fold-4    Fold-5\n",
       "0      Class         0       NaN       NaN       NaN       NaN       NaN\n",
       "1      W->S1         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "2      W->S2         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "3      W->S3         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "4      W->S4         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "5     W->REM         1  0.009430  0.018044  0.102245  0.000000  0.000000\n",
       "6      S1->W         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "7     S1->S1         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "8     S1->S2         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "9     S1->S3         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "10    S1->S4         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "11   S1->REM         1  0.000000  0.000000  0.089588  0.000000  0.206566\n",
       "12     S2->W         1  0.000000  0.020932  0.000000  0.000000  0.000000\n",
       "13    S2->S1         1  0.000000  0.000000  0.000000  0.000000  0.099530\n",
       "14    S2->S2         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "15    S2->S3         1  0.000000  0.059188  0.000000  0.000000  0.000000\n",
       "16    S2->S4         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "17   S2->REM         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "18     S3->W         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "19    S3->S1         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "20    S3->S2         1  0.177598  0.000000  0.000000  0.000000  0.000000\n",
       "21    S3->S3         1  0.000000  0.127839  0.000000  0.000000  0.000000\n",
       "22    S3->S4         1  0.000000  0.253213  0.000000  0.111985  0.000000\n",
       "23   S3->REM         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "24     S4->W         1  0.000000  0.000000  0.019727  0.000000  0.000000\n",
       "25    S4->S1         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "26    S4->S2         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "27    S4->S3         1  0.069736  0.000000  0.000000  0.000000  0.000000\n",
       "28    S4->S4         1  0.000000  0.235916  0.375931  0.000000  0.000000\n",
       "29   S4->REM         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "30    REM->W         1  0.085545  0.180445  0.079176  0.000000  0.000000\n",
       "31   REM->S1         1  0.000000  0.000000  0.000000  0.000000  0.027237\n",
       "32   REM->S2         1  0.000000  0.000000  0.000000  0.009567  0.000000\n",
       "33   REM->S3         1  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "34  REM->REM         1  0.000000  0.000000  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 1035,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_feat_importance_df = pd.read_csv(f'{result_save_path}rf_feat_importance.csv') \n",
    "rf_feat_importance_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2009c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa09ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d46a20c",
   "metadata": {},
   "source": [
    "#### Store AUC scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "id": "4841b208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Selected</th>\n",
       "      <th>AUC-Fold-1</th>\n",
       "      <th>AUC-Fold-2</th>\n",
       "      <th>AUC-Fold-3</th>\n",
       "      <th>AUC-Fold-4</th>\n",
       "      <th>AUC-Fold-5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Class</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.748397</td>\n",
       "      <td>0.680654</td>\n",
       "      <td>0.642136</td>\n",
       "      <td>0.672304</td>\n",
       "      <td>0.365950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.739583</td>\n",
       "      <td>0.690612</td>\n",
       "      <td>0.642496</td>\n",
       "      <td>0.646230</td>\n",
       "      <td>0.711828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.490785</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.462297</td>\n",
       "      <td>0.554839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W-&gt;S4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>W-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.657853</td>\n",
       "      <td>0.343528</td>\n",
       "      <td>0.405483</td>\n",
       "      <td>0.403453</td>\n",
       "      <td>0.613620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S1-&gt;W</td>\n",
       "      <td>1</td>\n",
       "      <td>0.665465</td>\n",
       "      <td>0.643670</td>\n",
       "      <td>0.310606</td>\n",
       "      <td>0.600423</td>\n",
       "      <td>0.734050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S1-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.453926</td>\n",
       "      <td>0.433144</td>\n",
       "      <td>0.680375</td>\n",
       "      <td>0.628964</td>\n",
       "      <td>0.407885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S1-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.636619</td>\n",
       "      <td>0.671053</td>\n",
       "      <td>0.703463</td>\n",
       "      <td>0.630726</td>\n",
       "      <td>0.662366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S1-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.497866</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.507400</td>\n",
       "      <td>0.521864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S1-&gt;S4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>S1-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.684696</td>\n",
       "      <td>0.686700</td>\n",
       "      <td>0.715729</td>\n",
       "      <td>0.698379</td>\n",
       "      <td>0.788172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>S2-&gt;W</td>\n",
       "      <td>1</td>\n",
       "      <td>0.719551</td>\n",
       "      <td>0.672831</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.660677</td>\n",
       "      <td>0.628315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>S2-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.758013</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.755411</td>\n",
       "      <td>0.772023</td>\n",
       "      <td>0.707168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>S2-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.595353</td>\n",
       "      <td>0.372333</td>\n",
       "      <td>0.698413</td>\n",
       "      <td>0.680761</td>\n",
       "      <td>0.369892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>S2-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.332131</td>\n",
       "      <td>0.667496</td>\n",
       "      <td>0.708153</td>\n",
       "      <td>0.323820</td>\n",
       "      <td>0.698208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>S2-&gt;S4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.455548</td>\n",
       "      <td>0.458514</td>\n",
       "      <td>0.467583</td>\n",
       "      <td>0.459498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>S2-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.357372</td>\n",
       "      <td>0.370555</td>\n",
       "      <td>0.386003</td>\n",
       "      <td>0.365046</td>\n",
       "      <td>0.414337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>S3-&gt;W</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492388</td>\n",
       "      <td>0.394381</td>\n",
       "      <td>0.384921</td>\n",
       "      <td>0.571177</td>\n",
       "      <td>0.617204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>S3-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.590545</td>\n",
       "      <td>0.549787</td>\n",
       "      <td>0.419913</td>\n",
       "      <td>0.494715</td>\n",
       "      <td>0.410753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>S3-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783253</td>\n",
       "      <td>0.772048</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.737491</td>\n",
       "      <td>0.767025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>S3-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.740785</td>\n",
       "      <td>0.774893</td>\n",
       "      <td>0.779221</td>\n",
       "      <td>0.766032</td>\n",
       "      <td>0.847312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>S3-&gt;S4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.808093</td>\n",
       "      <td>0.801209</td>\n",
       "      <td>0.795815</td>\n",
       "      <td>0.732558</td>\n",
       "      <td>0.829391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>S3-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.568510</td>\n",
       "      <td>0.449502</td>\n",
       "      <td>0.518038</td>\n",
       "      <td>0.434813</td>\n",
       "      <td>0.478853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>S4-&gt;W</td>\n",
       "      <td>1</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.392959</td>\n",
       "      <td>0.445527</td>\n",
       "      <td>0.586681</td>\n",
       "      <td>0.630824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>S4-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.661859</td>\n",
       "      <td>0.644026</td>\n",
       "      <td>0.617965</td>\n",
       "      <td>0.639535</td>\n",
       "      <td>0.612903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>S4-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>0.733642</td>\n",
       "      <td>0.648990</td>\n",
       "      <td>0.310430</td>\n",
       "      <td>0.725448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>S4-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.777244</td>\n",
       "      <td>0.762091</td>\n",
       "      <td>0.736291</td>\n",
       "      <td>0.752290</td>\n",
       "      <td>0.767384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>S4-&gt;S4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.913862</td>\n",
       "      <td>0.849218</td>\n",
       "      <td>0.775613</td>\n",
       "      <td>0.830162</td>\n",
       "      <td>0.856989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>S4-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>REM-&gt;W</td>\n",
       "      <td>1</td>\n",
       "      <td>0.729567</td>\n",
       "      <td>0.769915</td>\n",
       "      <td>0.684343</td>\n",
       "      <td>0.688865</td>\n",
       "      <td>0.697849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>REM-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753205</td>\n",
       "      <td>0.735420</td>\n",
       "      <td>0.638528</td>\n",
       "      <td>0.677237</td>\n",
       "      <td>0.705018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>REM-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.332532</td>\n",
       "      <td>0.708748</td>\n",
       "      <td>0.398990</td>\n",
       "      <td>0.638478</td>\n",
       "      <td>0.712903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>REM-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>REM-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.778446</td>\n",
       "      <td>0.766003</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.726920</td>\n",
       "      <td>0.726165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature  Selected  AUC-Fold-1  AUC-Fold-2  AUC-Fold-3  AUC-Fold-4  \\\n",
       "0      Class         0    0.500000    0.500000    0.500000    0.500000   \n",
       "1      W->S1         1    0.748397    0.680654    0.642136    0.672304   \n",
       "2      W->S2         1    0.739583    0.690612    0.642496    0.646230   \n",
       "3      W->S3         1    0.490785    0.472973    0.500000    0.462297   \n",
       "4      W->S4         1    0.500000    0.500000    0.500000    0.500000   \n",
       "5     W->REM         1    0.657853    0.343528    0.405483    0.403453   \n",
       "6      S1->W         1    0.665465    0.643670    0.310606    0.600423   \n",
       "7     S1->S1         1    0.453926    0.433144    0.680375    0.628964   \n",
       "8     S1->S2         1    0.636619    0.671053    0.703463    0.630726   \n",
       "9     S1->S3         1    0.500000    0.497866    0.507937    0.507400   \n",
       "10    S1->S4         1    0.500000    0.500000    0.500000    0.500000   \n",
       "11   S1->REM         1    0.684696    0.686700    0.715729    0.698379   \n",
       "12     S2->W         1    0.719551    0.672831    0.702381    0.660677   \n",
       "13    S2->S1         1    0.758013    0.763158    0.755411    0.772023   \n",
       "14    S2->S2         1    0.595353    0.372333    0.698413    0.680761   \n",
       "15    S2->S3         1    0.332131    0.667496    0.708153    0.323820   \n",
       "16    S2->S4         1    0.437500    0.455548    0.458514    0.467583   \n",
       "17   S2->REM         1    0.357372    0.370555    0.386003    0.365046   \n",
       "18     S3->W         1    0.492388    0.394381    0.384921    0.571177   \n",
       "19    S3->S1         1    0.590545    0.549787    0.419913    0.494715   \n",
       "20    S3->S2         1    0.783253    0.772048    0.746753    0.737491   \n",
       "21    S3->S3         1    0.740785    0.774893    0.779221    0.766032   \n",
       "22    S3->S4         1    0.808093    0.801209    0.795815    0.732558   \n",
       "23   S3->REM         1    0.568510    0.449502    0.518038    0.434813   \n",
       "24     S4->W         1    0.453125    0.392959    0.445527    0.586681   \n",
       "25    S4->S1         1    0.661859    0.644026    0.617965    0.639535   \n",
       "26    S4->S2         1    0.744792    0.733642    0.648990    0.310430   \n",
       "27    S4->S3         1    0.777244    0.762091    0.736291    0.752290   \n",
       "28    S4->S4         1    0.913862    0.849218    0.775613    0.830162   \n",
       "29   S4->REM         1    0.500000    0.486486    0.500000    0.515152   \n",
       "30    REM->W         1    0.729567    0.769915    0.684343    0.688865   \n",
       "31   REM->S1         1    0.753205    0.735420    0.638528    0.677237   \n",
       "32   REM->S2         1    0.332532    0.708748    0.398990    0.638478   \n",
       "33   REM->S3         1    0.500000    0.500000    0.500000    0.500000   \n",
       "34  REM->REM         1    0.778446    0.766003    0.659091    0.726920   \n",
       "\n",
       "    AUC-Fold-5  \n",
       "0     0.500000  \n",
       "1     0.365950  \n",
       "2     0.711828  \n",
       "3     0.554839  \n",
       "4     0.500000  \n",
       "5     0.613620  \n",
       "6     0.734050  \n",
       "7     0.407885  \n",
       "8     0.662366  \n",
       "9     0.521864  \n",
       "10    0.500000  \n",
       "11    0.788172  \n",
       "12    0.628315  \n",
       "13    0.707168  \n",
       "14    0.369892  \n",
       "15    0.698208  \n",
       "16    0.459498  \n",
       "17    0.414337  \n",
       "18    0.617204  \n",
       "19    0.410753  \n",
       "20    0.767025  \n",
       "21    0.847312  \n",
       "22    0.829391  \n",
       "23    0.478853  \n",
       "24    0.630824  \n",
       "25    0.612903  \n",
       "26    0.725448  \n",
       "27    0.767384  \n",
       "28    0.856989  \n",
       "29    0.500000  \n",
       "30    0.697849  \n",
       "31    0.705018  \n",
       "32    0.712903  \n",
       "33    0.500000  \n",
       "34    0.726165  "
      ]
     },
     "execution_count": 1036,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_feat_importance_df = pd.DataFrame() \n",
    "all_fts = processed_dataset.columns.values[4:]\n",
    "auc_feat_importance_df[f'Feature'] = all_fts \n",
    "auc_feat_importance_df[f'Selected'] = [0 for _ in all_fts] \n",
    "# auc_feat_importance_df.set_index([f'Feature'])\n",
    "\n",
    "for feat_ss in range(1, (splitting_crieteria[0][0]+1)):\n",
    "#     print('---> ', feat_ss, auc_feat_importance_df)\n",
    "    fts = ts_model[feat_ss][6].feature_names \n",
    "    auc_imp = (ts_model[feat_ss][6].feature_importance_scores).iloc[:,2].values.tolist() \n",
    "    ffts_n = auc_feat_importance_df[f'Feature'].values.tolist() #(ts_model[1][6].feature_importance_scores).iloc[:,0].values.tolist() \n",
    "    imp_val = [auc_imp[fts.index(f)] if f in fts else 0.5 for f in ffts_n]\n",
    "#     imp_val = [auc_imp[fts.index(f)] for f in ffts_n]   \n",
    "    \n",
    "    for f in fts: # Change the status of the selected features if this feature has been used in the experiment of not  \n",
    "        auc_feat_importance_df.loc[auc_feat_importance_df[f'Feature']==f, f'Selected'] = 1  \n",
    "    auc_feat_importance_df[f'AUC-Fold-{feat_ss}'] = imp_val  \n",
    "    \n",
    "#     print(auc_imp)\n",
    "    \n",
    "auc_feat_importance_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "id": "89bbda64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Results/_Classification/ML13003/',\n",
       " './Results/_Classification/ML13003/auc_feat_importance.csv')"
      ]
     },
     "execution_count": 1037,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_save_path, f'{result_save_path}auc_feat_importance.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "id": "43801423",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_feat_importance_df.to_csv(f'{result_save_path}auc_feat_importance.csv', index=False)  \n",
    "# auc_feat_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "id": "b1a4b540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Selected</th>\n",
       "      <th>AUC-Fold-1</th>\n",
       "      <th>AUC-Fold-2</th>\n",
       "      <th>AUC-Fold-3</th>\n",
       "      <th>AUC-Fold-4</th>\n",
       "      <th>AUC-Fold-5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Class</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.748397</td>\n",
       "      <td>0.680654</td>\n",
       "      <td>0.642136</td>\n",
       "      <td>0.672304</td>\n",
       "      <td>0.365950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.739583</td>\n",
       "      <td>0.690612</td>\n",
       "      <td>0.642496</td>\n",
       "      <td>0.646230</td>\n",
       "      <td>0.711828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.490785</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.462297</td>\n",
       "      <td>0.554839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W-&gt;S4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>W-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.657853</td>\n",
       "      <td>0.343528</td>\n",
       "      <td>0.405483</td>\n",
       "      <td>0.403453</td>\n",
       "      <td>0.613620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S1-&gt;W</td>\n",
       "      <td>1</td>\n",
       "      <td>0.665465</td>\n",
       "      <td>0.643670</td>\n",
       "      <td>0.310606</td>\n",
       "      <td>0.600423</td>\n",
       "      <td>0.734050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S1-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.453926</td>\n",
       "      <td>0.433144</td>\n",
       "      <td>0.680375</td>\n",
       "      <td>0.628964</td>\n",
       "      <td>0.407885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S1-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.636619</td>\n",
       "      <td>0.671053</td>\n",
       "      <td>0.703463</td>\n",
       "      <td>0.630726</td>\n",
       "      <td>0.662366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S1-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.497866</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.507400</td>\n",
       "      <td>0.521864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S1-&gt;S4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>S1-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.684696</td>\n",
       "      <td>0.686700</td>\n",
       "      <td>0.715729</td>\n",
       "      <td>0.698379</td>\n",
       "      <td>0.788172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>S2-&gt;W</td>\n",
       "      <td>1</td>\n",
       "      <td>0.719551</td>\n",
       "      <td>0.672831</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.660677</td>\n",
       "      <td>0.628315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>S2-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.758013</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.755411</td>\n",
       "      <td>0.772023</td>\n",
       "      <td>0.707168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>S2-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.595353</td>\n",
       "      <td>0.372333</td>\n",
       "      <td>0.698413</td>\n",
       "      <td>0.680761</td>\n",
       "      <td>0.369892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>S2-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.332131</td>\n",
       "      <td>0.667496</td>\n",
       "      <td>0.708153</td>\n",
       "      <td>0.323820</td>\n",
       "      <td>0.698208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>S2-&gt;S4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.455548</td>\n",
       "      <td>0.458514</td>\n",
       "      <td>0.467583</td>\n",
       "      <td>0.459498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>S2-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.357372</td>\n",
       "      <td>0.370555</td>\n",
       "      <td>0.386003</td>\n",
       "      <td>0.365046</td>\n",
       "      <td>0.414337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>S3-&gt;W</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492388</td>\n",
       "      <td>0.394381</td>\n",
       "      <td>0.384921</td>\n",
       "      <td>0.571177</td>\n",
       "      <td>0.617204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>S3-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.590545</td>\n",
       "      <td>0.549787</td>\n",
       "      <td>0.419913</td>\n",
       "      <td>0.494715</td>\n",
       "      <td>0.410753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>S3-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783253</td>\n",
       "      <td>0.772048</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.737491</td>\n",
       "      <td>0.767025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>S3-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.740785</td>\n",
       "      <td>0.774893</td>\n",
       "      <td>0.779221</td>\n",
       "      <td>0.766032</td>\n",
       "      <td>0.847312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>S3-&gt;S4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.808093</td>\n",
       "      <td>0.801209</td>\n",
       "      <td>0.795815</td>\n",
       "      <td>0.732558</td>\n",
       "      <td>0.829391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>S3-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.568510</td>\n",
       "      <td>0.449502</td>\n",
       "      <td>0.518038</td>\n",
       "      <td>0.434813</td>\n",
       "      <td>0.478853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>S4-&gt;W</td>\n",
       "      <td>1</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.392959</td>\n",
       "      <td>0.445527</td>\n",
       "      <td>0.586681</td>\n",
       "      <td>0.630824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>S4-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.661859</td>\n",
       "      <td>0.644026</td>\n",
       "      <td>0.617965</td>\n",
       "      <td>0.639535</td>\n",
       "      <td>0.612903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>S4-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>0.733642</td>\n",
       "      <td>0.648990</td>\n",
       "      <td>0.310430</td>\n",
       "      <td>0.725448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>S4-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.777244</td>\n",
       "      <td>0.762091</td>\n",
       "      <td>0.736291</td>\n",
       "      <td>0.752290</td>\n",
       "      <td>0.767384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>S4-&gt;S4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.913862</td>\n",
       "      <td>0.849218</td>\n",
       "      <td>0.775613</td>\n",
       "      <td>0.830162</td>\n",
       "      <td>0.856989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>S4-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>REM-&gt;W</td>\n",
       "      <td>1</td>\n",
       "      <td>0.729567</td>\n",
       "      <td>0.769915</td>\n",
       "      <td>0.684343</td>\n",
       "      <td>0.688865</td>\n",
       "      <td>0.697849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>REM-&gt;S1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753205</td>\n",
       "      <td>0.735420</td>\n",
       "      <td>0.638528</td>\n",
       "      <td>0.677237</td>\n",
       "      <td>0.705018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>REM-&gt;S2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.332532</td>\n",
       "      <td>0.708748</td>\n",
       "      <td>0.398990</td>\n",
       "      <td>0.638478</td>\n",
       "      <td>0.712903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>REM-&gt;S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>REM-&gt;REM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.778446</td>\n",
       "      <td>0.766003</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.726920</td>\n",
       "      <td>0.726165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature  Selected  AUC-Fold-1  AUC-Fold-2  AUC-Fold-3  AUC-Fold-4  \\\n",
       "0      Class         0    0.500000    0.500000    0.500000    0.500000   \n",
       "1      W->S1         1    0.748397    0.680654    0.642136    0.672304   \n",
       "2      W->S2         1    0.739583    0.690612    0.642496    0.646230   \n",
       "3      W->S3         1    0.490785    0.472973    0.500000    0.462297   \n",
       "4      W->S4         1    0.500000    0.500000    0.500000    0.500000   \n",
       "5     W->REM         1    0.657853    0.343528    0.405483    0.403453   \n",
       "6      S1->W         1    0.665465    0.643670    0.310606    0.600423   \n",
       "7     S1->S1         1    0.453926    0.433144    0.680375    0.628964   \n",
       "8     S1->S2         1    0.636619    0.671053    0.703463    0.630726   \n",
       "9     S1->S3         1    0.500000    0.497866    0.507937    0.507400   \n",
       "10    S1->S4         1    0.500000    0.500000    0.500000    0.500000   \n",
       "11   S1->REM         1    0.684696    0.686700    0.715729    0.698379   \n",
       "12     S2->W         1    0.719551    0.672831    0.702381    0.660677   \n",
       "13    S2->S1         1    0.758013    0.763158    0.755411    0.772023   \n",
       "14    S2->S2         1    0.595353    0.372333    0.698413    0.680761   \n",
       "15    S2->S3         1    0.332131    0.667496    0.708153    0.323820   \n",
       "16    S2->S4         1    0.437500    0.455548    0.458514    0.467583   \n",
       "17   S2->REM         1    0.357372    0.370555    0.386003    0.365046   \n",
       "18     S3->W         1    0.492388    0.394381    0.384921    0.571177   \n",
       "19    S3->S1         1    0.590545    0.549787    0.419913    0.494715   \n",
       "20    S3->S2         1    0.783253    0.772048    0.746753    0.737491   \n",
       "21    S3->S3         1    0.740785    0.774893    0.779221    0.766032   \n",
       "22    S3->S4         1    0.808093    0.801209    0.795815    0.732558   \n",
       "23   S3->REM         1    0.568510    0.449502    0.518038    0.434813   \n",
       "24     S4->W         1    0.453125    0.392959    0.445527    0.586681   \n",
       "25    S4->S1         1    0.661859    0.644026    0.617965    0.639535   \n",
       "26    S4->S2         1    0.744792    0.733642    0.648990    0.310430   \n",
       "27    S4->S3         1    0.777244    0.762091    0.736291    0.752290   \n",
       "28    S4->S4         1    0.913862    0.849218    0.775613    0.830162   \n",
       "29   S4->REM         1    0.500000    0.486486    0.500000    0.515152   \n",
       "30    REM->W         1    0.729567    0.769915    0.684343    0.688865   \n",
       "31   REM->S1         1    0.753205    0.735420    0.638528    0.677237   \n",
       "32   REM->S2         1    0.332532    0.708748    0.398990    0.638478   \n",
       "33   REM->S3         1    0.500000    0.500000    0.500000    0.500000   \n",
       "34  REM->REM         1    0.778446    0.766003    0.659091    0.726920   \n",
       "\n",
       "    AUC-Fold-5  \n",
       "0     0.500000  \n",
       "1     0.365950  \n",
       "2     0.711828  \n",
       "3     0.554839  \n",
       "4     0.500000  \n",
       "5     0.613620  \n",
       "6     0.734050  \n",
       "7     0.407885  \n",
       "8     0.662366  \n",
       "9     0.521864  \n",
       "10    0.500000  \n",
       "11    0.788172  \n",
       "12    0.628315  \n",
       "13    0.707168  \n",
       "14    0.369892  \n",
       "15    0.698208  \n",
       "16    0.459498  \n",
       "17    0.414337  \n",
       "18    0.617204  \n",
       "19    0.410753  \n",
       "20    0.767025  \n",
       "21    0.847312  \n",
       "22    0.829391  \n",
       "23    0.478853  \n",
       "24    0.630824  \n",
       "25    0.612903  \n",
       "26    0.725448  \n",
       "27    0.767384  \n",
       "28    0.856989  \n",
       "29    0.500000  \n",
       "30    0.697849  \n",
       "31    0.705018  \n",
       "32    0.712903  \n",
       "33    0.500000  \n",
       "34    0.726165  "
      ]
     },
     "execution_count": 1039,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_feat_importance_df = pd.read_csv(f'{result_save_path}auc_feat_importance.csv') \n",
    "auc_feat_importance_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f7d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf45fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5857c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea7fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a69f09ec",
   "metadata": {},
   "source": [
    "##### Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "5742875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_model_scores_df = ts_model_scores_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93222a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_map = {'n': 0, 'ins': 1, 'narco': 2, 'nfle': 3, 'plm': 4, 'rbd': 5}\n",
    "\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "81ad5094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(list(label_map.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "id": "7f14c6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': 0, 'dis': 1}"
      ]
     },
     "execution_count": 1040,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map\n",
    "\n",
    "label_map = {'n': 0, 'dis': 1}\n",
    "\n",
    "label_map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "id": "2fa20f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_No</th>\n",
       "      <th>Model_No</th>\n",
       "      <th>method</th>\n",
       "      <th>model</th>\n",
       "      <th>model_parameters</th>\n",
       "      <th>model_scores</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2, 'n_estim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[19, 0], [0, 1]]</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2, 'n_estim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[7, 0], [1, 11]]</td>\n",
       "      <td>94.737</td>\n",
       "      <td>100.0</td>\n",
       "      <td>91.667</td>\n",
       "      <td>91.667</td>\n",
       "      <td>100.000</td>\n",
       "      <td>95.652</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2, 'n_estim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[3, 0], [0, 16]]</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2, 'n_estim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[2, 0], [0, 16]]</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2, 'n_estim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[8, 6], [0, 4]]</td>\n",
       "      <td>66.667</td>\n",
       "      <td>40.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>57.143</td>\n",
       "      <td>57.143</td>\n",
       "      <td>0.786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Test_No  Model_No                                             method  \\\n",
       "5         1         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "13        2         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "21        3         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "29        4         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "37        5         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "\n",
       "                                                model  \\\n",
       "5   (DecisionTreeClassifier(max_depth=2, max_featu...   \n",
       "13  (DecisionTreeClassifier(max_depth=2, max_featu...   \n",
       "21  (DecisionTreeClassifier(max_depth=2, max_featu...   \n",
       "29  (DecisionTreeClassifier(max_depth=2, max_featu...   \n",
       "37  (DecisionTreeClassifier(max_depth=2, max_featu...   \n",
       "\n",
       "                                     model_parameters  model_scores  \\\n",
       "5   {'criterion': 'gini', 'max_depth': 2, 'n_estim...           NaN   \n",
       "13  {'criterion': 'gini', 'max_depth': 2, 'n_estim...           NaN   \n",
       "21  {'criterion': 'gini', 'max_depth': 2, 'n_estim...           NaN   \n",
       "29  {'criterion': 'gini', 'max_depth': 2, 'n_estim...           NaN   \n",
       "37  {'criterion': 'gini', 'max_depth': 2, 'n_estim...           NaN   \n",
       "\n",
       "     confusion_matrix  accuracy  precision   recall  sensitivity  specificity  \\\n",
       "5   [[19, 0], [0, 1]]   100.000      100.0  100.000      100.000      100.000   \n",
       "13  [[7, 0], [1, 11]]    94.737      100.0   91.667       91.667      100.000   \n",
       "21  [[3, 0], [0, 16]]   100.000      100.0  100.000      100.000      100.000   \n",
       "29  [[2, 0], [0, 16]]   100.000      100.0  100.000      100.000      100.000   \n",
       "37   [[8, 6], [0, 4]]    66.667       40.0  100.000      100.000       57.143   \n",
       "\n",
       "    f1_score  roc_auc  \n",
       "5    100.000    1.000  \n",
       "13    95.652    0.958  \n",
       "21   100.000    1.000  \n",
       "29   100.000    1.000  \n",
       "37    57.143    0.786  "
      ]
     },
     "execution_count": 1041,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_model_scores_df[(ts_model_scores_df['Model_No']==6)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "id": "1d7f53a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([[19, 0], [0, 1]]), list([[7, 0], [1, 11]]),\n",
       "       list([[3, 0], [0, 16]]), list([[2, 0], [0, 16]]),\n",
       "       list([[8, 6], [0, 4]])], dtype=object)"
      ]
     },
     "execution_count": 1042,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_conf_mat = ts_model_scores_df[(ts_model_scores_df['Model_No']==6)]['confusion_matrix'].values \n",
    "# all_conf_mat = [eval(cf) for cf in all_conf_mat] \n",
    "all_conf_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "id": "bc92e48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 1043,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_conf_mat[0]), len(all_conf_mat[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "id": "8d98e42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> [[45. 49.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[39.,  6.],\n",
       "       [ 1., 48.]])"
      ]
     },
     "execution_count": 1044,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_conf_mat = np.zeros( ( len(all_conf_mat[0]), len(all_conf_mat[0][0]) ) ) \n",
    "for i, arr in enumerate(all_conf_mat):\n",
    "    arr = np.array(arr) \n",
    "    if i==0:\n",
    "        final_conf_mat = np.zeros( (arr.shape[0], arr.shape[1]) ) \n",
    "#     print(type(arr), arr)\n",
    "    final_conf_mat += arr\n",
    "    \n",
    "# tot_counts = np.array( np.sum(final_conf_mat, axis=1) ) \n",
    "tot_counts = np.sum(final_conf_mat, axis=1).reshape(1,final_conf_mat.shape[1]) \n",
    "print(type(tot_counts), tot_counts) \n",
    "final_conf_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "id": "4d05032e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45, 49],\n",
       "       [39,  6],\n",
       "       [ 1, 48]])"
      ]
     },
     "execution_count": 1045,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_conf = np.concatenate( (tot_counts, final_conf_mat), axis=0).astype(int) \n",
    "tmp_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "id": "502d5d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>dis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        n  dis\n",
       "Total  45   49\n",
       "n      39    6\n",
       "dis     1   48"
      ]
     },
     "execution_count": 1046,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_conf_df = pd.DataFrame(tmp_conf, columns=list(label_map.keys()), index=['Total']+list(label_map.keys())) \n",
    "tmp_conf_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "id": "8d84b99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Results/_Classification/ML13003/',\n",
       " './Results/_Classification/ML13003/combined_conf_mat.csv')"
      ]
     },
     "execution_count": 1047,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_save_path, f'{result_save_path}combined_conf_mat.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "id": "d868be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_conf_df.to_csv(f'{result_save_path}combined_conf_mat.csv')  \n",
    "# tmp_conf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "id": "be0abf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>dis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        n  dis\n",
       "Total  45   49\n",
       "n      39    6\n",
       "dis     1   48"
      ]
     },
     "execution_count": 1049,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_conf_df = pd.read_csv(f'{result_save_path}combined_conf_mat.csv', index_col='Unnamed: 0') \n",
    "tmp_conf_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "id": "e2f5d536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\aliem\\\\OneDrive - Deakin University\\\\_MyResearch\\\\PhD_Research\\\\HML_IHC_Sleep_Data_Analysis'"
      ]
     },
     "execution_count": 1050,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "eb121dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import ttest_ind, f_oneway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a9ef0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379eceb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2ab5133",
   "metadata": {},
   "source": [
    "# ML Classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "038cc609",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data preprocessor class \n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class DataPreprocessor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def calculate_p_and_auc_for_feature(self, feat_data, label_data, binary_class=True): \n",
    "        # Extract the independent variable and dependent variable as dataframe and series \n",
    "        X = feat_data.copy()  # Replace 'independent_variable' with your column name\n",
    "        y = label_data.copy()  # Replace 'dependent_variable' with your column name\n",
    "        # print(X, y) \n",
    "        #print(\"111 Binary classification?\", binary_class)\n",
    "\n",
    "        # Perform a one-way ANOVA and calculate the p-value\n",
    "        p_value = 1.0\n",
    "        if binary_class:\n",
    "            _, p_value = ttest_ind(X[y==0], X[y==1])  # Assuming binary classification \n",
    "            #print(\"222 Binary classification?\", binary_class)\n",
    "        else: \n",
    "            groups = [X[y == label] for label in np.unique(y)] # For multiclass classification \n",
    "            _, p_value = f_oneway(*groups)\n",
    "            #print(\"222 Not binary classification?\", binary_class)\n",
    "        p_value = p_value[0] \n",
    "\n",
    "        # Display the p-value\n",
    "        #print(\"P-value:\", p_value)\n",
    "\n",
    "        # Encode the target variable - For multiclass \n",
    "        if not binary_class: \n",
    "            label_encoder = LabelEncoder()\n",
    "            y = label_encoder.fit_transform(y)\n",
    "            #print(\"333 Not binary classification?\", binary_class)\n",
    "\n",
    "        # Fit a logistic regression model and calculate the AUC\n",
    "        model = None \n",
    "        #if binary_class:\n",
    "        #    model = LogisticRegression()\n",
    "        #else:\n",
    "        #    model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "        # model = LogisticRegression()\n",
    "        # model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "        model = SVC(probability=True)\n",
    "        # model = SVC(C=1.0, random_state=1, kernel='linear', probability=True)\n",
    "        model.fit(X, y)\n",
    "        y_pred_proba = model.predict_proba(X)\n",
    "        if binary_class: \n",
    "            y_pred_proba = y_pred_proba[:, 1]\n",
    "            #print(\"444 Binary classification?\", binary_class)\n",
    "\n",
    "        # print(y_pred_proba) \n",
    "        auc = 0.0 \n",
    "        if binary_class:\n",
    "            auc = roc_auc_score(y, y_pred_proba)\n",
    "            #print(\"555 Binary classification?\", binary_class)\n",
    "        else:\n",
    "            auc = roc_auc_score(y, y_pred_proba, multi_class='ovr')\n",
    "            #print(\"555 Not binary classification?\", binary_class)\n",
    "\n",
    "        # Display the AUC\n",
    "        #print(\"AUC:\", auc)\n",
    "        return p_value, auc \n",
    "    \n",
    "    def calculate_p_and_auc_for_dataset(self, all_feats_df, label_df, binary_class=True): \n",
    "        feat_cols = all_feats_df.columns.values.tolist() \n",
    "\n",
    "        all_p_list = [] \n",
    "        all_auc_list = [] \n",
    "        for ft in feat_cols:\n",
    "            feat_data = all_feats_df[[ft]].copy() \n",
    "            label_data = label_df.copy() \n",
    "            # print(\"HHHHHH\", feat_data.shape, type(feat_data), label_data.shape, type(label_data), binary_class)\n",
    "            p, auc = self.calculate_p_and_auc_for_feature(feat_data, label_data, binary_class=binary_class) \n",
    "            all_p_list.append(p) \n",
    "            all_auc_list.append(auc) \n",
    "\n",
    "        all_p_and_auc_df = pd.DataFrame( {\"Features\": feat_cols, f\"P_Value_{'bin' if binary_class else 'multi'}\": all_p_list, f\"AUC_{'bin' if binary_class else 'multi'}\": all_auc_list} )    \n",
    "        return all_p_and_auc_df \n",
    "    \n",
    "    def get_selected_feature_list_based_on_PAUC(self, tmp_df, p_threshold=0.05, auc_threshold=0.5, sort=False, binary_class=True): \n",
    "        cols = tmp_df['Features'].values.tolist() \n",
    "        if p_threshold:\n",
    "            tmp_df = tmp_df[(tmp_df[f\"P_Value_{'bin' if binary_class else 'multi'}\"]<p_threshold)]\n",
    "        if auc_threshold:\n",
    "            tmp_df = tmp_df[(tmp_df[f\"AUC_{'bin' if binary_class else 'multi'}\"]>auc_threshold)]\n",
    "        if sort:\n",
    "            tmp_df = tmp_df.sort_values([f\"P_Value_{'bin' if binary_class else 'multi'}\", f\"AUC_{'bin' if binary_class else 'multi'}\"], ascending = [True, False])\n",
    "        selected_features = tmp_df['Features'].values.tolist() \n",
    "        return selected_features\n",
    "    \n",
    "    def select_pandauc_based_features(self, all_feats_df, label_df, binary_class=True, p_threshold=None, auc_threshold=None, sort=False): \n",
    "        tmp_df = self.calculate_p_and_auc_for_dataset(all_feats_df, label_df, binary_class=binary_class)\n",
    "        selected_features = self.get_selected_feature_list_based_on_PAUC(tmp_df, p_threshold=p_threshold, auc_threshold=auc_threshold, sort=sort, binary_class=binary_class)        \n",
    "        return selected_features, tmp_df \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "603cc81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom splitter class \n",
    "import math\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "# class MyCustomSplitter(BaseCrossValidator):\n",
    "class MyCustomSplitter():\n",
    "#     def __init__(self, n_splits):\n",
    "#         self.n_splits = n_splits\n",
    "        \n",
    "#     def set_criteria(self, splt_cri, groups=[['n'], ['SC', 'ST']]):\n",
    "#         self.splt_cri = splt_cri\n",
    "#         self.groups = groups\n",
    "        \n",
    "    def __init__(self, splt_cri, groups=[['n'], ['SC', 'ST']]):\n",
    "        self.splt_cri = splt_cri\n",
    "        self.groups = groups\n",
    "\n",
    "    def split(self, x, y=None):\n",
    "        groups = self.groups \n",
    "        fold = self.splt_cri[0] \n",
    "        percent = self.splt_cri[1]\n",
    "        y = np.array(y) \n",
    "        unique_y = np.unique(y)\n",
    "        num_y = len(unique_y)\n",
    "        y_indices = np.arange(num_y)\n",
    "        # print('AAAA--->> ', unique_y, y_indices)\n",
    "\n",
    "        remain_x = [i for i,v in enumerate(x)]\n",
    "        all_filterred_x = []\n",
    "        for grp in groups:\n",
    "#             print('Group', grp)\n",
    "            tmp_filterred_x = [] \n",
    "            for it in grp: \n",
    "                gg = f\"^{it}\\d\"\n",
    "                r = re.compile(gg) \n",
    "                filterred_x = list(filter(r.match, x))\n",
    "                # print('BBBB--->> ', gg, filterred_x) \n",
    "                filterred_x_ind = [i for i,v in enumerate(x) if v in filterred_x]\n",
    "                tmp_filterred_x.extend(filterred_x_ind) \n",
    "                # tmp_filterred_x.extend(filterred_x) \n",
    "            remain_x = [i for i in remain_x if i not in tmp_filterred_x] \n",
    "            all_filterred_x.append(tmp_filterred_x) \n",
    "            # print('CCCC--->> ', all_filterred_x)\n",
    "            # print('222--->', grp, remain_x) \n",
    "\n",
    "        # print('--->', remain_x) \n",
    "        remain_x_ind = [i for i,v in enumerate(x) if i in remain_x]\n",
    "        all_filterred_x.append(remain_x_ind) \n",
    "        # all_filterred_x.append(remain_x)     \n",
    "        all_dat = [item for row in all_filterred_x for item in row]\n",
    "        # print('DDDD--->> ', all_filterred_x, all_dat)\n",
    "\n",
    "        num_groups = len(all_filterred_x) \n",
    "        groups_item_len = [len(it) for it in all_filterred_x] \n",
    "        groups_item_ratio = [int(it/fold) if (it/fold)==int(it/fold) else int(it/fold+1) for it in groups_item_len] \n",
    "        # print('EEEE--->> ', num_groups, groups_item_len, groups_item_ratio)\n",
    "\n",
    "        main_grps = [it//fold  for it in groups_item_len]  # math.floor(it/fold) \n",
    "        ext_grps = [it%fold for it in groups_item_len] \n",
    "        # print('FFFF--->> ', main_grps, ext_grps)  \n",
    "\n",
    "        all_fold_values = [[] for _ in range(fold)] \n",
    "        for i, dat in enumerate(all_fold_values):\n",
    "            test_dat = [] \n",
    "            for l, (j,k,fd) in enumerate(zip(main_grps, ext_grps, all_filterred_x)): \n",
    "                # print('---->> ', i, j, k, i*j, i*j+j*1, i<k, fold*j+i)\n",
    "                dd = fd[i*j : i*j+j*1]\n",
    "                ex_dd = [fd[fold*j+i]] if i<k else []\n",
    "                dd.extend(ex_dd)\n",
    "                # print('---->> ', i, j, k, i*j, i*j+j*1, i<k, fold*j+i, dd, ex_dd)\n",
    "                test_dat.extend( dd ) \n",
    "            test_dat = list(set(test_dat))\n",
    "            train_dat = list( set(all_dat)-set(test_dat) ) \n",
    "            # print('GGGG--->> ', test_dat, train_dat)\n",
    "            yield train_dat, test_dat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7eb2001",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ML Classifier class \n",
    "##### import random\n",
    "import multiprocessing as mp\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, ShuffleSplit, LeavePOut, KFold, ParameterGrid\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier \n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# HumachLab_ML_CLassifiers     \n",
    "\n",
    "\n",
    "\n",
    "# ### All models' implementation\n",
    "\n",
    "class HumachLab_ML_CLassifiers:\n",
    "    \n",
    "    def print_message(self):\n",
    "#         ---------------------------------------------------------------------------------------------------\n",
    "#         ===================================================================================================\n",
    "#         ###################################################################################################\n",
    "#         ***************************************************************************************************\n",
    "        self.logger.info(f\"Hello from HumachLab_ML_CLassifiers class\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, logger, directory, dataset, class_name, label_map, metadata_column, split_column, random_state_value, split_balance_pattern, check_result=False): \n",
    "        self.logger = logger \n",
    "        self.directory = directory\n",
    "        self.dataset = dataset \n",
    "        self.class_name = class_name\n",
    "        self.label_map = label_map\n",
    "        self.metadata_column = metadata_column\n",
    "        self.split_column = split_column \n",
    "        self.is_multiclass = True if len(dataset[class_name].unique().tolist())>2 else False\n",
    "        self.random_state_value = random_state_value\n",
    "        self.split_balance_pattern = split_balance_pattern\n",
    "        if not check_result:\n",
    "            self.experiment_info = {\n",
    "                'logger':logger, 'directory':directory, 'dataset_size':dataset.shape, 'dataset_columns':dataset.columns.values.tolist(), 'metadata_column':metadata_column, \n",
    "                'class_name':class_name, 'label_map':label_map, 'split_column':split_column, 'total_unique_classes':dataset[class_name].value_counts().keys().tolist(), \n",
    "                'total_unique_classes':dataset[class_name].value_counts().values.tolist()\n",
    "                }\n",
    "        \n",
    "        self.best_model_scoring_metrics=[ML_Performace_Metrics.RECL, ML_Performace_Metrics.F1SCR] \n",
    "        \n",
    "        self.logger.info(f\"\"\"\n",
    "        Object is initialised with the following properties: \n",
    "        ###################################################################################################\n",
    "        Dataset size: {self.dataset.shape}, Columns: {self.dataset.columns.values.tolist()}\n",
    "        Target class column name: {self.class_name}\n",
    "        Metadata column names: {self.metadata_column}\n",
    "        Dataset split column on which the training and test sets will be devided: {self.split_column}\n",
    "        Is multi-class classification: {self.is_multiclass}\n",
    "        \"\"\") \n",
    "        return  \n",
    "    \n",
    "    \n",
    "    def convert_list_to_string(self, lst):\n",
    "        lst = [str(l) for l in lst]        \n",
    "        return '* '.join(lst) \n",
    "    \n",
    "    \n",
    "    \n",
    "    def classify(self, should_use_params, splitting_crieteria, model_list, is_validate_models, result_save_path, exp_name, exp_detail, apply_feature_selection, custom_splitter):\n",
    "        self.splitting_crieteria = splitting_crieteria    ### for test & training (validation) splitting_crieteria (n): n=0 -loso, n>0 -n-fold, n<0 -shuffled random splitting with n% testing\n",
    "        self.model_list = model_list \n",
    "        self.should_use_params = should_use_params\n",
    "        self.is_validate_models = is_validate_models \n",
    "        self.result_save_path = result_save_path \n",
    "        self.exp_name = exp_name \n",
    "        self.exp_detail = exp_detail \n",
    "        self.is_binary_classification = not self.is_multiclass\n",
    "        self.apply_feature_selection = apply_feature_selection \n",
    "        self.selected_features = None \n",
    "        self.custom_splitter = custom_splitter\n",
    "        \n",
    "        # self.experiment_info['exp_name'] = exp_name \n",
    "        self.experiment_info.update(exp_detail)\n",
    "        self.experiment_info['apply_feature_selection'] = apply_feature_selection\n",
    "        #self.experiment_info['selected_features'] = self.selected_features \n",
    "        self.experiment_info['is_multiclass_classification'] = self.is_multiclass\n",
    "        self.experiment_info['model_list'] = model_list \n",
    "        self.experiment_info['should_use_params'] = should_use_params\n",
    "        self.experiment_info['is_validate_models'] = is_validate_models\n",
    "        self.experiment_info['result_save_path'] = result_save_path\n",
    "        self.experiment_info['random_state_value'] = self.random_state_value \n",
    "        self.experiment_info['custom_splitter'] = self.custom_splitter\n",
    "        self.experiment_info['split_balance_pattern'] = self.split_balance_pattern\n",
    "        tmp = splitting_crieteria[0] \n",
    "        self.experiment_info['test_split_crieteria'] = tmp \n",
    "        self.experiment_info['test_split_details'] = f'Leave-one-out' if tmp[0]==0 else (f'{tmp[0]}-fold cross validation' if (tmp[0]>0 and tmp[1]<=0) else f'{tmp[0]}-fold {tmp[1]}% random test splitting') \n",
    "        tmp = splitting_crieteria[1] \n",
    "        self.experiment_info['training_split_crieteria'] = tmp \n",
    "        self.experiment_info['training_split_details'] = f'Leave-one-out' if tmp[0]==0 else (f'{tmp[0]}-fold cross validation' if (tmp[0]>0 and tmp[1]<=0) else f'{tmp[0]}-fold {tmp[1]}% random test splitting') \n",
    "        self.experiment_info['model_selection_matrics'] = self.best_model_scoring_metrics \n",
    "                                                                                                  \n",
    "                                                                                                                      \n",
    "        self.logger.info(f\"\"\"\n",
    "        Classification is set with the following parameters: \n",
    "        ###################################################################################################\n",
    "        Splitting crieteria: {self.splitting_crieteria}\n",
    "        Test split: {f'Leave-one-out' if splitting_crieteria[0] [0]==0 else (f'{splitting_crieteria[0] [0]}-fold cross validation' if (splitting_crieteria[0] [0]>0 and splitting_crieteria[0] [1]<=0) else f'{splitting_crieteria[0] [0]}-fold {splitting_crieteria[0] [1]}% random test splitting') }\n",
    "        Training split: {f'Leave-one-out' if splitting_crieteria[1] [0]==0 else (f'{splitting_crieteria[1] [0]}-fold cross validation' if (splitting_crieteria[1] [0]>0 and splitting_crieteria[1] [1]<=0) else f'{splitting_crieteria[1] [0]}-fold {splitting_crieteria[1] [1]}% random test splitting') }\n",
    "        List of ML models that will be applied: {[mn.value for mn in self.model_list]}\n",
    "        Use parameters for model: {self.should_use_params}\n",
    "        Is validate the model (or only train): {self.is_validate_models} \n",
    "        Classification results will be saved in the directory: {self.result_save_path}\n",
    "        \"\"\") \n",
    "        all_exp_info_df = pd.DataFrame(self.experiment_info.items(), columns=['Information', 'Description']) \n",
    "        all_best_tr_model, all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_ts_model, all_ts_scores_df, all_ts_prediction_df, all_ts_fold_info_df  = self.test() \n",
    "        \n",
    "        self.save_results(self.directory, all_best_tr_model, all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_ts_model, all_ts_scores_df, all_ts_prediction_df, all_ts_fold_info_df, all_exp_info_df) \n",
    "        \n",
    "        return all_best_tr_model, all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_ts_model, all_ts_scores_df, all_ts_prediction_df, all_ts_fold_info_df, all_exp_info_df \n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_results(self, save_directory):\n",
    "        # './Results/_Classification/ML001/'\n",
    "        self.logger.info(f\"\"\"\n",
    "        Data is being loaded from: {save_directory}\n",
    "        \"\"\") \n",
    "        save_path = f\"{save_directory}all_tr_scores.csv\" \n",
    "        all_tr_scores_df = pd.read_csv(save_path, index_col=False) \n",
    "        \n",
    "        save_path = f\"{save_directory}all_tr_prediction.csv\" \n",
    "        all_tr_prediction_df = pd.read_csv(save_path, index_col=False) \n",
    "        \n",
    "        save_path = f\"{save_directory}all_ts_scores.csv\" \n",
    "        all_ts_scores_df = pd.read_csv(save_path, index_col=False) \n",
    "        \n",
    "        save_path = f\"{save_directory}all_ts_prediction.csv\" \n",
    "        all_ts_prediction_df = pd.read_csv(save_path, index_col=False)         \n",
    "        \n",
    "        save_path = f\"{save_directory}all_ts_fold_info.csv\" \n",
    "        all_ts_fold_info_df = pd.read_csv(save_path, index_col=False)         \n",
    "        \n",
    "        save_path = f\"{save_directory}all_exp_info.csv\" \n",
    "        all_exp_info_df = pd.read_csv(save_path, index_col=False)  \n",
    "        \n",
    "        new_save_directory = f\"{save_directory}/Models/\"\n",
    "        \n",
    "        save_path = f\"{new_save_directory}ts_model\" \n",
    "        all_ts_model = self.load_models_from_file(save_path, 'Test Models')\n",
    "        \n",
    "        save_path = f\"{new_save_directory}tr_model\" \n",
    "        all_tr_model = self.load_models_from_file(save_path, 'Training Models')\n",
    "        \n",
    "        save_path = f\"{new_save_directory}best_tr_model\" \n",
    "        all_best_tr_model = self.load_models_from_file(save_path, 'Best Training Models')\n",
    "        \n",
    "        return all_best_tr_model, all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_ts_model, all_ts_scores_df, all_ts_prediction_df, all_ts_fold_info_df, all_exp_info_df \n",
    "\n",
    "\n",
    "    def load_models_from_file(self, save_path, model_type):\n",
    "        models_dict = {} \n",
    "        \n",
    "        save_path = f'{save_path}*'\n",
    "        files = self.sort_string_list(glob.glob(save_path)) \n",
    "        files\n",
    "        selected_files = [[int(fn) for fn in f[len(save_path):].split('.')[0].split('_')] for f in files]\n",
    "        selected_files\n",
    "        \n",
    "        self.logger.info(f'Start retrieving {model_type} model from file...')\n",
    "        model_dict = {}  \n",
    "        for i, (ind, fl) in enumerate(zip(selected_files, files)):\n",
    "            if len(ind)==3:\n",
    "                tsi, tri, modi = ind\n",
    "                mod = None \n",
    "                with open(fl, \"rb\") as f:\n",
    "                    mod = pickle.load(f) \n",
    "                if tsi not in model_dict.keys(): \n",
    "                    model_dict[tsi] = {}\n",
    "                if tri not in model_dict[tsi].keys(): \n",
    "                    model_dict[tsi][tri] = {} \n",
    "                if modi not in model_dict[tsi][tri].keys(): \n",
    "                    model_dict[tsi][tri][modi] = mod \n",
    "            elif len(ind)==2:\n",
    "                tsi, modi = ind\n",
    "                mod = None \n",
    "                with open(fl, \"rb\") as f:\n",
    "                    mod = pickle.load(f) \n",
    "                if tsi not in model_dict.keys(): \n",
    "                    model_dict[tsi] = {}\n",
    "                if modi not in model_dict[tsi].keys(): \n",
    "                    model_dict[tsi][modi] = mod \n",
    "            else:\n",
    "                self.logger.info(f'Doesn\\'t identify {model_type} model file to retrieve...')\n",
    "        \n",
    "        model_dict\n",
    "        self.logger.info(f'Finish retrieving {model_type} model from file...')\n",
    "        return model_dict \n",
    "    \n",
    "    \n",
    "    \n",
    "    def sort_string_list(self, string_list):\n",
    "        ## ref: https://blog.codinghorror.com/sorting-for-humans-natural-sort-order/\n",
    "        \"\"\" Sort the given list in the way that humans expect.\n",
    "        \"\"\"\n",
    "        convert = lambda text: int(text) if text.isdigit() else text\n",
    "        alphanum_key = lambda key: [ convert(c.replace(\"_\",\"\")) for c in re.split('([0-9]+)', key) ]\n",
    "        string_list.sort( key=alphanum_key )\n",
    "        return string_list\n",
    "    \n",
    "    \n",
    "        \n",
    "    def save_results(self, save_directory, all_best_tr_model, all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_ts_model, all_ts_scores_df, all_ts_prediction_df, all_ts_fold_info_df, all_exp_info_df):\n",
    "        # './Results/_Classification/ML001/'\n",
    "        save_path = f\"{save_directory}all_tr_scores.csv\" \n",
    "        all_tr_scores_df.to_csv(save_path, index=False) \n",
    "        \n",
    "        save_path = f\"{save_directory}all_tr_prediction.csv\" \n",
    "        all_tr_prediction_df.to_csv(save_path, index=False) \n",
    "        \n",
    "        save_path = f\"{save_directory}all_ts_scores.csv\" \n",
    "        all_ts_scores_df.to_csv(save_path, index=False) \n",
    "        \n",
    "        save_path = f\"{save_directory}all_ts_prediction.csv\" \n",
    "        all_ts_prediction_df.to_csv(save_path, index=False) \n",
    "        \n",
    "        save_path = f\"{save_directory}all_ts_fold_info.csv\" \n",
    "        all_ts_fold_info_df.to_csv(save_path, index=False) \n",
    "        \n",
    "        save_path = f\"{save_directory}all_exp_info.csv\" \n",
    "        all_exp_info_df.to_csv(save_path, index=False) \n",
    "        \n",
    "        new_save_directory = self.create_directory(save_directory, 'Models')\n",
    "        \n",
    "        save_path = f\"{new_save_directory}ts_model\" \n",
    "        self.save_models_to_file(save_path, all_ts_model, 'Test Models')\n",
    "        \n",
    "        save_path = f\"{new_save_directory}tr_model\" \n",
    "        self.save_models_to_file(save_path, all_tr_model, 'Training Models')\n",
    "        \n",
    "        save_path = f\"{new_save_directory}best_tr_model\" \n",
    "        self.save_models_to_file(save_path, all_best_tr_model, 'Best Training Models')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def create_directory(self, path, dir_name): \n",
    "        new_directory = f\"{path}/{dir_name}/\"\n",
    "        \n",
    "        if (not os.path.exists(new_directory)):\n",
    "            try:\n",
    "                os.makedirs(new_directory, exist_ok = True)\n",
    "                print(f\"Directory successfully created at path: {new_directory}\") \n",
    "            except OSError as error:\n",
    "                print(f\"Directory cannot be created at path: {new_directory}\") \n",
    "        else:\n",
    "            print(f\"Directory already exists at path: {new_directory}\") \n",
    "            \n",
    "        return new_directory\n",
    "\n",
    "\n",
    "    def save_models_to_file(self, save_path, models_dict, model_type):\n",
    "        self.logger.info(f'Start saving model to file...')\n",
    "        if model_type=='Training Models':\n",
    "            for tsfi, ts_dat in models_dict.items():\n",
    "                for trfi, tr_dat in ts_dat.items():\n",
    "                    for modi, mod in tr_dat.items():\n",
    "                        new_save_path = f'{save_path}_{tsfi}_{trfi}_{modi}.dat'\n",
    "                        try:\n",
    "                            with open(new_save_path, 'wb') as f:\n",
    "                                pickle.dump(mod, f)\n",
    "                                print( f'{model_type} is written to the file: {new_save_path}\\n' )\n",
    "                        except:\n",
    "                            print( f'Problem creating {model_type} file: {new_save_path}\\n' )\n",
    "        else:\n",
    "            for tsfi, ts_dat in models_dict.items():\n",
    "                for modi, mod in ts_dat.items():\n",
    "                    new_save_path = f'{save_path}_{tsfi}_{modi}.dat'\n",
    "                    try:\n",
    "                        with open(new_save_path, 'wb') as f:\n",
    "                            pickle.dump(mod, f)\n",
    "                            print( f'{model_type} is written to the file: {new_save_path}\\n' )\n",
    "                    except:\n",
    "                        print( f'Problem creating {model_type} file: {new_save_path}\\n' )\n",
    "        self.logger.info(f'Finish saving model to file...')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def evaluate_test_data(self, ind, model_list, test_ids, best_tr_model):\n",
    "        self.logger.info(f\"\"\"\n",
    "        ### MODEL EVALUATION PHASE \n",
    "        EVALUATION {ind} START... XXXXX \n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        \"\"\") \n",
    "        #print(\"HHHHHH3333\", best_tr_model)\n",
    "        \n",
    "        X_test, y_test, meta_dat_df, _ = self._get_data_from_indices(test_ids, from_where=f'_TS_ts_{ind}')\n",
    "        #print(\"meta33->\", meta_dat_df)     \n",
    "#         ### Call features selection algorithm \n",
    "#         if self.selected_features: \n",
    "#             X_test = X_test[self.selected_features]\n",
    "        \n",
    "        \n",
    "        ts_score_df, ts_prediction_df = pd.DataFrame(), pd.DataFrame() \n",
    "        # all_ts_model, all_ts_scores_df, all_ts_prediction_df = {}, pd.DataFrame(), pd.DataFrame()\n",
    "        for (modi, model), classifier_method in zip(best_tr_model.items(), model_list) : \n",
    "            y_pred = model.predict(X_test) \n",
    "            y_pred_proba = model.predict_proba(X_test) \n",
    "            if modi==1:\n",
    "                # meta_dat_df.reset_index(drop=True, inplace=True) \n",
    "                ts_prediction_df = pd.concat([ts_prediction_df, meta_dat_df]) \n",
    "                ts_prediction_df.reset_index(drop=True, inplace=True) \n",
    "                ts_prediction_df[self.class_name] = y_test \n",
    "                \n",
    "            ts_prediction_df[f\"Prediction_{ML_Classifiers.get_short_form(str(classifier_method.value))}\"] = y_pred \n",
    "            for p_ind in range(y_pred_proba.shape[1]):\n",
    "                ts_prediction_df[f\"Prediction_Proba_{p_ind}_{ML_Classifiers.get_short_form(str(classifier_method.value))}\"] = y_pred_proba[:, p_ind].tolist()\n",
    "            \n",
    "            scores_df = self.calculate_model_scores(model, y_test, y_pred, y_pred_proba)\n",
    "            scores_df.insert(0, \"Model_No\", modi) \n",
    "            ts_score_df = pd.concat([ts_score_df, scores_df]) \n",
    "            \n",
    "        ts_score_df.insert(0, \"Test_No\", ind) \n",
    "        ts_prediction_df.insert(0, \"Test_No\", ind) \n",
    "        return ts_score_df, ts_prediction_df \n",
    "    \n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "        # splitter = self.get_data_splitter(self.splitting_crieteria[0], stratified=True, custom=True) #Test split cusomised: usually LOSO or 10-fold \n",
    "        # splitter = self.get_data_splitter(self.splitting_crieteria[0], stratified=True, custom=False) #Test split: usually LOSO or 10-fold \n",
    "        splitter = self.get_data_splitter(self.splitting_crieteria[0], stratified=True, custom=self.custom_splitter) #Test split cusomised: usually LOSO or 10-fold \n",
    "        split_data_list = self.dataset[self.split_column].values.tolist() \n",
    "        class_data_list = self.dataset[self.class_name].values.tolist() \n",
    "        \n",
    "        all_ts_model, all_ts_scores_df, all_ts_prediction_df, all_ts_fold_info_df = {}, pd.DataFrame(), pd.DataFrame(), pd.DataFrame()  \n",
    "        cum_best_tr_model, cum_tr_model, cum_tr_scores_df, cum_tr_prediction_df, cum_tr_fold_info_df = {}, {}, pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "        for tsi, (train_all_ids, test_ids) in enumerate(splitter.split(split_data_list, class_data_list)): \n",
    "            ### Train-test split: fold based \n",
    "#             ts_dat = self.dataset[self.split_column][test_ids].values.tolist() \n",
    "#             tr_all_dat = self.dataset[self.split_column][train_all_ids].values.tolist() \n",
    "#             ts_dat = self.dataset.loc[test_ids, self.split_column].values.tolist() \n",
    "#             tr_all_dat = self.dataset.loc[train_all_ids, self.split_column].values.tolist() \n",
    "            ts_dat = self.dataset.iloc[test_ids][self.split_column].values.tolist() \n",
    "            tr_all_dat = self.dataset.iloc[train_all_ids][self.split_column].values.tolist() \n",
    "            print('test- tsi, train_all_ids, test_ids, tr_all_dat, ts_dat', tsi, train_all_ids, test_ids, tr_all_dat, ts_dat) \n",
    "#             print(f\"QQQQQQQQQQQQQ {train_all_ids}, {self.dataset[self.split_column][train_all_ids].values.tolist()}, {self.dataset[self.split_column].values.tolist() }\")\n",
    "            ind = tsi+1 \n",
    "            self.selected_features = None ### Resetting feature selection list \n",
    "            self.logger.info(f\"\"\"\n",
    "            ### MODEL TEST PHASE \n",
    "            TEST {ind} START... XXXXX \n",
    "            ===================================================================================================\n",
    "            Test=> {len(ts_dat)} {(ts_dat)} \n",
    "            Training (Including Validation)=> {len(tr_all_dat)} {(tr_all_dat)} \n",
    "            \"\"\") \n",
    "            best_tr_model, all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_tr_fold_info_df = self.train(ind, model_list, train_all_ids, tr_splitting_crieteria=self.splitting_crieteria[1]) #tr_splitting_crieteria: <0 random split, >0 fold\n",
    "#             continue\n",
    "            cum_tr_model[ind] = all_tr_model \n",
    "            cum_best_tr_model[ind] = best_tr_model \n",
    "            if self.is_validate_models:\n",
    "                all_tr_scores_df.insert(0, \"Test_No\", ind) \n",
    "                all_tr_prediction_df.insert(0, \"Test_No\", ind) \n",
    "            all_tr_fold_info_df.insert(0, \"Test_No\", ind) \n",
    "            all_tr_fold_info_df.insert(4, \"Test\", [ts_dat]*all_tr_fold_info_df.shape[0]) \n",
    "            all_tr_fold_info_df.insert(4, \"Selected_Features\", [self.selected_features]*all_tr_fold_info_df.shape[0]) \n",
    "            \n",
    "            cum_tr_scores_df = pd.concat([cum_tr_scores_df, all_tr_scores_df]) \n",
    "            cum_tr_prediction_df = pd.concat([cum_tr_prediction_df, all_tr_prediction_df])\n",
    "            all_ts_fold_info_df = pd.concat([all_ts_fold_info_df, all_tr_fold_info_df])\n",
    "            \n",
    "            cum_tr_scores_df.reset_index(drop=True, inplace=True) \n",
    "            cum_tr_prediction_df.reset_index(drop=True, inplace=True) \n",
    "            all_ts_fold_info_df.reset_index(drop=True, inplace=True) \n",
    "            \n",
    "#             print(\"TTTT\", best_tr_model.keys(), best_tr_model, all_tr_scores_df.shape, all_tr_scores_df.columns, all_tr_prediction_df.shape, all_tr_prediction_df.columns) \n",
    "                        \n",
    "            ###############\n",
    "            ### Model evaluation with the test data using the best trained model  \n",
    "            all_ts_model[ind] = best_tr_model\n",
    "            ts_score_df, ts_prediction_df = self.evaluate_test_data(ind, model_list, test_ids, best_tr_model) \n",
    "            \n",
    "            all_ts_scores_df = pd.concat([all_ts_scores_df, ts_score_df]) \n",
    "            all_ts_prediction_df = pd.concat([all_ts_prediction_df, ts_prediction_df])\n",
    "            \n",
    "            all_ts_scores_df.reset_index(drop=True, inplace=True) \n",
    "            all_ts_prediction_df.reset_index(drop=True, inplace=True) \n",
    "        \n",
    "            self.logger.info(f\"\"\"\n",
    "            ===================================================================================================\n",
    "            TEST {ind} END...\n",
    "            \"\"\") \n",
    "            \n",
    "        ### Sorting scores\n",
    "#             print( 'TTRR', cum_tr_scores_df.columns.values.tolist(), cum_tr_prediction_df.columns.values.tolist() )\n",
    "        cum_tr_scores_df.sort_values(['Model_No', 'Test_No', 'Training_No'], ascending = [True, True, True], inplace=True)  \n",
    "        cum_tr_prediction_df.sort_values(['Test_No', 'Training_No'], ascending = [True, True], inplace=True)  \n",
    "\n",
    "#             print( 'TTSS', all_ts_scores_df.columns.values.tolist(), all_ts_prediction_df.columns.values.tolist() )\n",
    "        all_ts_scores_df.sort_values(['Model_No', 'Test_No'], ascending = [True, True], inplace=True) \n",
    "        all_ts_prediction_df.sort_values(['Test_No'], ascending = [True], inplace=True) \n",
    "        all_ts_fold_info_df.sort_values(['Model_No', 'Test_No', 'Training_No'], ascending = [True, True, True], inplace=True) \n",
    "        \n",
    "        all_ts_fold_info_df\n",
    "        \n",
    "        return cum_best_tr_model, cum_tr_model, cum_tr_scores_df, cum_tr_prediction_df, all_ts_model, all_ts_scores_df, all_ts_prediction_df, all_ts_fold_info_df  \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self, tsi, model_list, train_all_ids, tr_splitting_crieteria):           \n",
    "        # tr_splitter = self.get_data_splitter(tr_splitting_crieteria, stratified=True, custom=True) #Training split customised: usually 5-fold or Random 20% split\n",
    "        tr_splitter = self.get_data_splitter(tr_splitting_crieteria, stratified=True, custom=False) #Training split: usually 5-fold or Random 20% split\n",
    "        all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_tr_fold_info_df = {}, pd.DataFrame(), pd.DataFrame(), pd.DataFrame()  \n",
    "        best_tr_model, best_acc, best_rec, best_prec = {}, [], [], []  \n",
    "        class_data_list2 = self.dataset.iloc[train_all_ids][self.class_name].values.tolist() \n",
    "        split_data_list2 = self.dataset.iloc[train_all_ids][self.split_column].values.tolist() \n",
    "#         split_data_list2 = self.dataset[self.split_column][train_all_ids].values.tolist()\n",
    "#         class_data_list2 = self.dataset[self.class_name][train_all_ids].values.tolist() \n",
    "#         class_data_list2 = self.dataset.loc[train_all_ids, self.class_name].values.tolist() \n",
    "#         split_data_list2 = self.dataset.loc[train_all_ids, self.split_column].values.tolist()\n",
    "#         print(f\"QQQQQQQQQQQQQ 222222 {train_all_ids}, {self.dataset[self.split_column][train_all_ids].values.tolist()}, {self.dataset[self.split_column].values.tolist()}\")\n",
    "        # print('QQQQQQQQQQQ', split_data_list2, class_data_list) \n",
    "        for tri, (tmp_train_ids, tmp_val_ids) in enumerate(tr_splitter.split(split_data_list2, class_data_list2)): \n",
    "        # for tri, (tmp_train_ids, tmp_val_ids) in enumerate(tr_splitter.split(split_data_list2)): \n",
    "            \n",
    "#             if not should_validate: \n",
    "#                 tmp_train_ids = np.concatenate((tmp_train_ids, tmp_val_ids)) \n",
    "#             else:\n",
    "#                 tmp_train_ids = np.concatenate((tmp_train_ids, tmp_val_ids)) \n",
    "#                 tmp_val_ids = tmp_train_ids.copy() \n",
    "            tmp_train_ids = np.concatenate((tmp_train_ids, tmp_val_ids)) \n",
    "            tmp_val_ids = tmp_train_ids.copy() \n",
    "            \n",
    "            ### This tmp_train_ids and tmp_val_ids are indexes re-starting from 0 again, but we need to get the actual indexes in the dataset \n",
    "            xx = (np.array(split_data_list2)[tmp_val_ids]).tolist() \n",
    "            val_ids = self.dataset[self.dataset[self.split_column].isin(xx)].index.tolist()\n",
    "            xx = (np.array(split_data_list2)[tmp_train_ids]).tolist() \n",
    "            train_ids = self.dataset[self.dataset[self.split_column].isin(xx)].index.tolist()\n",
    "            \n",
    "            ### Validation-train split: random percentage based \n",
    "            val_dat = self.dataset[self.split_column][val_ids].values.tolist() \n",
    "            tr_dat = self.dataset[self.split_column][train_ids].values.tolist() \n",
    "#             val_dat = self.dataset.loc[val_ids, self.split_column].values.tolist() \n",
    "#             tr_dat = self.dataset.loc[train_ids, self.split_column].values.tolist() \n",
    "#             val_dat = self.dataset.iloc[val_ids][self.split_column].values.tolist() \n",
    "#             tr_dat = self.dataset.iloc[train_ids][self.split_column].values.tolist() \n",
    "            print('train- tri, tmp_train_ids, tmp_val_ids, tr_dat, val_dat', tri, tmp_train_ids, tmp_val_ids, tr_dat, val_dat) \n",
    "            ind = tri+1 \n",
    "            self.logger.info(f\"\"\"\n",
    "            *** MODEL TRAINING AND VALIDATION PHASE FOR TEST {tsi} \n",
    "            TRAINING {tri+1} START... XXXXX \n",
    "            ***************************************************************************************************\n",
    "            Validation=> {len(val_dat)} {(val_dat)} \n",
    "            Training=> {len(tr_dat)} {(tr_dat)} \n",
    "            \"\"\")            \n",
    "#             continue\n",
    "            all_mtr_model, all_mtr_scores_df, all_mtr_prediction_df, all_mtr_fold_info_df = self.train_models(model_list, train_ids, val_ids, tsi, ind)            \n",
    "            all_tr_model[ind] = all_mtr_model \n",
    "            if self.is_validate_models:\n",
    "                all_mtr_scores_df.insert(0, \"Training_No\", ind) \n",
    "                all_mtr_prediction_df.insert(0, \"Training_No\", ind) \n",
    "            all_mtr_fold_info_df.insert(0, \"Training_No\", ind) \n",
    "                \n",
    "            all_tr_scores_df = pd.concat([all_tr_scores_df, all_mtr_scores_df]) \n",
    "            all_tr_prediction_df = pd.concat([all_tr_prediction_df, all_mtr_prediction_df])\n",
    "            all_tr_fold_info_df = pd.concat([all_tr_fold_info_df, all_mtr_fold_info_df])\n",
    "            \n",
    "            all_tr_scores_df.reset_index(drop=True, inplace=True) \n",
    "            all_tr_scores_df.reset_index(drop=True, inplace=True)    \n",
    "            all_tr_fold_info_df.reset_index(drop=True, inplace=True)          \n",
    "            \n",
    "            self.logger.info(f\"\"\"\n",
    "            ---------------------------------------------------------------------------------------------------\n",
    "            Best model index calculation  \n",
    "            \"\"\")             \n",
    "#             print(\"PPPPP\", tri, ind, all_mtr_model.keys(), all_mtr_model, all_mtr_scores_df)\n",
    "            if tri==0:\n",
    "                best_tr_model = all_mtr_model.copy() \n",
    "                print(\"WWWWWWWWWWWWWWWWWWW\", ML_Performace_Metrics.RECL.value, all_mtr_scores_df)\n",
    "                #print(\"HHHH\", all_mtr_scores_df.columns)\n",
    "                # best_acc, best_prec, best_rec = all_mtr_scores_df[ML_Performace_Metrics.ACC.value], all_mtr_scores_df[ML_Performace_Metrics.PREC.value], all_mtr_scores_df[ML_Performace_Metrics.RECL.value] \n",
    "                best_rec = all_mtr_scores_df[ML_Performace_Metrics.RECL.value].values.tolist()  \n",
    "            else:                \n",
    "                for jj, mn in enumerate(model_list):\n",
    "                    mod_name = ML_Classifiers.get_short_form(str(mn.value))\n",
    "                    tm_df = all_mtr_scores_df[(all_mtr_scores_df[\"Model_Name\"]==mod_name)] \n",
    "                    new = tm_df[ML_Performace_Metrics.RECL.value].values.tolist()[0] \n",
    "                    if new>best_rec[jj]: \n",
    "                        best_rec[jj] = new \n",
    "                        best_tr_model[tm_df[\"Model_No\"].values.tolist()[0]] = all_mtr_model[tm_df[\"Model_No\"].values.tolist()[0]]  \n",
    "                        \n",
    "            \n",
    "            self.logger.info(f\"\"\"\n",
    "            ***************************************************************************************************\n",
    "            TRAINING {ind} END... \n",
    "            \"\"\") \n",
    "        return best_tr_model, all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_tr_fold_info_df \n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_models(self, model_list, train_ids, val_ids, ts_serial, tr_serial):\n",
    "        all_mtr_model, all_mtr_scores_df, all_mtr_prediction_df, all_mtr_fold_info_df = {}, pd.DataFrame(), pd.DataFrame(), pd.DataFrame() \n",
    "        for modi, classifier_method in enumerate(model_list): \n",
    "            ind = modi+1 \n",
    "            self.logger.info(f\"\"\"\n",
    "            *** ML MODEL FOR TEST:{ts_serial}, TRAINING:{tr_serial} AND MODEL: {ML_Classifiers.get_short_form(str(classifier_method.value))} \n",
    "            ---------------------------------------------------------------------------------------------------\n",
    "            \"\"\")\n",
    "            mtr_model, mtr_scores_df, mtr_prediction_df, mtr_fold_info_df = self.start_training(classifier_method, train_ids, val_ids, ts_serial=ts_serial, tr_serial=tr_serial, mod_serial=ind) \n",
    "            #print(\"HELLO2222\", mtr_model, mtr_scores_df, mtr_prediction_df) \n",
    "            all_mtr_model[ind] = mtr_model \n",
    "            #all_mtr_model[\"Model_Name\"] = ML_Classifiers.get_short_form(str(classifier_method.value))  \n",
    "            if self.is_validate_models:                   \n",
    "                if modi>0:\n",
    "                    mtr_prediction_df.drop(self.metadata_column, axis=1, inplace=True)\n",
    "                    mtr_prediction_df.drop([self.class_name], axis=1, inplace=True)                \n",
    "                               \n",
    "                mtr_scores_df.insert(0, \"Model_No\", ind) \n",
    "                mtr_scores_df.insert(1, \"Model_Name\", ML_Classifiers.get_short_form(str(classifier_method.value)))  \n",
    "#                 mtr_scores_df.insert(2, \"Selected_Features\", [self.selected_features]*mtr_scores_df.shape[0]) \n",
    "                    \n",
    "                all_mtr_scores_df = pd.concat([all_mtr_scores_df, mtr_scores_df]) \n",
    "                all_mtr_prediction_df = pd.concat([all_mtr_prediction_df, mtr_prediction_df], axis=1) \n",
    "                \n",
    "                all_mtr_scores_df.reset_index(drop=True, inplace=True) \n",
    "                all_mtr_prediction_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            mtr_fold_info_df.insert(0, \"Model_No\", ind)    \n",
    "            mtr_fold_info_df.insert(1, \"Model_Name\", ML_Classifiers.get_short_form(str(classifier_method.value))) \n",
    "            all_mtr_fold_info_df = pd.concat([all_mtr_fold_info_df, mtr_fold_info_df]) \n",
    "            all_mtr_fold_info_df.reset_index(drop=True, inplace=True)  \n",
    "                    \n",
    "        #print(\"HELLO\", all_mtr_model, all_mtr_scores_df, all_mtr_prediction_df) \n",
    "        return all_mtr_model, all_mtr_scores_df, all_mtr_prediction_df, all_mtr_fold_info_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_data_splitter(self, split_crit_tuple, stratified=False, custom=False):\n",
    "        \n",
    "        if custom:\n",
    "            self.logger.info(f\"Custom splitter testing...\") \n",
    "#             splitter = MyCustomSplitter(n_splits=5)\n",
    "#             splitter.set_criteria(split_crit_tuple) \n",
    "            splitter = MyCustomSplitter(split_crit_tuple, groups=self.split_balance_pattern) ### , groups=[['n'], ['SC', 'ST']] for binary and [['n'], ['SC', 'ST']] for multi-class \n",
    "            return splitter\n",
    "        \n",
    "        spl_rand = self.random_state_value ##random.randint(1, 1000)\n",
    "        splitter = None\n",
    "        split_crit = split_crit_tuple[0] ### Fold \n",
    "        split_perc = split_crit_tuple[1] ### Fold \n",
    "        \n",
    "        if split_crit==0:\n",
    "            self.logger.info(f\"Leave-one-subject-out testing...\") \n",
    "            split_num = 1 \n",
    "            splitter = LeavePOut(p=split_num) if stratified else LeavePOut(p=split_num) \n",
    "            # splitter = StratifiedLeavePOut(p=split_num) if stratified else LeavePOut(p=split_num) \n",
    "            # splitter = LeavePOut(p=split_num) \n",
    "            # splitter = StratifiedLeavePOut(p=split_num) #Stratified\n",
    "        elif split_crit>0:\n",
    "            if split_perc<=0:\n",
    "                self.logger.info(f\"{split_crit}-fold testing\") \n",
    "                split_num = 5\n",
    "                if split_crit != split_num:\n",
    "                    split_num = split_crit \n",
    "                splitter = StratifiedKFold(n_splits=split_num, shuffle=False) if stratified else KFold(n_splits=split_num, shuffle=False) \n",
    "                # splitter = StratifiedKFold(n_splits=split_num, shuffle=True, random_state=spl_rand) if stratified else KFold(n_splits=split_num, shuffle=True, random_state=spl_rand) \n",
    "                # splitter = KFold(n_splits=split_num, shuffle=True, random_state=spl_rand)\n",
    "                # splitter = StratifiedKFold(n_splits=split_num, shuffle=True, random_state=spl_rand)\n",
    "                #splitter = KFold(n_splits=split_num, random_state=spl_rand)\n",
    "                #splitter = KFold(n_splits=split_num)\n",
    "            else:\n",
    "                split_num = split_crit \n",
    "                split_ratio = split_perc \n",
    "                self.logger.info(f\"Random {split_ratio} percentage splitting testing...\") \n",
    "                splitter = StratifiedShuffleSplit(n_splits=split_num, test_size=split_ratio, random_state=spl_rand) if stratified else ShuffleSplit(n_splits=split_num, test_size=split_ratio, random_state=spl_rand) \n",
    "                # splitter = ShuffleSplit(n_splits=split_num, test_size=split_ratio, random_state=spl_rand) #rs\n",
    "                # splitter = StratifiedShuffleSplit(n_splits=split_num, test_size=split_ratio, random_state=spl_rand) #rs\n",
    "                # splitter = ShuffleSplit(n_splits=split_num, test_size=split_ratio) #rs\n",
    "        else:\n",
    "            self.logger.info(f\"Problem with the splitting with the splitting criteria {split_crit_tuple}...\") \n",
    "            \n",
    "        # self.data_splitter = splitter \n",
    "        return splitter \n",
    "    \n",
    "    \n",
    "\n",
    "    def start_training(self, classifier_method, train_ids, val_ids, ts_serial, tr_serial, mod_serial):\n",
    "        parameters = self.get_parameters_for_ml_models(classifier_method) \n",
    "        print(\"Parameters: \", parameters)\n",
    "        model, model_scores, target_and_prediction = None, None, None\n",
    "        model, model_scores, target_and_prediction, fold_info_df = self.call_all_model_optimization(classifier_method, parameters, train_ids, val_ids, ts_serial, tr_serial, mod_serial, parameter_optimization=1)\n",
    "        return model, model_scores, target_and_prediction, fold_info_df \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _get_feature_importance_from_AUC_score(self, indices):       \n",
    "        self.logger.info(f\"\"\"\n",
    "        Here to calculate feature importance using AUC\n",
    "        \"\"\")\n",
    "        data = copy.deepcopy(self.dataset).iloc[indices] \n",
    "        \n",
    "        target = data[self.class_name] \n",
    "        metadata_df = data[self.metadata_column] \n",
    "        features = data.drop([self.class_name]+self.metadata_column, axis=1) \n",
    "        \n",
    "        selected_feats_list, importance_scores = self.select_appropriate_features(features, target, num_features=None, selection_criteria={'auc':0.5}) ### selection_criteria=None/{'auc':0.7} #'corr', 'p', 'auc', 'pandauc' \n",
    "        return importance_scores\n",
    "    \n",
    "    \n",
    "    def balance_data(self, features_, target_):\n",
    "#         oversample = SMOTE()\n",
    "#         X, y = oversample.fit_resample(X, y)\n",
    "        tdf = pd.concat([features_, target_], axis=1)\n",
    "        print()\n",
    "        dis_dat = tdf[tdf[self.class_name]!=0]\n",
    "        heal_dat = tdf[tdf[self.class_name]==0]\n",
    "        dis_cnt = dis_dat.shape[0]\n",
    "        heal_cnt = heal_dat.shape[0]\n",
    "        new_df = pd.DataFrame()\n",
    "\n",
    "        self.logger.info(f'Imbalanced data size: disorders= {dis_dat.shape}, healthy= {heal_dat.shape}')\n",
    "        \n",
    "        upsampled_dat = resample(heal_dat, replace=True, n_samples=dis_cnt)\n",
    "        new_df = pd.concat([dis_dat, upsampled_dat]) \n",
    "        \n",
    "        self.logger.info(f'After balancing data size: disorders= {dis_dat.shape}, healthy= {upsampled_dat.shape}')\n",
    "        target = tdf[self.class_name] \n",
    "        features = tdf.loc[:, tdf.columns != self.class_name]\n",
    "            \n",
    "        return features, target\n",
    "\n",
    "\n",
    "    \n",
    "    def _get_data_from_indices(self, indices, from_training=False, from_where='_TR_x'):\n",
    "        data = copy.deepcopy(self.dataset).iloc[indices] \n",
    "#         data.to_csv(f\"{self.directory}/{from_where}.csv\", index=False)\n",
    "        ### Downsample he raining data: 1=down, 2=up, 3=bound sampling\n",
    "        #if from_training:\n",
    "        #    self.logger.info(f'Resampling training data...')\n",
    "        #    data = self.preprocessor.get_resamplled_data(data, self.class_name, self.pat_id_col, random_sampling=True, up_or_down_sampling=1, min_scale=2.0, max_scale=3.0) ## 0-no, 1-down, 2-up, 3-bound\n",
    "                    \n",
    "        self.logger.info(f\"\"\"\n",
    "        From training? {from_training}, Data shape: {data.shape}, Indices: {indices}\n",
    "        All Columns: {data.columns.values.tolist()}\n",
    "        \"\"\") \n",
    "        \n",
    "        all_cols = data.columns.values.tolist() \n",
    "        ft_start_ind = all_cols.index(self.class_name)+1\n",
    "        \n",
    "        target = data[self.class_name] \n",
    "        metadata_df = data[self.metadata_column] \n",
    "#         features = data.drop([self.class_name]+self.metadata_column, axis=1) \n",
    "        features = data.iloc[:, ft_start_ind:]\n",
    "        importance_scores = None\n",
    "        \n",
    "        ### Call features selection algorithm \n",
    "        if self.apply_feature_selection and from_training and self.selected_features is None: \n",
    "            self.logger.info(f\"\"\"\"\n",
    "            Here comes to feature selection...\n",
    "            \"\"\") \n",
    "            selected_feats_list, importance_scores = self.select_appropriate_features(features, target, num_features=None, selection_criteria={'auc':0.5}) ### selection_criteria=None/{'auc':0.7} #'corr', 'p', 'auc', 'pandauc' \n",
    "            self.selected_features = selected_feats_list.copy() \n",
    "            self.logger.info(f\"\"\"\"\n",
    "            Selected features: {self.selected_features}\n",
    "            \"\"\") \n",
    "        elif from_training and self.selected_features is None: \n",
    "            self.selected_features = features.columns.values.tolist() \n",
    "            self.logger.info(f\"\"\"\"\n",
    "            Selected features: {self.selected_features}\n",
    "            \"\"\") \n",
    "            \n",
    "        if 'index' in self.selected_features: #feature_names\n",
    "            self.selected_features.remove('index')\n",
    "            \n",
    "        if self.selected_features is not None:\n",
    "            # features = features.loc[:, ~features.columns.isin(self.selected_features) ]\n",
    "            features = features.loc[:, features.columns.isin(self.selected_features) ]\n",
    "            \n",
    "            \n",
    "#         ### Call data balancing using SMOTE \n",
    "#         features, target = self.balance_data(features, target) \n",
    "        \n",
    "        \n",
    "        self.logger.info(f\"\"\"\n",
    "        Feature shape: {features.shape}, Target shape: {target.shape}, Metadata: {metadata_df.shape} \n",
    "        \"\"\") \n",
    "        \n",
    "        target = target.values.tolist() \n",
    "        features = features.values \n",
    "        \n",
    "        return features, target, metadata_df, importance_scores  \n",
    "    \n",
    "    \n",
    "    \n",
    "    def select_appropriate_features(self, X_dat, y_dat, num_features=None, selection_criteria=None): ### selection_criteria={'auc':0.7} #'corr', 'p', 'auc', 'pandauc' \n",
    "        selected_features = [] \n",
    "        importance_scores = None \n",
    "        crit_name = selection_criteria.keys() \n",
    "        crit_name = list(crit_name)[0] \n",
    "        dpp_obj = DataPreprocessor() \n",
    "        \n",
    "        if crit_name=='corr':\n",
    "            pass\n",
    "        elif crit_name=='p':\n",
    "            selected_features, importance_scores = dpp_obj.select_pandauc_based_features(X_dat, y_dat, binary_class=self.is_binary_classification, p_threshold=0.05, auc_threshold=None, sort=True) \n",
    "        elif crit_name=='auc':\n",
    "            selected_features, importance_scores = dpp_obj.select_pandauc_based_features(X_dat, y_dat, binary_class=self.is_binary_classification, p_threshold=None, auc_threshold=0.5, sort=True) \n",
    "            pass\n",
    "        elif crit_name=='pandauc':\n",
    "            selected_features, importance_scores = dpp_obj.select_pandauc_based_features(X_dat, y_dat, binary_class=self.is_binary_classification, p_threshold=0.05, auc_threshold=0.5, sort=True) \n",
    "            \n",
    "        if num_features:\n",
    "            selected_features = selected_features[:num_features]\n",
    "        \n",
    "        return selected_features, importance_scores \n",
    "    \n",
    "    \n",
    "\n",
    "    def run_model_gridSearch(self, classifier_method, params, train_ids, val_ids, ts_serial, tr_serial, mod_serial):\n",
    "        tmp_train_ids, tmp_val_ids = train_ids.copy(), val_ids.copy()  \n",
    "        should_validate = self.is_validate_models \n",
    "        # should_validate = True \n",
    "        \n",
    "#         if not should_validate:\n",
    "#             # tmp_train_ids.extend(tmp_val_ids) \n",
    "#             tmp_train_ids = np.concatenate((tmp_train_ids, tmp_val_ids)) \n",
    "#         tmp_train_ids = np.concatenate((tmp_train_ids, tmp_val_ids)) \n",
    "        \n",
    "        print('run_model_gridSearch- val_ids, train_ids', val_ids, train_ids)\n",
    "        ### Validation-train split: random percentage based \n",
    "        val_dat = self.dataset[self.split_column][val_ids].values.tolist() \n",
    "        tr_dat = self.dataset[self.split_column][train_ids].values.tolist() \n",
    "#         val_dat = self.dataset.iloc[val_ids, self.split_column].values.tolist() \n",
    "#         tr_dat = self.dataset.iloc[train_ids, self.split_column].values.tolist() \n",
    "        print('run_model_gridSearch- val_dat, tr_dat', val_dat, tr_dat) \n",
    "        fold_info_df = pd.DataFrame([[val_dat, tr_dat]], columns=['Validation', 'Training']) \n",
    "        \n",
    "        X_train, y_train, _, importance_scores = self._get_data_from_indices(tmp_train_ids, from_training=True, from_where=f'_TR_ts_{ts_serial}_tr_{tr_serial}_mod_{mod_serial}') \n",
    "#         if importance_scores is None:\n",
    "#             importance_scores = self._get_feature_importance_from_AUC_score(tmp_train_ids)\n",
    "        importance_scores = self._get_feature_importance_from_AUC_score(tmp_train_ids)\n",
    "        \n",
    "        mods = self.get_ml_model_instances(classifier_method)\n",
    "        self.logger.info(f\"\"\"\n",
    "        GridSearch: {ML_Classifiers.get_short_form(str(classifier_method.value))} - {params} \n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        \"\"\")\n",
    "        parameters = {}\n",
    "        model = mods\n",
    "        model_scores = None\n",
    "        if self.should_use_params:\n",
    "            parameters = params\n",
    "\n",
    "        scoring, refit = self.get_ml_scoring_metrices(self.best_model_scoring_metrics[0]) \n",
    "#         scoring, refit = 'f1', True\n",
    "        self.logger.info(f\"\"\"Refitting the model with best parameter {scoring} == {refit}\"\"\")\n",
    "        \n",
    "        model = GridSearchCV(mods, parameters, scoring=scoring, cv=5, refit=refit, return_train_score=True, n_jobs=50, verbose=2)\n",
    "        # model = GridSearchCV(mods, parameters, scoring=scoring, cv=5, refit=refit, return_train_score=True, error_score='raise', n_jobs=50, verbose=2)\n",
    "        # model = GridSearchCV(mods, parameters, scoring=scoring, cv=5, refit=refit, return_train_score=True, n_jobs=mp.cpu_count(), verbose=2)\n",
    "\n",
    "        # ### Scoring from custom method\n",
    "        # score = make_scorer(self.custom_precision_func, greater_is_better=False)\n",
    "        # # scoring = {'precision': score, 'f1':make_scorer(f1_score)}\n",
    "        # model = GridSearchCV(mods, parameters, scoring=score, cv=self.cross_validation_rounds, refit=refit, return_train_score=True, n_jobs=-1, verbose=2)\n",
    "        \n",
    "        # X_train = np.nan_to_num(X_train)\n",
    "        model = model.fit(X_train, y_train) \n",
    "        mod = copy.deepcopy(model) \n",
    "        mod_est = model.best_estimator_ \n",
    "        mod_par = model.best_params_\n",
    "        \n",
    "        # print('KKKKKKKKK-->>>', model, mod_est, mod_par)\n",
    "        model_scores = None \n",
    "        target_and_prediction_df = pd.DataFrame() \n",
    "        \n",
    "#         ### Rebuild the model with best parameter         \n",
    "#         # if should_validate:\n",
    "#         bst_parameters = model.best_params_\n",
    "#         self.logger.info(f\"\"\"Refitting the model with best parameter\"\"\")\n",
    "#         # mod = mod.set_params(**bst_parameters)\n",
    "#         mod = mod.best_estimator_.set_params(**bst_parameters)\n",
    "#         tmp_train_ids2 = np.concatenate((train_ids.copy(), val_ids.copy()))\n",
    "#         # X_train2, y_train2, _, _ = self._get_data_from_indices(tmp_train_ids2, from_training=True)  \n",
    "#         X_train2, y_train2, _, _ = self._get_data_from_indices(tmp_train_ids2, from_training=False)    \n",
    "#         mod = mod.fit(X_train2, y_train2)\n",
    "            \n",
    "#         X_val, y_val, meta_dat, _ = self._get_data_from_indices(val_ids) \n",
    "        X_val, y_val, meta_dat, _ = self._get_data_from_indices(tmp_val_ids, from_where=f'_VAL_ts_{ts_serial}_tr_{tr_serial}_mod_{mod_serial}')   \n",
    "#         X_val, y_val, meta_dat, _ = self._get_data_from_indices(tmp_train_ids)   \n",
    "        y_pred = mod.predict(X_val)  \n",
    "        y_pred_proba = mod.predict_proba(X_val) \n",
    "        target_and_prediction_df.reset_index(drop=True, inplace=True) \n",
    "        meta_dat.reset_index(drop=True, inplace=True) \n",
    "        target_and_prediction_df = pd.concat([target_and_prediction_df, meta_dat]) \n",
    "        target_and_prediction_df[self.class_name] = y_val \n",
    "        # target_and_prediction_df[f\"Prediction_{str(model.__class__.__name__)}\"] = y_pred \n",
    "        target_and_prediction_df[f\"Prediction_{ML_Classifiers.get_short_form(str(classifier_method.value))}\"] = y_pred  \n",
    "        for p_ind in range(y_pred_proba.shape[1]):\n",
    "            target_and_prediction_df[f\"Prediction_Proba_{p_ind}_{ML_Classifiers.get_short_form(str(classifier_method.value))}\"] = y_pred_proba[:, p_ind].tolist()\n",
    "            \n",
    "        model.feature_names = self.selected_features \n",
    "        model.feature_importance_scores = importance_scores\n",
    "        model_scores = self.calculate_model_scores(model, y_val, y_pred, y_pred_proba) \n",
    "\n",
    "        self.logger.info(f\"\"\"\n",
    "        Best model (GriveSearchCV): {model} \n",
    "        Best model: {mod} \n",
    "        Best estimator of the model: {mod_est} \n",
    "        Best parameters of the model: {mod_par} \n",
    "        Best model scores: {model_scores} \n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        \"\"\")\n",
    "\n",
    "        return model, model_scores, target_and_prediction_df, fold_info_df \n",
    "        # return model, model_scores, target_and_prediction_df, fold_info_df\n",
    "\n",
    "\n",
    "    def call_all_model_optimization(self, classifier_method, parameters, train_ids, val_ids, ts_serial, tr_serial, mod_serial, parameter_optimization):\n",
    "        model, model_scores, target_and_prediction, fold_info_df = None, None, None, None \n",
    "        if parameter_optimization == 1:\n",
    "            model, model_scores, target_and_prediction, fold_info_df = self.run_model_gridSearch(classifier_method, parameters, train_ids, val_ids, ts_serial, tr_serial, mod_serial)\n",
    "        elif parameter_optimization == 2:\n",
    "            model, model_scores, target_and_prediction, fold_info_df = self.run_model_randomizedSearch(classifier_method, parameters, train_ids, val_ids, ts_serial, tr_serial, mod_serial)\n",
    "        elif parameter_optimization == 3:\n",
    "            model, model_scores, target_and_prediction, fold_info_df = self.run_model_baysianSearch(classifier_method, parameters, train_ids, val_ids, ts_serial, tr_serial, mod_serial)\n",
    "        elif parameter_optimization == 4:\n",
    "            model, model_scores, target_and_prediction, fold_info_df = self.run_model_customGridSearch(classifier_method, parameters, train_ids, val_ids, ts_serial, tr_serial, mod_serial)\n",
    "        return model, model_scores, target_and_prediction, fold_info_df\n",
    "\n",
    "\n",
    "    def get_ml_model_instances(self, classifier_method, parameters=None):\n",
    "        classifier = None\n",
    "\n",
    "        ### GPU code START\n",
    "        global GPUs\n",
    "        global HAS_GPU\n",
    "\n",
    "        # GPUs = GPUtil.getGPUs()\n",
    "        # tot_gpus = len(GPUs)\n",
    "        # HAS_GPU = True if len(GPUs) > 0 else False\n",
    "        # avl_GPUIDs = GPUtil.getAvailable(order = 'first', limit = tot_gpus, maxLoad = 0.5, maxMemory = 0.5, includeNan=False, excludeID=[], excludeUUID=[])\n",
    "        # tot_avl_gpus = len(avl_GPUIDs)\n",
    "        # print(f'For GPU based tasks. There are {tot_gpus} GPUs in the system and {tot_avl_gpus} are available. \\nAvailable GPU IDs are: {avl_GPUIDs}')\n",
    "        allGPUs, bestGPU = HumachLab_Global.get_gpu_details(show_logs=False)\n",
    "        ### GPU code END\n",
    "\n",
    "        # ####### rf #######\n",
    "        # rf - random_forest classifier\n",
    "        if classifier_method == ML_Classifiers.RF:\n",
    "            classifier = RandomForestClassifier() if (parameters is None) else RandomForestClassifier(parameters)\n",
    "        # ####### knn #######\n",
    "        # knn - k_neares_neighbours classifier\n",
    "        elif classifier_method == ML_Classifiers.kNN:\n",
    "            classifier = KNeighborsClassifier() if (parameters is None) else KNeighborsClassifier(parameters)\n",
    "        # ####### nb #######\n",
    "        # knn - naieve bayes classifier\n",
    "        elif classifier_method == ML_Classifiers.NB:\n",
    "            classifier = GaussianNB() if (parameters is None) else GaussianNB(parameters)\n",
    "        # ####### svm/svc #######\n",
    "        # knn - support vector classifier\n",
    "        elif classifier_method == ML_Classifiers.SVC:\n",
    "            classifier = SVC() if (parameters is None) else SVC(parameters)\n",
    "        # ####### knn #######\n",
    "        # knn - k_neares_neighbours classifier\n",
    "        elif classifier_method == ML_Classifiers.DT:\n",
    "            classifier = DecisionTreeClassifier() if (parameters is None) else DecisionTreeClassifier(parameters)\n",
    "        # ####### LogReg #######\n",
    "        # LogReg - logistic regression classifier\n",
    "        elif classifier_method == ML_Classifiers.LogReg:\n",
    "            classifier = LogisticRegression() if (parameters is None) else LogisticRegression(parameters)\n",
    "        # ####### GBoost #######\n",
    "        # GBoost - gradient boosting classifier\n",
    "        elif classifier_method == ML_Classifiers.GBoost:\n",
    "            classifier = GradientBoostingClassifier() if (parameters is None) else GradientBoostingClassifier(parameters)\n",
    "        # ####### XGBoost #######\n",
    "        # GBoost - eXtreme gradient boosting classifier\n",
    "        elif classifier_method == ML_Classifiers.XGBoost:\n",
    "            classifier = XGBClassifier() if (parameters is None) else XGBClassifier(parameters)\n",
    "\n",
    "        ### GPU code - Comment it if no gpu available or not linux system or no support for RapidsAI package\n",
    "        # ####### gpu-rf #######\n",
    "        # gpu-rf - gpu-random_forest classifier\n",
    "        # elif classifier_method == ML_Classifiers.GPURF and tot_avl_gpus>0:\n",
    "        #     classifier = gpuRandomForestClassifier() if (parameters is None) else gpuRandomForestClassifier(parameters)\n",
    "\n",
    "        # ####### None #######\n",
    "        # No classifier\n",
    "        else:\n",
    "            self.logger.info(f'No classifier is selected...')\n",
    "\n",
    "        # ####### ####### #######\n",
    "        return classifier\n",
    "\n",
    "\n",
    "    def get_ml_scoring_metrices(self, reft=None):\n",
    "        model_scoring_mets = [ML_Performace_Metrics.ACC, ML_Performace_Metrics.PREC, ML_Performace_Metrics.RECL,\n",
    "                              ML_Performace_Metrics.SEN, ML_Performace_Metrics.SPEC, ML_Performace_Metrics.FPR,\n",
    "                              ML_Performace_Metrics.FNR, ML_Performace_Metrics.F1, ML_Performace_Metrics.ROC_AUC]\n",
    "\n",
    "        scoring = [ML_Performace_Metrics.ACC.value]\n",
    "        bst_mod_mets_1 = None\n",
    "        i = 0\n",
    "        for met in self.best_model_scoring_metrics:\n",
    "            if i==0:\n",
    "                scoring.clear()\n",
    "                if (reft is not None):\n",
    "                    if reft == ML_Performace_Metrics.F1SCR:\n",
    "                        reft = ML_Performace_Metrics.F1\n",
    "                    if (reft not in model_scoring_mets):\n",
    "                        reft = None\n",
    "\n",
    "            if met == ML_Performace_Metrics.F1SCR:\n",
    "                met = ML_Performace_Metrics.F1\n",
    "\n",
    "            if met in model_scoring_mets:\n",
    "                scoring.append(met.value)\n",
    "            i += 1\n",
    "\n",
    "        refit = (scoring[0]) if reft is None else reft.value\n",
    "        \n",
    "#         scoring = [ML_Performace_Metrics.F1]\n",
    "#         refit = True\n",
    "\n",
    "        return scoring, refit\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    def get_parameters_for_ml_models(self, classifier_method):\n",
    "        parameters = {}\n",
    "        if not self.should_use_params:\n",
    "            return parameters\n",
    "\n",
    "        # Parameter generation method name\n",
    "        method_name = f'{str(classifier_method.value)}_parameters'\n",
    "\n",
    "        try:\n",
    "            method = getattr(self, method_name)\n",
    "            # Call method for parameter generation\n",
    "            self.logger.info(f'Calling method: {method_name}')\n",
    "            parameters = method()\n",
    "        except AttributeError:\n",
    "            self.logger.warning(f'No such method exists with the name: {method_name}')\n",
    "            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.__class__.__name__, method_name))\n",
    "\n",
    "        # ####### ####### #######\n",
    "        return parameters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    def generate_parameter_dictionary(self, par_names, par_vals, par_ind):\n",
    "        self.logger.info(f'All parameters: {par_names}, {par_vals}, {par_ind}')\n",
    "        final_par_names = []\n",
    "        par_dict = {}\n",
    "\n",
    "#         for i in par_ind:\n",
    "#             pn = par_names[i]\n",
    "#             pv = par_vals[i]\n",
    "#             exec(f'{pn}={pv}')\n",
    "#             final_par_names.append(pn)\n",
    "        \n",
    "        sel_par = [pp for ii,pp in enumerate(par_names) if ii in par_ind] \n",
    "        for (pn, pv) in zip(sel_par, par_vals):\n",
    "            exec(f'{pn}={pv}')\n",
    "            final_par_names.append(pn)\n",
    "\n",
    "        for par in final_par_names:\n",
    "            par_dict[par] = eval(par)\n",
    "\n",
    "        return par_dict\n",
    "\n",
    "\n",
    "    # def float_range(self, start, stop, step):\n",
    "    #     start = decimal.Decimal(start)\n",
    "    #     stop = decimal.Decimal(stop)\n",
    "    #     while start < stop:\n",
    "    #         yield float(start)\n",
    "    #         start *= decimal.Decimal(step)\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    # Model parameter settings\n",
    "    # #########################################################################\n",
    "    # ### ML Classifier Method Parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def logistic_regression_parameters(self):\n",
    "\n",
    "        # ### Parameter generation using function\n",
    "        par_names = ['penalty', 'solver', 'max_iter', 'C']\n",
    "        par_vals = [['l1', 'l2', 'elasticnet'],\n",
    "                    ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "                    list(range(50, 5000, 10)),\n",
    "                    list(HumachLab_StaticMethods.float_range('0.001', '1.0', '0.01'))]\n",
    "\n",
    "        par_vals = [[30, 50, 75, 100, 200, 500, 750, 1000], [2, 3, 5, 7], [5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000]]\n",
    "        par_vals = [[5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000]]\n",
    "        par_vals = [[15, 21, 30, 50, 75, 100, 200, 500]]\n",
    "        par_vals = [[50, 75, 100]]\n",
    "        par_vals = [[15, 21, 30, 50, 75, 100, 200, 500], [3, 5, 7, 10, 15, 20, 25, 30]]\n",
    "        par_vals = [['l1', 'l2', 'elasticnet'],[50, 100, 130, 150, 170, 200, 250, 350, 500, 750, 1000]]\n",
    "        par_vals = [['l1', 'l2', 'elasticnet'],[50, 100, 130, 150, 170, 200],[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]]\n",
    "        par_vals = [['l2', 'elasticnet'],[50, 100, 130, 150, 170, 200],[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]]\n",
    "        par_ind = [0, 2, 3]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def k_nearest_neighbors_parameters(self):\n",
    "\n",
    "        # ### Parameter generation using function\n",
    "        par_names = ['n_neighbors', 'p', 'metric', 'n_splits']\n",
    "        par_vals = [list(range(2, 100)),\n",
    "                    list(range(2, 100)),\n",
    "                    ['manhattan', 'minkowski', 'euclidean'],\n",
    "                    list(range(2, 10))]\n",
    "\n",
    "        par_vals = [list(range(100, 1000, 50)), list(range(2, 11, 1)), [2, 3, 5, 9, 13, 19, 29]]\n",
    "        par_vals = [[2, 3, 5, 9, 13, 19, 29]]\n",
    "        par_vals = [[2, 3, 5, 10, 15, 25, 35], ['manhattan', 'minkowski', 'euclidean']]\n",
    "        par_ind = [0, 2]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def naive_bayes_parameters(self):\n",
    "\n",
    "        # ### Parameter generation using function\n",
    "        par_names = ['var_smoothing']\n",
    "        par_vals = [list(np.logspace(0, -9, num=100))]\n",
    "        par_vals = [list(np.logspace(0, -9, num=100))]\n",
    "\n",
    "        # par_vals = []\n",
    "        # par_vals = []\n",
    "        par_vals = [list(np.logspace(0,-9, num=5))]\n",
    "        par_ind = [0]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "\n",
    "\n",
    "    def support_vector_classifier_parameters(self):\n",
    "\n",
    "        # ### Parameter generation using function #, probability=True\n",
    "        par_names = ['probability', 'C', 'kernel', 'gamma', 'degree', 'class_weightdict']\n",
    "        par_vals = [[True],\n",
    "                    list(HumachLab_StaticMethods.float_range('0.001', '1', '0.01')),\n",
    "                    ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "                    list(HumachLab_StaticMethods.float_range('0.000001', '1', '10')),\n",
    "                    list(range(1, 10)),\n",
    "                    [None, 'balanced']]\n",
    "\n",
    "        # par_vals = [list(HumachLab_StaticMethods.float_range('0.000001', '1', '10')), list(HumachLab_StaticMethods.float_range('0.00001', '1', '10')), list(HumachLab_StaticMethods.float_range('0.0001', '1', '10'))]\n",
    "        par_vals = [list(HumachLab_StaticMethods.float_range('0.001', '1.', '0.1')), ['linear', 'rbf', 'poly']]\n",
    "        par_vals = [[True],[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], ['linear', 'rbf', 'poly']]\n",
    "        par_ind = [0, 1, 2]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def decision_tree_parameters(self):\n",
    "\n",
    "        # ### Parameter generation using function\n",
    "        par_names = ['max_depth', 'criterion', 'splitter', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes']\n",
    "        par_vals = [list(range(1, 100)),\n",
    "                    ['gini', 'entropy', 'log_loss'],\n",
    "                    ['best', 'random'],\n",
    "                    list(range(1, 10)),\n",
    "                    list(range(1, 10)),\n",
    "                    list(range(1, 100))]\n",
    "\n",
    "        par_vals = [list(range(1, 100)), list(range(1, 100, 2)), list(range(1, 100, 3))]\n",
    "        par_vals = [list(range(1, 100))]\n",
    "        par_vals = [[2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']]\n",
    "        par_ind = [0, 1]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def random_forest_parameters(self): \n",
    "\n",
    "        # ### Parameter generation using function\n",
    "        par_names = ['n_estimators', 'max_depth', 'criterion', 'splitter', 'max_features', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes']\n",
    "        par_vals = [list(range(1, 500, 5)),\n",
    "                    list(range(1, 100)),\n",
    "                    ['gini', 'entropy', 'log_loss'], \n",
    "                    ['best', 'random'],\n",
    "                    list(range(2, 20, 1)),\n",
    "                    list(range(1, 10)),\n",
    "                    list(range(1, 10)),\n",
    "                    list(range(1, 100))]\n",
    "\n",
    "        par_vals = [[30, 50, 75, 100, 200, 500, 750, 1000], [2, 3, 5, 7], [5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000]]\n",
    "        par_vals = [[5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000]]\n",
    "        par_vals = [[15, 21, 30, 50, 75, 100, 200, 500]]\n",
    "        par_vals = [[50, 75, 100]]\n",
    "        par_vals = [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']]\n",
    "        # par_vals = [[2, 3, 5, 7]]\n",
    "        par_ind = [0, 1, 2]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def graphics_processing_unit_random_forest(self):\n",
    "\n",
    "        # ### Parameter generation using function\n",
    "        par_names = ['n_estimators', 'n_bins', 'n_streams', 'max_depth', 'max_features', 'criterion', 'splitter', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes']\n",
    "        par_vals = [list(range(1, 500, 5)),\n",
    "                    list(range(1, 100)),\n",
    "                    list(range(2, 20, 1)),\n",
    "                    ['gini', 'entropy', 'log_loss'], \n",
    "                    ['best', 'random'],\n",
    "                    list(range(1, 10)),\n",
    "                    list(range(1, 10)),\n",
    "                    list(range(1, 100))]\n",
    "\n",
    "        par_vals = [[30, 50, 75, 100, 200, 500, 750, 1000], [2, 3, 5, 7], [5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000]]\n",
    "        par_vals = [[5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000], 15, 8]\n",
    "        # par_vals = [[2, 3, 5, 7]]\n",
    "        par_ind = [0, 1, 2]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def gradient_boosting_parameters(self):\n",
    "\n",
    "        # ### Parameter generation using function \n",
    "        par_names = ['n_estimators', 'max_depth', 'learning_rate', 'max_features', 'loss', 'min_samples_split', 'min_samples_leaf']\n",
    "        par_vals = [list(range(1, 500, 5)),\n",
    "                    list(range(1, 100)),\n",
    "                    list(HumachLab_StaticMethods.float_range('0.001', '1', '0.01')), \n",
    "                    list(range(2, 20, 1)),\n",
    "                    ['log_loss', 'exponential'],\n",
    "                    list(range(1, 10)),\n",
    "                    list(range(1, 10))]\n",
    "\n",
    "        par_vals = [[30, 50, 75, 100, 200, 500, 750, 1000], [2, 3, 5, 7], [5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000]]\n",
    "        par_vals = [[5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000]]\n",
    "        par_vals = [[15, 21, 30, 50, 75, 100, 200, 500]]\n",
    "        par_vals = [[50, 75, 100]]\n",
    "        par_vals = [[15, 21, 30, 50, 75, 100, 200, 500], [3, 5, 7, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.5, 1.0]]\n",
    "        par_vals = [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.5, 1.0]]\n",
    "        # par_vals = [[2, 3, 5, 7]]\n",
    "        par_ind = [0, 1, 2]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def xtreme_gradient_boosting_parameters(self): #xtreme_gradient_boosting\n",
    "\n",
    "        # ### Parameter generation using function \n",
    "        par_names = ['max_depth', 'eta', 'max_leaves']\n",
    "        par_vals = [list(range(1, 100)),\n",
    "                    list(HumachLab_StaticMethods.float_range('0.001', '1', '0.01')),\n",
    "                    list(range(0, 20, 1)) ]\n",
    "\n",
    "        par_vals = [[3, 6, 10, 15, 25, 40, 100, 250, 500, 750, 1000], [0.01, 0.05, 0.1, 0.3, 0.5, 1.0], [0, 5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000]]\n",
    "        par_vals = [[2, 3, 6, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]]\n",
    "        # par_vals = [[2, 3, 5, 7]]\n",
    "        par_ind = [0, 1]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    # Calculate and save classification details and model scores\n",
    "    # #########################################################################\n",
    "    #############################\n",
    "\n",
    "    def calculate_model_scores(self, mods, y_test, y_pred, y_pred_proba): \n",
    "#         print(y_test, '\\n', y_pred, '\\n', y_pred_proba, '\\n')\n",
    "#         target_labels = np.unique(np.array(y_test)).tolist() \n",
    "        target_labels = sorted( self.dataset[self.class_name].unique().tolist() )\n",
    "        \n",
    "        y_pred = y_pred.tolist() \n",
    "        perf_scores = self.calculate_performance_scores(y_test, y_pred, y_pred_proba, labels=target_labels)  # average = 'weighted', 'macro', 'micro' \n",
    "        confMat = perf_scores['Conf_Mat']\n",
    "\n",
    "        acc = round(perf_scores['ACC'], 3) #Accuracy score or total correct prediction rate \n",
    "        prec = round(perf_scores['PREC'], 3) #precision or positive predictive value (PPV)\n",
    "        reca_sens = round(perf_scores['REC'], 3) #sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "        spec = round(perf_scores['SPE'], 3) #specificity, selectivity or true negative rate (TNR)\n",
    "        f1sc = round(perf_scores['F1SCR'], 3)\n",
    "        auc_s = round(perf_scores['AUC'], 3) \n",
    "        \n",
    "        scr_dict = {'method': str(mods), 'model': mods.best_estimator_, 'model_parameters': mods.best_params_, \n",
    "                    'model_scores': round(mods.best_score_*100,2),\n",
    "                    ML_Performace_Metrics.CONF_MAT.value: confMat, ML_Performace_Metrics.ACC.value: acc, ML_Performace_Metrics.PREC.value: prec,\n",
    "                    ML_Performace_Metrics.RECL.value: reca_sens, ML_Performace_Metrics.SEN.value: reca_sens, ML_Performace_Metrics.SPEC.value: spec,\n",
    "                    ML_Performace_Metrics.F1SCR.value: f1sc, ML_Performace_Metrics.ROC_AUC.value: auc_s}\n",
    "        \n",
    "#         scr_dict = {'method_class': str(mods.__class__.__name__), 'model_name': mods.best_estimator_, 'model_parameters': mods.best_params_, \n",
    "#                     'model_scores': round(mods.best_score_*100,2),\n",
    "#                     ML_Performace_Metrics.CONF_MAT.value: confMat, ML_Performace_Metrics.ACC.value: acc, ML_Performace_Metrics.PREC.value: prec,\n",
    "#                     ML_Performace_Metrics.RECL.value: reca_sens, ML_Performace_Metrics.SEN.value: reca_sens, ML_Performace_Metrics.SPEC.value: spec,\n",
    "#                     ML_Performace_Metrics.F1SCR.value: f1sc, ML_Performace_Metrics.ROC_AUC.value: auc_s}\n",
    "#         scr_dict = {'model_class': str(mods.__class__.__name__), 'method_name': mods.estimator, 'method_parameters': mods.best_params_, \n",
    "#                     'method_scores': round(mods.best_score_*100,2),\n",
    "#                     ML_Performace_Metrics.CONF_MAT.value: confMat, ML_Performace_Metrics.ACC.value: acc, ML_Performace_Metrics.PREC.value: prec,\n",
    "#                     ML_Performace_Metrics.RECL.value: reca_sens, ML_Performace_Metrics.SEN.value: reca_sens, ML_Performace_Metrics.SPEC.value: spec,\n",
    "#                     ML_Performace_Metrics.FPR.value: fpr, ML_Performace_Metrics.FNR.value: fnr, ML_Performace_Metrics.F1SCR.value: f1sc, ML_Performace_Metrics.ROC_AUC.value: auc_s}\n",
    "#         scr_dict = {'model_class': str(mods.__class__.__name__), 'method_name': str(mods), 'method_parameters': mods.best_params_, \n",
    "#                     'method_scores': round(mods.best_score_*100,2),\n",
    "#                     ML_Performace_Metrics.CONF_MAT.value: confMat, ML_Performace_Metrics.ACC.value: acc, ML_Performace_Metrics.PREC.value: prec,\n",
    "#                     ML_Performace_Metrics.RECL.value: reca_sens, ML_Performace_Metrics.SEN.value: reca_sens, ML_Performace_Metrics.SPEC.value: spec,\n",
    "#                     ML_Performace_Metrics.FPR.value: fpr, ML_Performace_Metrics.FNR.value: fnr, ML_Performace_Metrics.F1SCR.value: f1sc, ML_Performace_Metrics.ROC_AUC.value: auc_s}\n",
    "        scr_df = pd.DataFrame([list( scr_dict.values() )], columns=list( scr_dict.keys() )) \n",
    "        self.logger.info(f\"\"\"Score columns: {scr_df.shape} {scr_df.columns.values.tolist()}\"\"\") \n",
    "\n",
    "        return scr_df\n",
    "    \n",
    "    \n",
    "    def calculate_performance_scores(self, y_true, y_pred, y_pred_proba, labels=[0, 1], verbose=2, average='weighted'): # average = 'macro', 'micro', 'weighted' \n",
    "        #### SOURCES: https://www.youtube.com/watch?v=PCHf_7jBor8 \n",
    "        # https://www.mariakhalusova.com/posts/2019-04-17-ml-model-evaluation-metrics-p2/ \n",
    "        # https://www.mariakhalusova.com/posts/2019-04-11-ml-model-evaluation-metrics-p1/\n",
    "        # https://www.evidentlyai.com/classification-metrics/multi-class-metrics \n",
    "        # https://www.kaggle.com/code/nkitgupta/evaluation-metrics-for-multi-class-classification \n",
    "\n",
    "        model_scores = []\n",
    "        true_label_uniq = np.unique(np.array(y_true)).tolist() \n",
    "        print(np.unique(np.array(y_true)), np.unique(np.array(y_pred)))\n",
    "        print(y_true, y_pred) \n",
    "        conf_matrix = confusion_matrix(y_true, y_pred, labels=labels).tolist()\n",
    "        print(np.array(conf_matrix) )\n",
    "\n",
    "        ### For micro averaging and binary class \n",
    "        conf_matrix_arr = np.array(conf_matrix) \n",
    "        one_vs_all_confMat = []     \n",
    "        for label in labels:\n",
    "            tp_lbl = conf_matrix_arr[label, label] \n",
    "            fp_lbl = np.sum(conf_matrix_arr[:, label])-tp_lbl \n",
    "            fn_lbl = np.sum(conf_matrix_arr[label, :])-tp_lbl \n",
    "            tn_lbl = np.sum(conf_matrix_arr)-(tp_lbl+fp_lbl+fn_lbl) \n",
    "            one_vs_all_confMat.append([tn_lbl, fp_lbl, fn_lbl, tp_lbl]) \n",
    "        print(np.array(one_vs_all_confMat)) \n",
    "\n",
    "        tn_tot = np.sum( np.array(one_vs_all_confMat)[:, 0] ) \n",
    "        fp_tot = np.sum( np.array(one_vs_all_confMat)[:, 1] )  \n",
    "        fn_tot = np.sum( np.array(one_vs_all_confMat)[:, 2] )  \n",
    "        tp_tot = np.sum( np.array(one_vs_all_confMat)[:, 3] )\n",
    "\n",
    "        conf_matrix_tol = [[tn_tot, fp_tot], [fn_tot, tp_tot]] \n",
    "        print(np.array(conf_matrix_tol)) \n",
    "\n",
    "        if len(labels)==2:\n",
    "            tn_tot = one_vs_all_confMat[1][0] \n",
    "            fp_tot = one_vs_all_confMat[1][1] \n",
    "            fn_tot = one_vs_all_confMat[1][2] \n",
    "            tp_tot = one_vs_all_confMat[1][3] \n",
    "            average = \"micro\"\n",
    "\n",
    "        result = [] \n",
    "        for label in labels:\n",
    "            precision, recall, fscore, support = precision_recall_fscore_support( np.array(y_true)==label, np.array(y_pred)==label ) \n",
    "            # tmp_fpr, tmp_tpr, tmp_thresholds = roc_curve(np.array(y_true)==label, np.array(y_pred)==label, pos_label=label) \n",
    "            tmp_fpr, tmp_tpr, tmp_thresholds = roc_curve(np.array(y_true)==label, np.array(y_pred)==label) \n",
    "            auc_score = auc(tmp_fpr, tmp_tpr)*100 \n",
    "\n",
    "            if label in true_label_uniq: \n",
    "                result.append( [label, precision[1], recall[1], recall[1], recall[0], fscore[1], auc_score, support[1]] ) \n",
    "            else:\n",
    "                result.append( [label, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0] ) \n",
    "\n",
    "            accuracy = accuracy_score(np.array(y_true)==label, np.array(y_pred)==label)*100 \n",
    "            if verbose>1:\n",
    "                print(\n",
    "                    f'Class-wise info: For multilevel internal scores fo label {label}: \\n', \n",
    "                    f'Accuracy = {accuracy}\\n', \n",
    "                    f'Precision = {precision}\\n', \n",
    "                    f'Recall = {recall}\\n', \n",
    "                    f'F1 score = {fscore}\\n', \n",
    "                    f'AUC score = {auc_score}\\n', \n",
    "                    f'Support = {support}\\n', \n",
    "                )\n",
    "        tdf = pd.DataFrame(result, columns=['Label', 'Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', 'AUC', 'Support']) \n",
    "\n",
    "        if average=='macro': #average = \"weighted\", \"macro\", \"micro\" \n",
    "            tdf = tdf[['Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', 'AUC']].apply(lambda col:np.mean(col), axis=0) \n",
    "        elif average=='micro':\n",
    "            prc = (tp_tot / (tp_tot+fp_tot))*100 if (tp_tot+fp_tot)!=0 else 0.0 #precision or positive predictive value (PPV)\n",
    "            rec = (tp_tot / (tp_tot+fn_tot))*100 if (tp_tot+fn_tot)!=0 else 0.0 #sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "            sns = rec #sensitivity same as recall \n",
    "            spc = (tn_tot / (tn_tot+fp_tot))*100 if (tn_tot+fp_tot)!=0 else 0.0 #specificity, selectivity or true negative rate (TNR)\n",
    "            f1s = (2*tp_tot / (2*tp_tot+fp_tot+fn_tot))*100 if (2*tp_tot+fp_tot+fn_tot)!=0 else 0.0 #specificity, selectivity or true negative rate (TNR)  \n",
    "            auc_s = roc_auc_score(y_true, y_pred) if len(labels)==2 else roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average=average) #multi_class='ovo', 'ovr' \n",
    "#             auc_s = 0.5\n",
    "#             if len(labels)==2:\n",
    "#                 auc_s = roc_auc_score(y_true, y_pred)  \n",
    "#             else:\n",
    "#                 auc_s = roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average=average) #multi_class='ovo', 'ovr' \n",
    "            tdf = pd.Series([prc, rec, sns, spc, f1s, auc_s], index=['Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', 'AUC'])  \n",
    "        else: ## Default = weighted\n",
    "            class_weights = tdf['Support']/tdf['Support'].sum() \n",
    "            tdf = tdf[['Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', 'AUC']].apply(lambda col:np.sum(col*class_weights), axis=0) \n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)*100 \n",
    "        tdf_summary = pd.Series([conf_matrix, acc, tdf['Precision'], tdf['Recall'], tdf['Sensitivity'], tdf['Specificity'], tdf['F1 Score'], tdf['AUC']],\n",
    "                               index=['Conf_Mat', 'ACC', 'PREC', 'REC', 'SEN', 'SPE', 'F1SCR', 'AUC'])\n",
    "\n",
    "        if verbose>1:\n",
    "            confMat = tdf_summary['Conf_Mat']\n",
    "            acc = round(tdf_summary['ACC'], 3) #Accuracy score or total correct prediction rate \n",
    "            prec = round(tdf_summary['PREC'], 3) #precision or positive predictive value (PPV)\n",
    "            reca_sens = round(tdf_summary['REC'], 3) #sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "            spec = round(tdf_summary['SPE'], 3) #specificity, selectivity or true negative rate (TNR)\n",
    "            f1sc = round(tdf_summary['F1SCR'], 3)\n",
    "            auc_s = round(tdf_summary['AUC'], 3) \n",
    "            print(\n",
    "                f'CLASSIFICATION MERICS:\\n',\n",
    "                f'{\"_\"*55}\\n',\n",
    "                f'Confusion Matrix: \\n{np.array(conf_matrix)}\\n',\n",
    "                f'Accuracy (acc): {acc}\\n',\n",
    "                f'Precision (prc): {prec}\\n',\n",
    "                f'Recall (rec): {reca_sens}\\n',\n",
    "                f'Sensitivity (sns): {reca_sens}\\n',\n",
    "                f'Specificity (spc): {spec}\\n',\n",
    "                f'F1 Score (f1s): {f1sc}\\n',\n",
    "                f'ROC AUC (AUC): {auc_s}',\n",
    "            )\n",
    "\n",
    "        return tdf_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee68c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d32c26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
