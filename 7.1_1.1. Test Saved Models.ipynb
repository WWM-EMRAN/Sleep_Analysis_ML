{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa60fc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ML Applicaiton: Applying ML Models to the transition data\n",
    "\n",
    "### Tasks included:\n",
    "- Reading transition matrices \n",
    "- Reading P/AUC scores \n",
    "- Apply ML models \n",
    "- Select features and apply ML models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c7f06b",
   "metadata": {},
   "source": [
    "## Imports and system info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "36e58af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "List of OS platforms and codes\n",
      "___________________________________________\n",
      "0 Darwin\n",
      "1 Windows\n",
      "2 Linux\n",
      "===> \"1 - Windows\" OS is detected.\n",
      "\n",
      "===========================================\n",
      "Processor (CPU) details: \n",
      "___________________________________________\n",
      "{'python_version': '3.7.10.final.0 (64 bit)', 'cpuinfo_version': [8, 0, 0], 'cpuinfo_version_string': '8.0.0', 'arch': 'X86_64', 'bits': 64, 'count': 40, 'arch_string_raw': 'AMD64', 'vendor_id_raw': 'GenuineIntel', 'brand_raw': 'Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz', 'hz_advertised_friendly': '2.2000 GHz', 'hz_actual_friendly': '2.2010 GHz', 'hz_advertised': [2200000000, 0], 'hz_actual': [2201000000, 0], 'l2_cache_size': 2621440, 'stepping': 1, 'model': 79, 'family': 6, 'l3_cache_size': 26214400, 'flags': ['3dnow', '3dnowprefetch', 'abm', 'acpi', 'adx', 'aes', 'apic', 'avx', 'avx2', 'bmi1', 'bmi2', 'clflush', 'cmov', 'cx16', 'cx8', 'dca', 'de', 'ds_cpl', 'dtes64', 'dts', 'erms', 'est', 'f16c', 'fma', 'fpu', 'fxsr', 'hle', 'ht', 'ia64', 'intel_pt', 'invpcid', 'lahf_lm', 'mca', 'mce', 'mmx', 'monitor', 'movbe', 'msr', 'mtrr', 'osxsave', 'pae', 'pat', 'pbe', 'pcid', 'pclmulqdq', 'pdcm', 'pge', 'pni', 'popcnt', 'pqe', 'pqm', 'pse', 'pse36', 'rdrnd', 'rdseed', 'rtm', 'sep', 'serial', 'smap', 'smep', 'smx', 'ss', 'sse', 'sse2', 'sse4_1', 'sse4_2', 'ssse3', 'tm', 'tm2', 'tsc', 'tscdeadline', 'vme', 'vmx', 'x2apic', 'xsave', 'xtpr'], 'l2_cache_line_size': 256, 'l2_cache_associativity': 6}\n",
      "Brand_raw = Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\n",
      "Arch_string_raw = AMD64\n",
      "Arch = X86_64\n",
      "Count = 40\n",
      "Python_version = 3.7.10.final.0 (64 bit)\n",
      "___________________________________________\n",
      "Processor (CPU) usage: \n",
      "___________________________________________\n",
      "svmem(total=137322393600, available=15579340800, percent=88.7, used=121743052800, free=15579340800)\n",
      "Cpu_usage = 60.5\n",
      "Ram_usage = 88.7\n",
      "Total_ram = 127.9\n",
      "Used_ram = 113.4\n",
      "Available_ram = 14.5\n",
      "\n",
      "===========================================\n",
      "Processor (GPU) details: \n",
      "___________________________________________\n",
      "For GPU based tasks. There are 8 GPUs in the system and 7 are available. \n",
      "Available GPU IDs with MaxLoad>=0.5 and MaxMem>=0.5 are: [1, 2, 3, 4, 5, 6, 7]\n",
      "___________________________________________\n",
      "Processor (GPU) usage: \n",
      "___________________________________________\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 79% |\n",
      "|  1 |  0% |  4% |\n",
      "|  2 |  0% |  4% |\n",
      "|  3 |  0% |  4% |\n",
      "|  4 |  0% |  4% |\n",
      "|  5 |  0% |  4% |\n",
      "|  6 |  0% |  4% |\n",
      "|  7 | 16% | 13% |\n",
      "\n",
      "===========================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'Windows',\n",
       " {'brand_raw': 'Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz',\n",
       "  'arch_string_raw': 'AMD64',\n",
       "  'arch': 'X86_64',\n",
       "  'count': 40,\n",
       "  'python_version': '3.7.10.final.0 (64 bit)',\n",
       "  'CPU_usage': 60.5,\n",
       "  'RAM_usage': 88.7,\n",
       "  'Total_RAM': 127.9,\n",
       "  'Used_RAM': 113.4,\n",
       "  'Available_RAM': 14.5},\n",
       " [1, 2, 3, 4, 5, 6, 7],\n",
       " 1)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import HumachLab_Global \n",
    "HumachLab_Global.get_system_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca479a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\OneDrive - Deakin University\\_MyResearch\\PhD_Research\\HML_IHC_Sleep_Data_Analysis\n",
      "C:\\Users\\aliem\\OneDrive - Deakin University\\_MyResearch\\PhD_Research\\HML_IHC_Sleep_Data_Analysis\\HumachLab\n",
      "\n",
      "===========================================\n",
      "List of OS platforms and codes\n",
      "___________________________________________\n",
      "0 Darwin\n",
      "1 Windows\n",
      "2 Linux\n",
      "===> \"1 - Windows\" OS is detected.\n",
      "\n",
      "===========================================\n",
      "Processor (CPU) details: \n",
      "___________________________________________\n",
      "{'python_version': '3.7.10.final.0 (64 bit)', 'cpuinfo_version': [8, 0, 0], 'cpuinfo_version_string': '8.0.0', 'arch': 'X86_64', 'bits': 64, 'count': 40, 'arch_string_raw': 'AMD64', 'vendor_id_raw': 'GenuineIntel', 'brand_raw': 'Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz', 'hz_advertised_friendly': '2.2000 GHz', 'hz_actual_friendly': '2.2010 GHz', 'hz_advertised': [2200000000, 0], 'hz_actual': [2201000000, 0], 'l2_cache_size': 2621440, 'stepping': 1, 'model': 79, 'family': 6, 'l3_cache_size': 26214400, 'flags': ['3dnow', '3dnowprefetch', 'abm', 'acpi', 'adx', 'aes', 'apic', 'avx', 'avx2', 'bmi1', 'bmi2', 'clflush', 'cmov', 'cx16', 'cx8', 'dca', 'de', 'ds_cpl', 'dtes64', 'dts', 'erms', 'est', 'f16c', 'fma', 'fpu', 'fxsr', 'hle', 'ht', 'ia64', 'intel_pt', 'invpcid', 'lahf_lm', 'mca', 'mce', 'mmx', 'monitor', 'movbe', 'msr', 'mtrr', 'osxsave', 'pae', 'pat', 'pbe', 'pcid', 'pclmulqdq', 'pdcm', 'pge', 'pni', 'popcnt', 'pqe', 'pqm', 'pse', 'pse36', 'rdrnd', 'rdseed', 'rtm', 'sep', 'serial', 'smap', 'smep', 'smx', 'ss', 'sse', 'sse2', 'sse4_1', 'sse4_2', 'ssse3', 'tm', 'tm2', 'tsc', 'tscdeadline', 'vme', 'vmx', 'x2apic', 'xsave', 'xtpr'], 'l2_cache_line_size': 256, 'l2_cache_associativity': 6}\n",
      "Brand_raw = Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\n",
      "Arch_string_raw = AMD64\n",
      "Arch = X86_64\n",
      "Count = 40\n",
      "Python_version = 3.7.10.final.0 (64 bit)\n",
      "___________________________________________\n",
      "Processor (CPU) usage: \n",
      "___________________________________________\n",
      "svmem(total=137322393600, available=25273208832, percent=81.6, used=112049184768, free=25273208832)\n",
      "Cpu_usage = 100.0\n",
      "Ram_usage = 81.6\n",
      "Total_ram = 127.9\n",
      "Used_ram = 104.4\n",
      "Available_ram = 23.5\n",
      "\n",
      "===========================================\n",
      "Processor (GPU) details: \n",
      "___________________________________________\n",
      "For GPU based tasks. There are 8 GPUs in the system and 7 are available. \n",
      "Available GPU IDs with MaxLoad>=0.5 and MaxMem>=0.5 are: [1, 2, 3, 4, 5, 6, 7]\n",
      "___________________________________________\n",
      "Processor (GPU) usage: \n",
      "___________________________________________\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 79% |\n",
      "|  1 |  0% |  4% |\n",
      "|  2 |  0% |  4% |\n",
      "|  3 |  0% |  4% |\n",
      "|  4 |  0% |  4% |\n",
      "|  5 |  0% |  4% |\n",
      "|  6 |  0% |  4% |\n",
      "|  7 | 10% | 12% |\n",
      "\n",
      "===========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Importing necessary modules\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir(os.getcwd())\n",
    "print(f\"{os.getcwd()}\\HumachLab\")\n",
    "sys.path.append(f\"{os.getcwd()}\\HumachLab\")\n",
    "sys.path.insert(0, os.path.abspath('./HumachLab'))\n",
    "\n",
    "import itertools as it\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import copy\n",
    "from pprint import pprint\n",
    "\n",
    "import glob\n",
    "\n",
    "import math\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib import rc, rcParams\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.utils import resample\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import HumachLab_Global\n",
    "from HumachLab import * \n",
    "# from HumachLab.HumachLab_Global import *\n",
    "# import HumachLab_Global\n",
    "HumachLab_Global.get_system_info()\n",
    "\n",
    "import mne\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]\n",
    "plt.rcParams[\"figure.figsize\"] = (20,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618266e",
   "metadata": {},
   "source": [
    "## Get directory list: Subject-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "363987a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Explore the contents/files in the directory\n",
    "'''\n",
    "\n",
    "def get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=None, exclude=None):\n",
    "    '''\n",
    "    directory: valid path string, path_type: p_file|p_dir, containes: string, extension: valid string file extension \n",
    "    '''\n",
    "    os_path = os.path\n",
    "    list_of_paths = []\n",
    "        \n",
    "    path_keywords = \"*\"\n",
    "    if containes:\n",
    "        path_keywords = f\"{path_keywords}{containes}*\"\n",
    "    \n",
    "    if extension:\n",
    "        path_keywords = f\"{path_keywords}.{extension}\"\n",
    "        \n",
    "    complete_path = f\"{directory}/{path_keywords}\"\n",
    "    print(f\"============> {path_keywords}, {path_type}, {complete_path}\")\n",
    "    \n",
    "    all_paths = glob.glob(complete_path) \n",
    "    all_temp_paths = None\n",
    "    list_of_paths = None\n",
    "    \n",
    "    if path_type:\n",
    "        if path_type==\"p_file\":\n",
    "            all_temp_paths = [path.replace(\"\\\\\", \"/\") for path in all_paths if (os_path.exists(path) and os_path.isfile(path))]\n",
    "        if path_type==\"p_dir\":\n",
    "            all_temp_paths = [path.replace(\"\\\\\", \"/\") for path in all_paths if (os_path.exists(path) and os_path.isdir(path))]   \n",
    "    else:\n",
    "        all_temp_paths = [path.replace(\"\\\\\", \"/\") for path in all_paths]\n",
    "        \n",
    "    if exclude:\n",
    "        # print(all_temp_paths)\n",
    "        # print(len(all_temp_paths), exclude)\n",
    "        # list_of_paths = [path for path in all_temp_paths for ex in exclude if ex not in path]\n",
    "        # list_of_paths = [path for ex in exclude for path in all_temp_paths if ex not in path]\n",
    "        list_of_paths = [path for path in all_temp_paths if not any((ex in path) for ex in exclude)]\n",
    "        # list_of_paths = [path for ex in exclude if any(ex not in path for path in all_temp_paths)]\n",
    "        # any(substring in string for substring in substring_list)\n",
    "        # print(len(list_of_paths))\n",
    "    else:\n",
    "        list_of_paths = all_temp_paths.copy()\n",
    "    \n",
    "    return list_of_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8dfeea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Results//_Combined/STP_From_Same_Stages', './Results//_Classification')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_directory = \"./Results/\" \n",
    "data_dir = \"/_Combined/\"\n",
    "# data_directory = \"./Results/\" \n",
    "# tran_directory = \"/Transition_Matrices\"\n",
    "# result_directory = \"./Results/_Combined\" \n",
    "result_subdirctory = \"/_Classification\"\n",
    "prob_cal_from_all = False  \n",
    "result_subdir = ['STP_From_Same_Stages', 'STP_From_All_Stages'][int(prob_cal_from_all)]\n",
    "data_directory = f\"{root_directory}{data_dir}{result_subdir}\"\n",
    "result_directory = f\"{root_directory}{result_subdirctory}\" \n",
    "wake_state_trimmed = True  \n",
    "result_subdirectory = f\"Subject_One_Night{'_TrimW' if wake_state_trimmed else ''}\"  ###\"Subject_Combined_Record\"  \"Subject_Separate_Record\"  \"Subject_One_Night\" \"Subject_One_Night_TrimW\"   ## Change for new type of result \n",
    "data_subdirectory = [\"CAP_Sleep\", \"Sleep_EDFX\", \"SDRC\"] \n",
    "metadata_subdirectory = [\"CAP_Sleep\", \"Sleep_EDFX\", \"SDRC\"] \n",
    "tran_matrix_type = [\"count\", \"dura\", \"proba\"] \n",
    "annotation_type = ['annot', 'tran'] \n",
    "tran_step = 3\n",
    "exclude_contents_in_dataset_directory = [\"SHA256SUMS\", \"RECORDS\"]\n",
    "exclude_contents_in_result_directory = [\"SHA256SUMS\", \"RECORDS\", \"all_annotions\", \"annot_sequence\", \"transition_sequence\", \"hypno\", \"DATASET_CHANGELOG\"]\n",
    "sleep_stage_labels = ['W', 'S1', 'S2', 'S3', 'S4', 'REM']\n",
    "sleep_stage_labels_dict = {'W':0, 'S1':1, 'S2':2, 'S3':3, 'S4':4, 'REM':5}\n",
    "sleep_stage_names_dict = {'W':0, 'S1':1, 'S2':2, 'S3':3, 'S4':4, 'R':5}\n",
    "list_of_paths = None \n",
    "data_directory, result_directory\n",
    "\n",
    "# directory = dataset_directory\n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=None, exclude=None) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=\"p_file\", containes=None, extension=None, exclude=None) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=\"p_dir\", containes=None, extension=None, exclude=None) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=\"nfle\", extension=None, exclude=None) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=\"edf\", exclude=None) \n",
    "# list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=None, exclude=exclude_contents_in_dataset_directory) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=None, exclude=None) \n",
    "# # pprint(list_of_paths)\n",
    "# list_of_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddead82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5023ec69",
   "metadata": {},
   "source": [
    "### Get basic information and transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b43ca5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category_Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Total_Count</th>\n",
       "      <th>Male_Count</th>\n",
       "      <th>Female_Count</th>\n",
       "      <th>Total_AgeRange</th>\n",
       "      <th>Male_AgeRange</th>\n",
       "      <th>Female_AgeRange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Bruxism</td>\n",
       "      <td>brux</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23 - 34</td>\n",
       "      <td>23 - 34</td>\n",
       "      <td>0 - 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Sleep-Disordered Breathing</td>\n",
       "      <td>sdb</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>65 - 78</td>\n",
       "      <td>65 - 78</td>\n",
       "      <td>0 - 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Insomnia</td>\n",
       "      <td>ins</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>47 - 82</td>\n",
       "      <td>54 - 82</td>\n",
       "      <td>47 - 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Narcolepsy</td>\n",
       "      <td>narco</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18 - 44</td>\n",
       "      <td>24 - 43</td>\n",
       "      <td>18 - 44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Nocturnal Frontal Lobe Epilepsy</td>\n",
       "      <td>nfle</td>\n",
       "      <td>40</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>14 - 67</td>\n",
       "      <td>14 - 44</td>\n",
       "      <td>16 - 67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Periodic Leg Movements</td>\n",
       "      <td>plm</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>40 - 62</td>\n",
       "      <td>40 - 62</td>\n",
       "      <td>50 - 52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>REM Behavior Disorder</td>\n",
       "      <td>rbd</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>58 - 82</td>\n",
       "      <td>58 - 82</td>\n",
       "      <td>73 - 76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>23 - 42</td>\n",
       "      <td>23 - 34</td>\n",
       "      <td>24 - 42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108</td>\n",
       "      <td>66</td>\n",
       "      <td>42</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>16 - 76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Sleep Disorders</td>\n",
       "      <td>dis</td>\n",
       "      <td>92</td>\n",
       "      <td>59</td>\n",
       "      <td>33</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>16 - 76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>23 - 42</td>\n",
       "      <td>23 - 34</td>\n",
       "      <td>24 - 42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108</td>\n",
       "      <td>66</td>\n",
       "      <td>42</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>16 - 76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>SC</td>\n",
       "      <td>n</td>\n",
       "      <td>153</td>\n",
       "      <td>71</td>\n",
       "      <td>82</td>\n",
       "      <td>25 - 101</td>\n",
       "      <td>26 - 97</td>\n",
       "      <td>25 - 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>ST</td>\n",
       "      <td>n</td>\n",
       "      <td>44</td>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>18 - 79</td>\n",
       "      <td>18 - 79</td>\n",
       "      <td>20 - 60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>Total</td>\n",
       "      <td>n</td>\n",
       "      <td>197</td>\n",
       "      <td>101</td>\n",
       "      <td>96</td>\n",
       "      <td>19 - 101</td>\n",
       "      <td>19 - 97</td>\n",
       "      <td>21 - 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>197</td>\n",
       "      <td>101</td>\n",
       "      <td>96</td>\n",
       "      <td>18 - 101</td>\n",
       "      <td>18 - 97</td>\n",
       "      <td>20 - 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>Total</td>\n",
       "      <td>n</td>\n",
       "      <td>197</td>\n",
       "      <td>101</td>\n",
       "      <td>96</td>\n",
       "      <td>19 - 101</td>\n",
       "      <td>19 - 97</td>\n",
       "      <td>21 - 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>Insomnia</td>\n",
       "      <td>ins</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>28-62</td>\n",
       "      <td>41-53</td>\n",
       "      <td>29-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>18-63</td>\n",
       "      <td>18-63</td>\n",
       "      <td>19-57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>Total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>18-63</td>\n",
       "      <td>18-63</td>\n",
       "      <td>19-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>Sleep Disorders</td>\n",
       "      <td>dis</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>28-62</td>\n",
       "      <td>41-53</td>\n",
       "      <td>29-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>18-63</td>\n",
       "      <td>18-63</td>\n",
       "      <td>19-57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     #     Dataset                    Category_Name Category  Total_Count  \\\n",
       "0    1   CAP_Sleep                          Bruxism     brux            2   \n",
       "1    2   CAP_Sleep       Sleep-Disordered Breathing      sdb            4   \n",
       "2    3   CAP_Sleep                         Insomnia      ins            9   \n",
       "3    4   CAP_Sleep                       Narcolepsy    narco            5   \n",
       "4    5   CAP_Sleep  Nocturnal Frontal Lobe Epilepsy     nfle           40   \n",
       "5    6   CAP_Sleep           Periodic Leg Movements      plm           10   \n",
       "6    7   CAP_Sleep            REM Behavior Disorder      rbd           22   \n",
       "7    8   CAP_Sleep          No Pathology (Controls)        n           16   \n",
       "8   10   CAP_Sleep                            Total      NaN          108   \n",
       "9   11   CAP_Sleep                  Sleep Disorders      dis           92   \n",
       "10  12   CAP_Sleep          No Pathology (Controls)        n           16   \n",
       "11  13   CAP_Sleep                            Total      NaN          108   \n",
       "12   1  Sleep_EDFX                               SC        n          153   \n",
       "13   2  Sleep_EDFX                               ST        n           44   \n",
       "14   3  Sleep_EDFX                            Total        n          197   \n",
       "15   4  Sleep_EDFX          No Pathology (Controls)        n          197   \n",
       "16   5  Sleep_EDFX                            Total        n          197   \n",
       "17   1        SDRC                         Insomnia      ins           11   \n",
       "18   8        SDRC          No Pathology (Controls)        n           11   \n",
       "19  10        SDRC                            Total      NaN           22   \n",
       "20  11        SDRC                  Sleep Disorders      dis           11   \n",
       "21  12        SDRC          No Pathology (Controls)        n           11   \n",
       "\n",
       "    Male_Count  Female_Count Total_AgeRange Male_AgeRange Female_AgeRange  \n",
       "0            2             0        23 - 34       23 - 34           0 - 0  \n",
       "1            4             0        65 - 78       65 - 78           0 - 0  \n",
       "2            4             5        47 - 82       54 - 82         47 - 59  \n",
       "3            2             3        18 - 44       24 - 43         18 - 44  \n",
       "4           21            19        14 - 67       14 - 44         16 - 67  \n",
       "5            7             3        40 - 62       40 - 62         50 - 52  \n",
       "6           19             3        58 - 82       58 - 82         73 - 76  \n",
       "7            7             9        23 - 42       23 - 34         24 - 42  \n",
       "8           66            42        14 - 82       14 - 82         16 - 76  \n",
       "9           59            33        14 - 82       14 - 82         16 - 76  \n",
       "10           7             9        23 - 42       23 - 34         24 - 42  \n",
       "11          66            42        14 - 82       14 - 82         16 - 76  \n",
       "12          71            82       25 - 101       26 - 97        25 - 101  \n",
       "13          30            14        18 - 79       18 - 79         20 - 60  \n",
       "14         101            96       19 - 101       19 - 97        21 - 101  \n",
       "15         101            96       18 - 101       18 - 97        20 - 101  \n",
       "16         101            96       19 - 101       19 - 97        21 - 101  \n",
       "17           2             9          28-62         41-53           29-62  \n",
       "18           6             5          18-63         18-63           19-57  \n",
       "19           8            14          18-63         18-63           19-62  \n",
       "20           2             9          28-62         41-53           29-62  \n",
       "21           6             5          18-63         18-63           19-57  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_demography_info(tmp_dir): \n",
    "#     list_of_tran_mat_paths = [] \n",
    "    all_demography_df = pd.read_csv(f\"{tmp_dir}/all_demography.csv\", index_col=False) \n",
    "    all_demography_detail_df = pd.read_csv(f\"{tmp_dir}/all_demography_detail.csv\", index_col=False)  \n",
    "    return all_demography_df, all_demography_detail_df \n",
    "\n",
    "\n",
    "info_type=\"sub\" ##\"sub\"/\"file\"  ## Change for new type of result \n",
    "annot_type = annotation_type[0]   ## Change for different data preparation for 'annot' and 'tran' \n",
    "\n",
    "all_demography_df, all_demography_detail_df = get_demography_info(data_directory) \n",
    "all_demography_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80782a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Explore the contents/files in the directory\n",
    "# '''\n",
    "\n",
    "# def get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=None, exclude=None):\n",
    "#     '''\n",
    "#     directory: valid path string, path_type: p_file|p_dir, containes: string, extension: valid string file extension \n",
    "#     '''\n",
    "#     os_path = os.path\n",
    "#     list_of_paths = []\n",
    "        \n",
    "#     path_keywords = \"*\"\n",
    "#     if containes:\n",
    "#         path_keywords = f\"{path_keywords}{containes}*\"\n",
    "    \n",
    "#     if extension:\n",
    "#         path_keywords = f\"{path_keywords}.{extension}\"\n",
    "        \n",
    "#     complete_path = f\"{directory}/{path_keywords}\"\n",
    "#     print(f\"============> {path_keywords}, {path_type}, {complete_path}\")\n",
    "    \n",
    "#     all_paths = glob.glob(complete_path) \n",
    "#     all_temp_paths = None\n",
    "#     list_of_paths = None\n",
    "    \n",
    "#     if path_type:\n",
    "#         if path_type==\"p_file\":\n",
    "#             all_temp_paths = [path.replace(\"\\\\\", \"/\") for path in all_paths if (os_path.exists(path) and os_path.isfile(path))]\n",
    "#         if path_type==\"p_dir\":\n",
    "#             all_temp_paths = [path.replace(\"\\\\\", \"/\") for path in all_paths if (os_path.exists(path) and os_path.isdir(path))]   \n",
    "#     else:\n",
    "#         all_temp_paths = [path.replace(\"\\\\\", \"/\") for path in all_paths]\n",
    "        \n",
    "#     if exclude:\n",
    "#         # print(all_temp_paths)\n",
    "#         # print(len(all_temp_paths), exclude)\n",
    "#         # list_of_paths = [path for path in all_temp_paths for ex in exclude if ex not in path]\n",
    "#         # list_of_paths = [path for ex in exclude for path in all_temp_paths if ex not in path]\n",
    "#         list_of_paths = [path for path in all_temp_paths if not any((ex in path) for ex in exclude)]\n",
    "#         # list_of_paths = [path for ex in exclude if any(ex not in path for path in all_temp_paths)]\n",
    "#         # any(substring in string for substring in substring_list)\n",
    "#         # print(len(list_of_paths))\n",
    "#     else:\n",
    "#         list_of_paths = all_temp_paths.copy()\n",
    "    \n",
    "#     return list_of_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b5606ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_directory = \"./Results/\" \n",
    "# data_directory = \"/_Combined\"\n",
    "# prob_cal_from_all = False \n",
    "# data_subdir = ['STP_From_Same_Stages', 'STP_From_All_Stages'][int(prob_cal_from_all)] \n",
    "# wake_state_trimmed = False \n",
    "# data_subdirectory = f\"Subject_One_Night{'_TrimW' if wake_state_trimmed else ''}\"  ###\"Subject_Combined_Record\"  \"Subject_Separate_Record\"  \"Subject_One_Night\" \"Subject_One_Night_TrimW\"   ## Change for new type of result \n",
    "# # result_directory = f\"./Results/_Classification/{result_subdir}\" \n",
    "# result_directory = \"./Results/_Classification\" \n",
    "# metadata_subdirectory = [\"CAP_Sleep\", \"Sleep_EDFX\", \"SDRC\"] \n",
    "# dataset_list = [\"CAP_Sleep\", \"Sleep_EDFX\", \"SDRC\"] \n",
    "# tran_matrix_type = [\"count\", \"dura\", \"proba\"] \n",
    "# annotation_type = ['annot', 'tran']\n",
    "# tran_step = 2\n",
    "# exclude_contents_in_dataset_directory = [\"SHA256SUMS\", \"RECORDS\"]\n",
    "# exclude_contents_in_result_directory = [\"SHA256SUMS\", \"RECORDS\", \"all_annotations\", \"annot_sequence\", \"transition_sequence\", \"hypno\", \"DATASET_CHANGELOG\"]\n",
    "# sleep_stage_labels = ['W', 'S1', 'S2', 'S3', 'S4', 'REM']\n",
    "# sleep_stage_labels_dict = {'W':0, 'S1':1, 'S2':2, 'S3':3, 'S4':4, 'REM':5}\n",
    "# sleep_stage_names_dict = {'W':0, 'S1':1, 'S2':2, 'S3':3, 'S4':4, 'R':5}\n",
    "# list_of_paths = None \n",
    "\n",
    "\n",
    "# info_type=\"sub\" ##\"sub\"/\"file\"  ## Change for new type of result\n",
    "# annot_type = annotation_type[0]   ## Change for different data preparation for 'annot' and 'tran' \n",
    "\n",
    "\n",
    "# directory = dataset_directory\n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=None, exclude=None) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=\"p_file\", containes=None, extension=None, exclude=None) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=\"p_dir\", containes=None, extension=None, exclude=None) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=\"nfle\", extension=None, exclude=None) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=\"edf\", exclude=None) \n",
    "# list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=None, exclude=exclude_contents_in_dataset_directory) \n",
    "# # list_of_paths = get_list_of_paths_from_a_directory(directory, path_type=None, containes=None, extension=None, exclude=None) \n",
    "# # pprint(list_of_paths)\n",
    "# list_of_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c2d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "921da830",
   "metadata": {},
   "source": [
    "### Get basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1fb6106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category_Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Total_Count</th>\n",
       "      <th>Male_Count</th>\n",
       "      <th>Female_Count</th>\n",
       "      <th>Total_AgeRange</th>\n",
       "      <th>Male_AgeRange</th>\n",
       "      <th>Female_AgeRange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Bruxism</td>\n",
       "      <td>brux</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23 - 34</td>\n",
       "      <td>23 - 34</td>\n",
       "      <td>0 - 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Sleep-Disordered Breathing</td>\n",
       "      <td>sdb</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>65 - 78</td>\n",
       "      <td>65 - 78</td>\n",
       "      <td>0 - 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Insomnia</td>\n",
       "      <td>ins</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>47 - 82</td>\n",
       "      <td>54 - 82</td>\n",
       "      <td>47 - 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Narcolepsy</td>\n",
       "      <td>narco</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18 - 44</td>\n",
       "      <td>24 - 43</td>\n",
       "      <td>18 - 44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Nocturnal Frontal Lobe Epilepsy</td>\n",
       "      <td>nfle</td>\n",
       "      <td>40</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>14 - 67</td>\n",
       "      <td>14 - 44</td>\n",
       "      <td>16 - 67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Periodic Leg Movements</td>\n",
       "      <td>plm</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>40 - 62</td>\n",
       "      <td>40 - 62</td>\n",
       "      <td>50 - 52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>REM Behavior Disorder</td>\n",
       "      <td>rbd</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>58 - 82</td>\n",
       "      <td>58 - 82</td>\n",
       "      <td>73 - 76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>23 - 42</td>\n",
       "      <td>23 - 34</td>\n",
       "      <td>24 - 42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108</td>\n",
       "      <td>66</td>\n",
       "      <td>42</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>16 - 76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Sleep Disorders</td>\n",
       "      <td>dis</td>\n",
       "      <td>92</td>\n",
       "      <td>59</td>\n",
       "      <td>33</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>16 - 76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>23 - 42</td>\n",
       "      <td>23 - 34</td>\n",
       "      <td>24 - 42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>Total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108</td>\n",
       "      <td>66</td>\n",
       "      <td>42</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>14 - 82</td>\n",
       "      <td>16 - 76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>SC</td>\n",
       "      <td>n</td>\n",
       "      <td>153</td>\n",
       "      <td>71</td>\n",
       "      <td>82</td>\n",
       "      <td>25 - 101</td>\n",
       "      <td>26 - 97</td>\n",
       "      <td>25 - 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>ST</td>\n",
       "      <td>n</td>\n",
       "      <td>44</td>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>18 - 79</td>\n",
       "      <td>18 - 79</td>\n",
       "      <td>20 - 60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>Total</td>\n",
       "      <td>n</td>\n",
       "      <td>197</td>\n",
       "      <td>101</td>\n",
       "      <td>96</td>\n",
       "      <td>19 - 101</td>\n",
       "      <td>19 - 97</td>\n",
       "      <td>21 - 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>197</td>\n",
       "      <td>101</td>\n",
       "      <td>96</td>\n",
       "      <td>18 - 101</td>\n",
       "      <td>18 - 97</td>\n",
       "      <td>20 - 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>Total</td>\n",
       "      <td>n</td>\n",
       "      <td>197</td>\n",
       "      <td>101</td>\n",
       "      <td>96</td>\n",
       "      <td>19 - 101</td>\n",
       "      <td>19 - 97</td>\n",
       "      <td>21 - 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>Insomnia</td>\n",
       "      <td>ins</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>28-62</td>\n",
       "      <td>41-53</td>\n",
       "      <td>29-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>18-63</td>\n",
       "      <td>18-63</td>\n",
       "      <td>19-57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>Total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>18-63</td>\n",
       "      <td>18-63</td>\n",
       "      <td>19-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>Sleep Disorders</td>\n",
       "      <td>dis</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>28-62</td>\n",
       "      <td>41-53</td>\n",
       "      <td>29-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12</td>\n",
       "      <td>SDRC</td>\n",
       "      <td>No Pathology (Controls)</td>\n",
       "      <td>n</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>18-63</td>\n",
       "      <td>18-63</td>\n",
       "      <td>19-57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     #     Dataset                    Category_Name Category  Total_Count  \\\n",
       "0    1   CAP_Sleep                          Bruxism     brux            2   \n",
       "1    2   CAP_Sleep       Sleep-Disordered Breathing      sdb            4   \n",
       "2    3   CAP_Sleep                         Insomnia      ins            9   \n",
       "3    4   CAP_Sleep                       Narcolepsy    narco            5   \n",
       "4    5   CAP_Sleep  Nocturnal Frontal Lobe Epilepsy     nfle           40   \n",
       "5    6   CAP_Sleep           Periodic Leg Movements      plm           10   \n",
       "6    7   CAP_Sleep            REM Behavior Disorder      rbd           22   \n",
       "7    8   CAP_Sleep          No Pathology (Controls)        n           16   \n",
       "8   10   CAP_Sleep                            Total      NaN          108   \n",
       "9   11   CAP_Sleep                  Sleep Disorders      dis           92   \n",
       "10  12   CAP_Sleep          No Pathology (Controls)        n           16   \n",
       "11  13   CAP_Sleep                            Total      NaN          108   \n",
       "12   1  Sleep_EDFX                               SC        n          153   \n",
       "13   2  Sleep_EDFX                               ST        n           44   \n",
       "14   3  Sleep_EDFX                            Total        n          197   \n",
       "15   4  Sleep_EDFX          No Pathology (Controls)        n          197   \n",
       "16   5  Sleep_EDFX                            Total        n          197   \n",
       "17   1        SDRC                         Insomnia      ins           11   \n",
       "18   8        SDRC          No Pathology (Controls)        n           11   \n",
       "19  10        SDRC                            Total      NaN           22   \n",
       "20  11        SDRC                  Sleep Disorders      dis           11   \n",
       "21  12        SDRC          No Pathology (Controls)        n           11   \n",
       "\n",
       "    Male_Count  Female_Count Total_AgeRange Male_AgeRange Female_AgeRange  \n",
       "0            2             0        23 - 34       23 - 34           0 - 0  \n",
       "1            4             0        65 - 78       65 - 78           0 - 0  \n",
       "2            4             5        47 - 82       54 - 82         47 - 59  \n",
       "3            2             3        18 - 44       24 - 43         18 - 44  \n",
       "4           21            19        14 - 67       14 - 44         16 - 67  \n",
       "5            7             3        40 - 62       40 - 62         50 - 52  \n",
       "6           19             3        58 - 82       58 - 82         73 - 76  \n",
       "7            7             9        23 - 42       23 - 34         24 - 42  \n",
       "8           66            42        14 - 82       14 - 82         16 - 76  \n",
       "9           59            33        14 - 82       14 - 82         16 - 76  \n",
       "10           7             9        23 - 42       23 - 34         24 - 42  \n",
       "11          66            42        14 - 82       14 - 82         16 - 76  \n",
       "12          71            82       25 - 101       26 - 97        25 - 101  \n",
       "13          30            14        18 - 79       18 - 79         20 - 60  \n",
       "14         101            96       19 - 101       19 - 97        21 - 101  \n",
       "15         101            96       18 - 101       18 - 97        20 - 101  \n",
       "16         101            96       19 - 101       19 - 97        21 - 101  \n",
       "17           2             9          28-62         41-53           29-62  \n",
       "18           6             5          18-63         18-63           19-57  \n",
       "19           8            14          18-63         18-63           19-62  \n",
       "20           2             9          28-62         41-53           29-62  \n",
       "21           6             5          18-63         18-63           19-57  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_demography_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d7b836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Subject_Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux1</td>\n",
       "      <td>brux1</td>\n",
       "      <td>brux</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux2</td>\n",
       "      <td>brux2</td>\n",
       "      <td>brux</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb1</td>\n",
       "      <td>sdb1</td>\n",
       "      <td>sdb</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb2</td>\n",
       "      <td>sdb2</td>\n",
       "      <td>sdb</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb3</td>\n",
       "      <td>sdb3</td>\n",
       "      <td>sdb</td>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>SDRC</td>\n",
       "      <td>ins1107</td>\n",
       "      <td>ins1107</td>\n",
       "      <td>ins</td>\n",
       "      <td>7</td>\n",
       "      <td>F</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>SDRC</td>\n",
       "      <td>ins1108</td>\n",
       "      <td>ins1108</td>\n",
       "      <td>ins</td>\n",
       "      <td>8</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>SDRC</td>\n",
       "      <td>ins1109</td>\n",
       "      <td>ins1109</td>\n",
       "      <td>ins</td>\n",
       "      <td>9</td>\n",
       "      <td>F</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>SDRC</td>\n",
       "      <td>ins1110</td>\n",
       "      <td>ins1110</td>\n",
       "      <td>ins</td>\n",
       "      <td>10</td>\n",
       "      <td>M</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>SDRC</td>\n",
       "      <td>ins1111</td>\n",
       "      <td>ins1111</td>\n",
       "      <td>ins</td>\n",
       "      <td>11</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>327 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Dataset File_Name Subject_Name Category  Subject_ID Gender  Age\n",
       "0    CAP_Sleep     brux1        brux1     brux           1      M   34\n",
       "1    CAP_Sleep     brux2        brux2     brux           2      M   23\n",
       "2    CAP_Sleep      sdb1         sdb1      sdb           1      M   65\n",
       "3    CAP_Sleep      sdb2         sdb2      sdb           2      M   77\n",
       "4    CAP_Sleep      sdb3         sdb3      sdb           3      M   78\n",
       "..         ...       ...          ...      ...         ...    ...  ...\n",
       "322       SDRC   ins1107      ins1107      ins           7      F   62\n",
       "323       SDRC   ins1108      ins1108      ins           8      F   57\n",
       "324       SDRC   ins1109      ins1109      ins           9      F   32\n",
       "325       SDRC   ins1110      ins1110      ins          10      M   41\n",
       "326       SDRC   ins1111      ins1111      ins          11      F   50\n",
       "\n",
       "[327 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_demography_detail_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adc199da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6182d47",
   "metadata": {},
   "source": [
    "### Get transition matrix or features information from transition probabilities and P/AUC information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "685721b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Annot_Proba_Transition3.csv\n",
      "./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Annot_Proba_Transition3_STAT_bin.csv\n"
     ]
    }
   ],
   "source": [
    "tran_mat_type = ['Proba', 'Dura', 'Count']\n",
    "mat_info_type = 0 \n",
    "print( f\"{data_directory}/{result_subdirectory}/{annot_type.capitalize()}_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv\" ) \n",
    "print( f\"{data_directory}/{result_subdirectory}/{annot_type.capitalize()}_{tran_mat_type[mat_info_type]}_Transition{tran_step}_STAT_bin.csv\" ) \n",
    "# print( f\"{data_directory}/{result_subdirectory}/Annot_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv\" ) Annot_Proba_Transition3_STAT_bin\n",
    "# data_directory, result_subdirectory, tran_mat_type, mat_info_type, tran_step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f70723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_STP_matrix_files_for_PhDStudy01(mat_info_type, tran_step): \n",
    "    tran_mat_type = ['Proba', 'Dura', 'Count']\n",
    "    ## ['EDFXSCRem', 'CAPOnly', 'CAPandSDRC']\n",
    "    \n",
    "    annot_proba_file_list = [] \n",
    "    annot_proba_file_list.append(f'./Results//_Combined/STP_From_Same_Stages/Subject_One_Night/Annot_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv') \n",
    "    annot_proba_file_list.append(f'./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Annot_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv') \n",
    "    annot_proba_file_list.append(f'./Results//_Combined/STP_From_All_Stages/Subject_One_Night/Annot_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv') \n",
    "    annot_proba_file_list.append(f'./Results//_Combined/STP_From_All_Stages/Subject_One_Night_TrimW/Annot_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv') \n",
    "    tran_proba_file_list = [] \n",
    "    tran_proba_file_list.append(f'./Results//_Combined/STP_From_Same_Stages/Subject_One_Night/Tran_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv') \n",
    "    tran_proba_file_list.append(f'./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Tran_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv') \n",
    "    tran_proba_file_list.append(f'./Results//_Combined/STP_From_All_Stages/Subject_One_Night/Tran_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv') \n",
    "    tran_proba_file_list.append(f'./Results//_Combined/STP_From_All_Stages/Subject_One_Night_TrimW/Tran_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv') \n",
    "    return annot_proba_file_list, tran_proba_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f71d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_for_file_conditions2(mat_type, STP_From_All, TrimW, mat_info_type, tran_step):\n",
    "    print(f'Reading criteria: {mat_type}, {STP_From_All}, {TrimW}, {mat_info_type}, {tran_step}')\n",
    "    annot_proba_file_list, tran_proba_file_list = get_all_STP_matrix_files_for_PhDStudy01(mat_info_type, tran_step) \n",
    "    expected_list = annot_proba_file_list if mat_type==0 else tran_proba_file_list \n",
    "    ind = 0 \n",
    "    if STP_From_All==1 and TrimW==1:\n",
    "        ind = 3\n",
    "    elif STP_From_All==1 and TrimW==0:\n",
    "        ind = 2 \n",
    "    elif STP_From_All==0 and TrimW==1:\n",
    "        ind = 1\n",
    "    else:\n",
    "        ind = 0 \n",
    "        \n",
    "    filename = expected_list[ind] \n",
    "    print(f'Reading data from the file: {ind}, {filename}') \n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "972b9706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_for_file_conditions(mat_type, STP_From_All, TrimW, mat_info_type, tran_step):\n",
    "    print(f'Reading criteria: {mat_type}, {STP_From_All}, {TrimW}, {mat_info_type}, {tran_step}')\n",
    "    \n",
    "    filename = f\"{data_directory}/{result_subdirectory}/{annot_type.capitalize()}_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv\"  \n",
    "    stat_filename = f\"{data_directory}/{result_subdirectory}/{annot_type.capitalize()}_{tran_mat_type[mat_info_type]}_Transition{tran_step}_STAT_bin.csv\" \n",
    "    \n",
    "    print(f'Reading data from the file: {filename}\\nStatistics from the file: {stat_filename}') \n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    stat_df = pd.read_csv(stat_filename, index_col=False)\n",
    "    return df, stat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61847c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_name='Class' \n",
    "# mat_type = 0 ## annot_type  # mat_type=annot/tran -- 0/1 \n",
    "# STP_From_All = 0 ## int(tran_info_cal_from_all)  # STP_From_All=same/all -- 0/1 \n",
    "# TrimW = 1 ## int(wake_state_trimmed)  # TrimW=normal/trimmed -- 0/1 \n",
    "# mat_info_type = 0 ## int(tran_info_type) # Proba/Dura/Count -- 0/1/2\n",
    "# tran_step = 3 ## int (tran step) # one level=2 step, two level=3 step -- 2/3\n",
    "\n",
    "# dataset, stat_dataset = get_dataset_for_file_conditions(mat_type, STP_From_All, TrimW, mat_info_type, tran_step=tran_step) # mat_type=annot/tran, STP_From_All=same/all, TrimW=normal/trimmed, mat_info_type=proba/dura/count, tran_step=2/3 \n",
    "# dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79de4f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0693ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ddba49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "385efeb9",
   "metadata": {},
   "source": [
    "#### Prepare final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9040a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf83191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_age_category_to_class(dat_set, demo_det, source_cols=['Subject_Name', 'File_Name'], age_col = 'Age', age_ranges = [[19, 30], [31, 40]], class_name='Class', multi_class=True, healthy_only=True):\n",
    "#     tmp_df = dataset.merge(all_demography_detail_df, how='inner', left_on=[source_cols[0]], right_on=[source_cols[1]])\n",
    "#     tmp_df\n",
    "\n",
    "#     dat_set.insert(3, age_col, tmp_df[age_col].values) \n",
    "#     dat_set\n",
    "\n",
    "#     dat_set = dat_set[ ((dat_set[age_col]>=age_ranges[0][0]) & (dat_set[age_col]<=age_ranges[0][1])) | ((dat_set[age_col]>=age_ranges[1][0]) & (dat_set[age_col]<=age_ranges[1][1])) ]  \n",
    "#     dat_set.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     tmp_age_range = [list(range(age_ranges[0][0], age_ranges[0][1]+1)), list(range(age_ranges[1][0], age_ranges[1][1]+1))] \n",
    "#     tmp_age_range \n",
    "\n",
    "#     cls_map = {} \n",
    "#     for i, lst in enumerate(tmp_age_range): \n",
    "#         for l in lst:\n",
    "#             cls_map[l]=i\n",
    "#     cls_map\n",
    "    \n",
    "#     all_cols = dat_set.columns.values.tolist() \n",
    "#     if (age_col in all_cols):\n",
    "#         if (class_name in all_cols): \n",
    "#             dat_set = dat_set.drop([class_name], axis=1)\n",
    "#         dat_set = dat_set.rename(columns={age_col: class_name})\n",
    "    \n",
    "#     all_cols2 = dat_set.columns.values.tolist() \n",
    "#     if (class_name not in all_cols) and (class_name in all_cols2):\n",
    "#         dat_set.replace({class_name: cls_map}, inplace=True) \n",
    "        \n",
    "#     if healthy_only==True: \n",
    "#         pass\n",
    "#     return cls_map, dat_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e742ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_demography_detail_df\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdac87a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_age_category_to_class(dat_set, demo_det, source_cols=['Subject_Name', 'File_Name'], age_col = 'Age', age_ranges = [[19, 30], [31, 40]], class_name='Class', multi_class=True, healthy_only=True):\n",
    "    if healthy_only==True: \n",
    "        dat_set = dat_set[(dat_set['Category']=='n')] \n",
    "    \n",
    "    tmp_df = dat_set.merge(demo_det, how='inner', left_on=[source_cols[0]], right_on=[source_cols[1]])\n",
    "    tmp_df\n",
    "#     print(tmp_df.shape[0], dat_set.shape[0])\n",
    "    assert tmp_df.shape[0]==dat_set.shape[0], \"Something hapend. No shape after dataframe joining is same...\"\n",
    "\n",
    "    dat_set.insert(3, age_col, tmp_df[age_col].values) \n",
    "    dat_set\n",
    "#     print(age_ranges, dat_set)\n",
    "\n",
    "#     dat_set = dat_set[ ((dat_set[age_col]>=age_ranges[0][0]) & (dat_set[age_col]<=age_ranges[0][1])) | ((dat_set[age_col]>=age_ranges[1][0]) & (dat_set[age_col]<=age_ranges[1][1])) ]  \n",
    "#     dat_set.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    tmp_age_range = []\n",
    "    tmp_dat_set = pd.DataFrame() \n",
    "    for i, ar in enumerate(age_ranges):\n",
    "        sel_df = dat_set[((dat_set[age_col]>=ar[0]) & (dat_set[age_col]<=ar[1]))] \n",
    "        tmp_dat_set = pd.concat([tmp_dat_set, sel_df ] )\n",
    "        tmp_age_range.append( sorted(sel_df[age_col].values.tolist()) ) \n",
    "        tmp_age_range \n",
    "#         print(tmp_age_range, dat_set)\n",
    "    dat_set = tmp_dat_set.copy()\n",
    "    dat_set.reset_index(drop=True, inplace=True)\n",
    "#     print(dat_set)\n",
    "\n",
    "    cls_map = {} \n",
    "    for i, lst in enumerate(tmp_age_range): \n",
    "        for l in lst:\n",
    "            cls_map[l]=i\n",
    "    cls_map\n",
    "    \n",
    "    all_cols = dat_set.columns.values.tolist() \n",
    "    if (age_col in all_cols):\n",
    "        if (class_name in all_cols): \n",
    "            dat_set = dat_set.drop([class_name], axis=1)\n",
    "        dat_set = dat_set.rename(columns={age_col: class_name})\n",
    "    \n",
    "    all_cols2 = dat_set.columns.values.tolist() \n",
    "    if (class_name not in all_cols) and (class_name in all_cols2):\n",
    "        dat_set.replace({class_name: cls_map}, inplace=True) \n",
    "    return cls_map, dat_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0834ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # label_map, dataset2 = map_age_category_to_class(dataset.copy(), all_demography_detail_df.copy(), source_cols=['Subject_Name', 'File_Name'], age_col = 'Age', age_ranges = [[19, 30], [31, 200]], class_name='Class', multi_class=True, healthy_only=True) ##19-101 \n",
    "# label_map, dataset2 = map_age_category_to_class(dataset.copy(), all_demography_detail_df.copy(), source_cols=['Subject_Name', 'File_Name'], age_col = 'Age', age_ranges = [[1, 30], [40, 60], [70, 200]], class_name='Class', multi_class=True, healthy_only=True) ##19-101 \n",
    "# print(label_map)\n",
    "# dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef2ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2f967fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_category_to_class(dat_set, source_col='Category', class_name='Class', removable_cats=None, multi_class=True, age_data=None, age_col = 'Age', source_cols=['Subject_Name', 'File_Name']): \n",
    "    if class_name in dat_set.columns.tolist():\n",
    "        dat_set = dat_set.drop(columns=[class_name])\n",
    "    dat_set.insert(3, class_name, dat_set[source_col].values) \n",
    "    dat_set\n",
    "\n",
    "    cat_val = dat_set[source_col].unique().tolist() \n",
    "    cat_val.remove('n')\n",
    "    cat_val.insert(0, 'n')\n",
    "    print(cat_val) \n",
    "    \n",
    "    if removable_cats:\n",
    "        cat_val = [c for c in cat_val if c not in removable_cats]\n",
    "        dat_set = dat_set[dat_set[source_col].isin(cat_val)]\n",
    "        dat_set.reset_index(drop=True, inplace=True)\n",
    "    print(cat_val) \n",
    "        \n",
    "    cls_map = dict(zip(cat_val, list(range(len(cat_val))))) \n",
    "    cls_map\n",
    "    \n",
    "    if not multi_class:\n",
    "        for k in cls_map.keys():\n",
    "            if cls_map[k]>1:\n",
    "                cls_map[k]=1\n",
    "\n",
    "    dat_set.replace({class_name: cls_map}, inplace=True) \n",
    "    \n",
    "    if age_data is not None: \n",
    "        tmp_df = dat_set.merge(age_data, how='inner', left_on=[source_cols[0]], right_on=[source_cols[1]])\n",
    "        tmp_df\n",
    "        assert tmp_df.shape[0]==dat_set.shape[0], \"Something hapend. No shape after dataframe joining is same...\" \n",
    "        sel_ind = dat_set.columns.values.tolist().index(class_name)+1 \n",
    "        dat_set.insert(sel_ind, age_col, tmp_df[age_col].values)\n",
    "    \n",
    "    return cls_map, dat_set \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea25123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_map, dataset2 = map_category_to_class(dataset.copy(), source_col='Category', class_name='Class', removable_cats=None, multi_class=False, age_data=all_demography_detail_df.copy(), age_col = 'Age', source_cols=['Subject_Name', 'File_Name'])\n",
    "# print(label_map)\n",
    "# dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee9ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca312c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94200a61",
   "metadata": {},
   "source": [
    "#### Get features with zeo transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceaff4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_with_zero_values(tmp_df):\n",
    "    feat_names = tmp_df.columns.values[4:] \n",
    "    feat_names\n",
    "    zero_feats = [] \n",
    "    for f in feat_names:\n",
    "        if tmp_df[f].values.sum() == 0:\n",
    "            zero_feats.append(f)\n",
    "    return zero_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79abfd39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a86d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a1fd1d5",
   "metadata": {},
   "source": [
    "# Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecf88587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_directory(path, exp_name): \n",
    "    exp_directory = f\"{path}/{exp_name}/\"\n",
    "\n",
    "    if (not os.path.exists(exp_directory)):\n",
    "        try:\n",
    "            os.makedirs(exp_directory, exist_ok = True)\n",
    "            print(f\"Directory successfully created at path: {exp_directory}\") \n",
    "        except OSError as error:\n",
    "            print(f\"Directory cannot be created at path: {exp_directory}\") \n",
    "    else:\n",
    "        print(f\"Directory already exists at path: {exp_directory}\") \n",
    "\n",
    "    return exp_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c85e5229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<ML_Classifiers.SVC: 'support_vector_classifier'>,\n",
       " <ML_Classifiers.XGBoost: 'xtreme_gradient_boosting'>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_Classifiers.SVC, ML_Classifiers.XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52e9251e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('__main__',\n",
       " 'C:\\\\Users\\\\aliem\\\\OneDrive - Deakin University\\\\_MyResearch\\\\PhD_Research\\\\HML_IHC_Sleep_Data_Analysis')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__name__, os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e53ff71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_logger(result_save_path, exp_name): \n",
    "    util = Humachlab_Utility() \n",
    "    all_log_file_name = f'{result_save_path}/all_logs_{exp_name}.txt'\n",
    "    # logger = util.get_logger(logger_name=__name__, log_file_name=all_log_file_name)\n",
    "    logger = util.get_logger(logger_name=\"Sleep ML Model Analysis\", log_file_name=all_log_file_name)\n",
    "    util, all_log_file_name, logger \n",
    "    return util, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "851bfb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_logger(logger): \n",
    "    handlers = logger.handlers[:]\n",
    "    for handler in handlers:\n",
    "        logger.removeHandler(handler)\n",
    "        handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ccf787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_experiment_information_summarry(exp_dir, dict_dat=None):\n",
    "    exp_sum_dir = f'{exp_dir}/Experiment_Information.csv'\n",
    "    df = pd.DataFrame(columns=['exp_name', 'stp_from', 'exp_description', 'datasets', 'feature_selection', 'special_consideration', 'classification_type'])\n",
    "    if (os.path.exists(exp_sum_dir)):\n",
    "        df = pd.read_csv(exp_sum_dir, index_col=False) \n",
    "    if dict_dat:\n",
    "        if df.shape[0]>0:\n",
    "            print(dict_dat['exp_name'], (df['exp_name'].values.tolist()))\n",
    "            if dict_dat['exp_name'] in (df['exp_name'].values.tolist()):\n",
    "                # nn = 1 \n",
    "                # print(f'Cannot add this data. this experiment is already exited. Please recheck and do a new experiment')\n",
    "                # assert nn < 0, f'Cannot add this data. this experiment is already exited. Please recheck and do a new experiment' \n",
    "                print(f'This experiment is already exited. So this data is removed...')\n",
    "                df = df.loc[df[\"exp_name\"] != dict_dat['exp_name']] \n",
    "                df.reset_index(drop=True, inplace=True) \n",
    "                \n",
    "        df.loc[len(df)] = dict_dat\n",
    "        df.sort_values(by=['exp_name'], ascending=[True], inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True) \n",
    "        df.to_csv(exp_sum_dir, index=False) \n",
    "        print(f'Data is successfully inserted...')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021d2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d2e812b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading criteria: 0, 0, 1, 0, 2\n",
      "Reading data from the file: ./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Annot_Proba_Transition2.csv\n",
      "Statistics from the file: ./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Annot_Proba_Transition2_STAT_bin.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subject_Name</th>\n",
       "      <th>W-&gt;W</th>\n",
       "      <th>W-&gt;S1</th>\n",
       "      <th>W-&gt;S2</th>\n",
       "      <th>W-&gt;S3</th>\n",
       "      <th>W-&gt;S4</th>\n",
       "      <th>W-&gt;REM</th>\n",
       "      <th>S1-&gt;W</th>\n",
       "      <th>...</th>\n",
       "      <th>S4-&gt;S2</th>\n",
       "      <th>S4-&gt;S3</th>\n",
       "      <th>S4-&gt;S4</th>\n",
       "      <th>S4-&gt;REM</th>\n",
       "      <th>REM-&gt;W</th>\n",
       "      <th>REM-&gt;S1</th>\n",
       "      <th>REM-&gt;S2</th>\n",
       "      <th>REM-&gt;S3</th>\n",
       "      <th>REM-&gt;S4</th>\n",
       "      <th>REM-&gt;REM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux1</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.970443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.966480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux2</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.984940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb1</td>\n",
       "      <td>0.876147</td>\n",
       "      <td>0.123853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.949239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.962617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb2</td>\n",
       "      <td>0.948387</td>\n",
       "      <td>0.045161</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.974359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb3</td>\n",
       "      <td>0.934272</td>\n",
       "      <td>0.046948</td>\n",
       "      <td>0.018779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.935065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7191</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7201</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.932836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7211</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7221</td>\n",
       "      <td>0.818792</td>\n",
       "      <td>0.181208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021645</td>\n",
       "      <td>0.043290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.935065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7241</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dataset Category Subject_Name      W->W     W->S1     W->S2  W->S3  \\\n",
       "0     CAP_Sleep     brux        brux1  0.798077  0.192308  0.009615    0.0   \n",
       "1     CAP_Sleep     brux        brux2  0.824000  0.176000  0.000000    0.0   \n",
       "2     CAP_Sleep      sdb         sdb1  0.876147  0.123853  0.000000    0.0   \n",
       "3     CAP_Sleep      sdb         sdb2  0.948387  0.045161  0.006452    0.0   \n",
       "4     CAP_Sleep      sdb         sdb3  0.934272  0.046948  0.018779    0.0   \n",
       "..          ...      ...          ...       ...       ...       ...    ...   \n",
       "225  Sleep_EDFX        n       ST7191  0.739130  0.246377  0.014493    0.0   \n",
       "226  Sleep_EDFX        n       ST7201  0.294118  0.588235  0.058824    0.0   \n",
       "227  Sleep_EDFX        n       ST7211  0.894118  0.100000  0.000000    0.0   \n",
       "228  Sleep_EDFX        n       ST7221  0.818792  0.181208  0.000000    0.0   \n",
       "229  Sleep_EDFX        n       ST7241  0.607143  0.285714  0.035714    0.0   \n",
       "\n",
       "     W->S4    W->REM     S1->W  ...    S4->S2    S4->S3    S4->S4  S4->REM  \\\n",
       "0      0.0  0.000000  0.030769  ...  0.004926  0.024631  0.970443      0.0   \n",
       "1      0.0  0.000000  0.061728  ...  0.000000  0.012048  0.984940      0.0   \n",
       "2      0.0  0.000000  0.099237  ...  0.005076  0.030457  0.949239      0.0   \n",
       "3      0.0  0.000000  0.011364  ...  0.008475  0.016949  0.966102      0.0   \n",
       "4      0.0  0.000000  0.108696  ...  0.025974  0.038961  0.935065      0.0   \n",
       "..     ...       ...       ...  ...       ...       ...       ...      ...   \n",
       "225    0.0  0.000000  0.044118  ...  0.013889  0.166667  0.805556      0.0   \n",
       "226    0.0  0.058824  0.014493  ...  0.038462  0.615385  0.346154      0.0   \n",
       "227    0.0  0.005882  0.116667  ...  0.016949  0.254237  0.728814      0.0   \n",
       "228    0.0  0.000000  0.052133  ...  0.000000  0.000000  0.000000      0.0   \n",
       "229    0.0  0.071429  0.047619  ...  0.142857  0.571429  0.142857      0.0   \n",
       "\n",
       "       REM->W   REM->S1   REM->S2  REM->S3  REM->S4  REM->REM  \n",
       "0    0.033520  0.000000  0.000000      0.0      0.0  0.966480  \n",
       "1    0.019324  0.004831  0.000000      0.0      0.0  0.975845  \n",
       "2    0.009346  0.000000  0.028037      0.0      0.0  0.962617  \n",
       "3    0.000000  0.000000  0.025641      0.0      0.0  0.974359  \n",
       "4    0.062500  0.000000  0.000000      0.0      0.0  0.937500  \n",
       "..        ...       ...       ...      ...      ...       ...  \n",
       "225  0.000000  0.020000  0.015000      0.0      0.0  0.965000  \n",
       "226  0.044776  0.007463  0.014925      0.0      0.0  0.932836  \n",
       "227  0.025641  0.000000  0.019231      0.0      0.0  0.955128  \n",
       "228  0.021645  0.043290  0.000000      0.0      0.0  0.935065  \n",
       "229  0.016878  0.016878  0.008439      0.0      0.0  0.957806  \n",
       "\n",
       "[230 rows x 39 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name='Class' \n",
    "mat_type = 0 ## annot_type  # mat_type=annot/tran -- 0/1 \n",
    "STP_From_All = 0 ## int(tran_info_cal_from_all)  # STP_From_All=same/all -- 0/1 \n",
    "TrimW = 1 ## int(wake_state_trimmed)  # TrimW=normal/trimmed -- 0/1 \n",
    "mat_info_type = 0 ## int(tran_info_type) # Proba/Dura/Count -- 0/1/2\n",
    "tran_step = 2 ## int (tran step) # one level=2 step, two level=3 step -- 2/3\n",
    "\n",
    "# ##dataset, stat_dataset = get_dataset_for_file_conditions(mat_type, STP_From_All, TrimW, mat_info_type, tran_step=tran_step) # mat_type=annot/tran, STP_From_All=same/all, TrimW=normal/trimmed, mat_info_type=proba/dura/count, tran_step=2/3 \n",
    "# ##dataset \n",
    "\n",
    "# dataset1, stat_dataset1 = get_dataset_for_file_conditions(mat_type, STP_From_All, TrimW, mat_info_type, tran_step=tran_step) # mat_type=annot/tran, STP_From_All=same/all, TrimW=normal/trimmed, mat_info_type=proba/dura/count \n",
    "# dataset1 \n",
    "\n",
    "dataset2, stat_dataset2 = get_dataset_for_file_conditions(mat_type, STP_From_All, TrimW, mat_info_type, tran_step=tran_step) # mat_type=annot/tran, STP_From_All=same/all, TrimW=normal/trimmed, mat_info_type=proba/dura/count \n",
    "dataset2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4afbf204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((230, 39), (230, 219))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset1.shape, dataset2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "687195d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i for i in dataset2.Subject_Name.values.tolist() if i not in dataset1.Subject_Name.values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bb6a1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner_merged_total = pd.merge(\n",
    "#     dataset1, dataset2, on=['Dataset', 'Category', 'Subject_Name'], suffixes=['_proba', '_dura']\n",
    "# )\n",
    "# inner_merged_total\n",
    "\n",
    "# inner_merged_total = pd.merge(\n",
    "#     dataset1, dataset2, on=['Dataset', 'Category', 'Subject_Name'], suffixes=['_proba', '_dura']\n",
    "# )\n",
    "# print(inner_merged_total.columns.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7780ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([stat_dataset1, stat_dataset2], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "486e4994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Features' 'Sum' 'Mean' 'STD' 'Healthy_Sum' 'Healthy_Mean' 'Healthy_STD'\n",
      " 'Disorder_Sum' 'Disorder_Mean' 'Disorder_STD' 'P_Value' 'AUC'\n",
      " 'Wilcoxon_zscore' 'Wilcoxon_pvalue' 'MannWhitney_statistic'\n",
      " 'MannWhitney_pvalue']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subject_Name</th>\n",
       "      <th>W-&gt;W</th>\n",
       "      <th>W-&gt;S1</th>\n",
       "      <th>W-&gt;S2</th>\n",
       "      <th>W-&gt;S3</th>\n",
       "      <th>W-&gt;S4</th>\n",
       "      <th>W-&gt;REM</th>\n",
       "      <th>S1-&gt;W</th>\n",
       "      <th>...</th>\n",
       "      <th>S4-&gt;S2</th>\n",
       "      <th>S4-&gt;S3</th>\n",
       "      <th>S4-&gt;S4</th>\n",
       "      <th>S4-&gt;REM</th>\n",
       "      <th>REM-&gt;W</th>\n",
       "      <th>REM-&gt;S1</th>\n",
       "      <th>REM-&gt;S2</th>\n",
       "      <th>REM-&gt;S3</th>\n",
       "      <th>REM-&gt;S4</th>\n",
       "      <th>REM-&gt;REM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux1</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.970443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.966480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux2</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.984940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb1</td>\n",
       "      <td>0.876147</td>\n",
       "      <td>0.123853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.949239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.962617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb2</td>\n",
       "      <td>0.948387</td>\n",
       "      <td>0.045161</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.974359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb3</td>\n",
       "      <td>0.934272</td>\n",
       "      <td>0.046948</td>\n",
       "      <td>0.018779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.935065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7191</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7201</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.932836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7211</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7221</td>\n",
       "      <td>0.818792</td>\n",
       "      <td>0.181208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021645</td>\n",
       "      <td>0.043290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.935065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7241</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dataset Category Subject_Name      W->W     W->S1     W->S2  W->S3  \\\n",
       "0     CAP_Sleep     brux        brux1  0.798077  0.192308  0.009615    0.0   \n",
       "1     CAP_Sleep     brux        brux2  0.824000  0.176000  0.000000    0.0   \n",
       "2     CAP_Sleep      sdb         sdb1  0.876147  0.123853  0.000000    0.0   \n",
       "3     CAP_Sleep      sdb         sdb2  0.948387  0.045161  0.006452    0.0   \n",
       "4     CAP_Sleep      sdb         sdb3  0.934272  0.046948  0.018779    0.0   \n",
       "..          ...      ...          ...       ...       ...       ...    ...   \n",
       "225  Sleep_EDFX        n       ST7191  0.739130  0.246377  0.014493    0.0   \n",
       "226  Sleep_EDFX        n       ST7201  0.294118  0.588235  0.058824    0.0   \n",
       "227  Sleep_EDFX        n       ST7211  0.894118  0.100000  0.000000    0.0   \n",
       "228  Sleep_EDFX        n       ST7221  0.818792  0.181208  0.000000    0.0   \n",
       "229  Sleep_EDFX        n       ST7241  0.607143  0.285714  0.035714    0.0   \n",
       "\n",
       "     W->S4    W->REM     S1->W  ...    S4->S2    S4->S3    S4->S4  S4->REM  \\\n",
       "0      0.0  0.000000  0.030769  ...  0.004926  0.024631  0.970443      0.0   \n",
       "1      0.0  0.000000  0.061728  ...  0.000000  0.012048  0.984940      0.0   \n",
       "2      0.0  0.000000  0.099237  ...  0.005076  0.030457  0.949239      0.0   \n",
       "3      0.0  0.000000  0.011364  ...  0.008475  0.016949  0.966102      0.0   \n",
       "4      0.0  0.000000  0.108696  ...  0.025974  0.038961  0.935065      0.0   \n",
       "..     ...       ...       ...  ...       ...       ...       ...      ...   \n",
       "225    0.0  0.000000  0.044118  ...  0.013889  0.166667  0.805556      0.0   \n",
       "226    0.0  0.058824  0.014493  ...  0.038462  0.615385  0.346154      0.0   \n",
       "227    0.0  0.005882  0.116667  ...  0.016949  0.254237  0.728814      0.0   \n",
       "228    0.0  0.000000  0.052133  ...  0.000000  0.000000  0.000000      0.0   \n",
       "229    0.0  0.071429  0.047619  ...  0.142857  0.571429  0.142857      0.0   \n",
       "\n",
       "       REM->W   REM->S1   REM->S2  REM->S3  REM->S4  REM->REM  \n",
       "0    0.033520  0.000000  0.000000      0.0      0.0  0.966480  \n",
       "1    0.019324  0.004831  0.000000      0.0      0.0  0.975845  \n",
       "2    0.009346  0.000000  0.028037      0.0      0.0  0.962617  \n",
       "3    0.000000  0.000000  0.025641      0.0      0.0  0.974359  \n",
       "4    0.062500  0.000000  0.000000      0.0      0.0  0.937500  \n",
       "..        ...       ...       ...      ...      ...       ...  \n",
       "225  0.000000  0.020000  0.015000      0.0      0.0  0.965000  \n",
       "226  0.044776  0.007463  0.014925      0.0      0.0  0.932836  \n",
       "227  0.025641  0.000000  0.019231      0.0      0.0  0.955128  \n",
       "228  0.021645  0.043290  0.000000      0.0      0.0  0.935065  \n",
       "229  0.016878  0.016878  0.008439      0.0      0.0  0.957806  \n",
       "\n",
       "[230 rows x 39 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = pd.concat([dataset1.add_prefix('proba_'), dataset2.add_prefix('dura_')], axis=1)\n",
    "# dataset = pd.merge(dataset1, dataset2, on=['Dataset', 'Category', 'Subject_Name'], suffixes=['_proba', '_dura'])\n",
    "# dataset = pd.merge(dataset1, dataset2, on=['Dataset', 'Category', 'Subject_Name'], suffixes=['_annot', '_tran'])\n",
    "\n",
    "# dataset = dataset1.copy()\n",
    "# stat_dataset = stat_dataset1.copy() \n",
    "dataset = dataset2.copy()\n",
    "stat_dataset = stat_dataset2.copy() \n",
    "\n",
    "# dataset = pd.merge(\n",
    "#     dataset1, dataset2, on=['Dataset', 'Category', 'Subject_Name'], suffixes=['_proba', '_dura']\n",
    "# )\n",
    "# stat_dataset = pd.concat([stat_dataset1, stat_dataset2], axis=0) \n",
    "\n",
    "print(stat_dataset.columns.values) \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91530443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43b24359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a1d112f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CAP_Sleep', 'SDRC', 'Sleep_EDFX'], dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Dataset'].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8ea92fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Subject_Name'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6accca6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_features_with_zero_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-769603359701>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mzero_feats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_features_with_zero_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzero_feats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzero_feats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_features_with_zero_values' is not defined"
     ]
    }
   ],
   "source": [
    "zero_feats = get_features_with_zero_values(dataset)\n",
    "print(len(zero_feats), zero_feats) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c25f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f584a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Sum</th>\n",
       "      <th>Mean</th>\n",
       "      <th>STD</th>\n",
       "      <th>Healthy_Sum</th>\n",
       "      <th>Healthy_Mean</th>\n",
       "      <th>Healthy_STD</th>\n",
       "      <th>Disorder_Sum</th>\n",
       "      <th>Disorder_Mean</th>\n",
       "      <th>Disorder_STD</th>\n",
       "      <th>P_Value</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Wilcoxon_zscore</th>\n",
       "      <th>Wilcoxon_pvalue</th>\n",
       "      <th>MannWhitney_statistic</th>\n",
       "      <th>MannWhitney_pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W-&gt;W</td>\n",
       "      <td>200.023853</td>\n",
       "      <td>0.869669</td>\n",
       "      <td>0.111130</td>\n",
       "      <td>108.678140</td>\n",
       "      <td>0.855733</td>\n",
       "      <td>0.120193</td>\n",
       "      <td>91.345712</td>\n",
       "      <td>0.886852</td>\n",
       "      <td>0.096074</td>\n",
       "      <td>3.481301e-02</td>\n",
       "      <td>0.403218</td>\n",
       "      <td>-2.522887</td>\n",
       "      <td>1.163957e-02</td>\n",
       "      <td>5274.5</td>\n",
       "      <td>5.836077e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W-&gt;S1</td>\n",
       "      <td>24.969913</td>\n",
       "      <td>0.108565</td>\n",
       "      <td>0.094053</td>\n",
       "      <td>15.941897</td>\n",
       "      <td>0.125527</td>\n",
       "      <td>0.099497</td>\n",
       "      <td>9.028017</td>\n",
       "      <td>0.087651</td>\n",
       "      <td>0.082186</td>\n",
       "      <td>2.277298e-03</td>\n",
       "      <td>0.347374</td>\n",
       "      <td>3.978629</td>\n",
       "      <td>6.931372e-05</td>\n",
       "      <td>4544.0</td>\n",
       "      <td>3.479935e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W-&gt;S2</td>\n",
       "      <td>4.074743</td>\n",
       "      <td>0.017716</td>\n",
       "      <td>0.030857</td>\n",
       "      <td>1.629452</td>\n",
       "      <td>0.012830</td>\n",
       "      <td>0.027523</td>\n",
       "      <td>2.445291</td>\n",
       "      <td>0.023741</td>\n",
       "      <td>0.033563</td>\n",
       "      <td>7.522503e-03</td>\n",
       "      <td>0.653811</td>\n",
       "      <td>-4.009518</td>\n",
       "      <td>6.084289e-05</td>\n",
       "      <td>4528.5</td>\n",
       "      <td>1.431124e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W-&gt;S3</td>\n",
       "      <td>0.219641</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.004911</td>\n",
       "      <td>0.108293</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.004927</td>\n",
       "      <td>0.111348</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.004890</td>\n",
       "      <td>7.272645e-01</td>\n",
       "      <td>0.511199</td>\n",
       "      <td>-0.291945</td>\n",
       "      <td>7.703283e-01</td>\n",
       "      <td>6394.0</td>\n",
       "      <td>2.483123e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W-&gt;S4</td>\n",
       "      <td>0.125777</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.007757</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.010398</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>4.121993e-01</td>\n",
       "      <td>0.500879</td>\n",
       "      <td>-0.022917</td>\n",
       "      <td>9.817163e-01</td>\n",
       "      <td>6529.0</td>\n",
       "      <td>4.457874e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>W-&gt;REM</td>\n",
       "      <td>0.586073</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.009081</td>\n",
       "      <td>0.524571</td>\n",
       "      <td>0.004130</td>\n",
       "      <td>0.011837</td>\n",
       "      <td>0.061502</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>3.217895e-03</td>\n",
       "      <td>0.422139</td>\n",
       "      <td>2.029669</td>\n",
       "      <td>4.239021e-02</td>\n",
       "      <td>5522.0</td>\n",
       "      <td>1.180169e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S1-&gt;W</td>\n",
       "      <td>15.246232</td>\n",
       "      <td>0.066288</td>\n",
       "      <td>0.067213</td>\n",
       "      <td>9.969784</td>\n",
       "      <td>0.078502</td>\n",
       "      <td>0.053088</td>\n",
       "      <td>5.276448</td>\n",
       "      <td>0.051228</td>\n",
       "      <td>0.078752</td>\n",
       "      <td>2.102759e-03</td>\n",
       "      <td>0.700673</td>\n",
       "      <td>5.231105</td>\n",
       "      <td>1.684994e-07</td>\n",
       "      <td>3915.5</td>\n",
       "      <td>8.178827e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S1-&gt;S1</td>\n",
       "      <td>164.464191</td>\n",
       "      <td>0.715062</td>\n",
       "      <td>0.143797</td>\n",
       "      <td>85.942910</td>\n",
       "      <td>0.676716</td>\n",
       "      <td>0.142570</td>\n",
       "      <td>78.521282</td>\n",
       "      <td>0.762343</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>4.897664e-06</td>\n",
       "      <td>0.686301</td>\n",
       "      <td>-4.854466</td>\n",
       "      <td>1.207117e-06</td>\n",
       "      <td>4104.5</td>\n",
       "      <td>6.063551e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S1-&gt;S2</td>\n",
       "      <td>44.629370</td>\n",
       "      <td>0.194041</td>\n",
       "      <td>0.112006</td>\n",
       "      <td>27.225130</td>\n",
       "      <td>0.214371</td>\n",
       "      <td>0.116463</td>\n",
       "      <td>17.404240</td>\n",
       "      <td>0.168973</td>\n",
       "      <td>0.100755</td>\n",
       "      <td>2.128943e-03</td>\n",
       "      <td>0.381011</td>\n",
       "      <td>3.101796</td>\n",
       "      <td>1.923502e-03</td>\n",
       "      <td>4984.0</td>\n",
       "      <td>9.646592e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S1-&gt;S3</td>\n",
       "      <td>1.493990</td>\n",
       "      <td>0.006496</td>\n",
       "      <td>0.026515</td>\n",
       "      <td>0.857565</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>0.024687</td>\n",
       "      <td>0.636425</td>\n",
       "      <td>0.006179</td>\n",
       "      <td>0.028607</td>\n",
       "      <td>8.711021e-01</td>\n",
       "      <td>0.505695</td>\n",
       "      <td>0.148464</td>\n",
       "      <td>8.819768e-01</td>\n",
       "      <td>6466.0</td>\n",
       "      <td>3.975669e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S1-&gt;S4</td>\n",
       "      <td>0.065311</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.028199</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.037113</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>6.595031e-01</td>\n",
       "      <td>0.488457</td>\n",
       "      <td>-0.300913</td>\n",
       "      <td>7.634808e-01</td>\n",
       "      <td>6389.5</td>\n",
       "      <td>1.386924e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>S1-&gt;REM</td>\n",
       "      <td>4.100905</td>\n",
       "      <td>0.017830</td>\n",
       "      <td>0.026896</td>\n",
       "      <td>2.976412</td>\n",
       "      <td>0.023436</td>\n",
       "      <td>0.030251</td>\n",
       "      <td>1.124493</td>\n",
       "      <td>0.010917</td>\n",
       "      <td>0.020013</td>\n",
       "      <td>4.013109e-04</td>\n",
       "      <td>0.329600</td>\n",
       "      <td>4.445941</td>\n",
       "      <td>8.750782e-06</td>\n",
       "      <td>4309.5</td>\n",
       "      <td>1.435113e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>S2-&gt;W</td>\n",
       "      <td>4.539591</td>\n",
       "      <td>0.019737</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>1.833698</td>\n",
       "      <td>0.014439</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>2.705894</td>\n",
       "      <td>0.026271</td>\n",
       "      <td>0.080325</td>\n",
       "      <td>1.036297e-01</td>\n",
       "      <td>0.616467</td>\n",
       "      <td>3.036034</td>\n",
       "      <td>2.397124e-03</td>\n",
       "      <td>5017.0</td>\n",
       "      <td>1.201852e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>S2-&gt;S1</td>\n",
       "      <td>9.884910</td>\n",
       "      <td>0.042978</td>\n",
       "      <td>0.103662</td>\n",
       "      <td>6.406378</td>\n",
       "      <td>0.050444</td>\n",
       "      <td>0.112111</td>\n",
       "      <td>3.478532</td>\n",
       "      <td>0.033772</td>\n",
       "      <td>0.091348</td>\n",
       "      <td>2.269664e-01</td>\n",
       "      <td>0.732436</td>\n",
       "      <td>6.059115</td>\n",
       "      <td>1.368728e-09</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>6.512661e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>S2-&gt;S2</td>\n",
       "      <td>204.594912</td>\n",
       "      <td>0.889543</td>\n",
       "      <td>0.157759</td>\n",
       "      <td>111.333292</td>\n",
       "      <td>0.876640</td>\n",
       "      <td>0.138031</td>\n",
       "      <td>93.261620</td>\n",
       "      <td>0.905453</td>\n",
       "      <td>0.177832</td>\n",
       "      <td>1.698499e-01</td>\n",
       "      <td>0.240081</td>\n",
       "      <td>-6.775527</td>\n",
       "      <td>1.239536e-11</td>\n",
       "      <td>3140.5</td>\n",
       "      <td>6.240471e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>S2-&gt;S3</td>\n",
       "      <td>8.390459</td>\n",
       "      <td>0.036480</td>\n",
       "      <td>0.042502</td>\n",
       "      <td>5.940230</td>\n",
       "      <td>0.046773</td>\n",
       "      <td>0.044281</td>\n",
       "      <td>2.450230</td>\n",
       "      <td>0.023789</td>\n",
       "      <td>0.036392</td>\n",
       "      <td>3.588028e-05</td>\n",
       "      <td>0.689626</td>\n",
       "      <td>4.943145</td>\n",
       "      <td>7.687208e-07</td>\n",
       "      <td>4060.0</td>\n",
       "      <td>3.857922e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>S2-&gt;S4</td>\n",
       "      <td>0.092039</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.076477</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.015561</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>6.547331e-02</td>\n",
       "      <td>0.464185</td>\n",
       "      <td>0.933628</td>\n",
       "      <td>3.504959e-01</td>\n",
       "      <td>6072.0</td>\n",
       "      <td>2.817489e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>S2-&gt;REM</td>\n",
       "      <td>2.498089</td>\n",
       "      <td>0.010861</td>\n",
       "      <td>0.011107</td>\n",
       "      <td>1.409925</td>\n",
       "      <td>0.011102</td>\n",
       "      <td>0.007044</td>\n",
       "      <td>1.088163</td>\n",
       "      <td>0.010565</td>\n",
       "      <td>0.014633</td>\n",
       "      <td>7.167950e-01</td>\n",
       "      <td>0.639248</td>\n",
       "      <td>3.625903</td>\n",
       "      <td>2.879531e-04</td>\n",
       "      <td>4721.0</td>\n",
       "      <td>1.445188e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>S3-&gt;W</td>\n",
       "      <td>2.630026</td>\n",
       "      <td>0.011435</td>\n",
       "      <td>0.043096</td>\n",
       "      <td>1.622712</td>\n",
       "      <td>0.012777</td>\n",
       "      <td>0.031093</td>\n",
       "      <td>1.007314</td>\n",
       "      <td>0.009780</td>\n",
       "      <td>0.054316</td>\n",
       "      <td>6.017798e-01</td>\n",
       "      <td>0.442665</td>\n",
       "      <td>1.494602</td>\n",
       "      <td>1.350185e-01</td>\n",
       "      <td>5790.5</td>\n",
       "      <td>4.815483e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>S3-&gt;S1</td>\n",
       "      <td>4.076654</td>\n",
       "      <td>0.017725</td>\n",
       "      <td>0.078417</td>\n",
       "      <td>2.016563</td>\n",
       "      <td>0.015878</td>\n",
       "      <td>0.046974</td>\n",
       "      <td>2.060091</td>\n",
       "      <td>0.020001</td>\n",
       "      <td>0.104887</td>\n",
       "      <td>6.933089e-01</td>\n",
       "      <td>0.559399</td>\n",
       "      <td>1.548407</td>\n",
       "      <td>1.215243e-01</td>\n",
       "      <td>5763.5</td>\n",
       "      <td>2.506618e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>S3-&gt;S2</td>\n",
       "      <td>42.811394</td>\n",
       "      <td>0.186136</td>\n",
       "      <td>0.250092</td>\n",
       "      <td>38.829390</td>\n",
       "      <td>0.305743</td>\n",
       "      <td>0.282573</td>\n",
       "      <td>3.982004</td>\n",
       "      <td>0.038660</td>\n",
       "      <td>0.042719</td>\n",
       "      <td>3.887191e-18</td>\n",
       "      <td>0.831206</td>\n",
       "      <td>8.633815</td>\n",
       "      <td>5.933901e-18</td>\n",
       "      <td>2208.0</td>\n",
       "      <td>2.969660e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>S3-&gt;S3</td>\n",
       "      <td>157.753562</td>\n",
       "      <td>0.685885</td>\n",
       "      <td>0.304326</td>\n",
       "      <td>66.038094</td>\n",
       "      <td>0.519985</td>\n",
       "      <td>0.288398</td>\n",
       "      <td>91.715468</td>\n",
       "      <td>0.890441</td>\n",
       "      <td>0.168750</td>\n",
       "      <td>2.200777e-24</td>\n",
       "      <td>0.886821</td>\n",
       "      <td>-10.083578</td>\n",
       "      <td>6.530362e-24</td>\n",
       "      <td>1480.5</td>\n",
       "      <td>3.203071e-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>S3-&gt;S4</td>\n",
       "      <td>12.123690</td>\n",
       "      <td>0.052712</td>\n",
       "      <td>0.079878</td>\n",
       "      <td>9.205868</td>\n",
       "      <td>0.072487</td>\n",
       "      <td>0.101889</td>\n",
       "      <td>2.917822</td>\n",
       "      <td>0.028328</td>\n",
       "      <td>0.019256</td>\n",
       "      <td>2.356107e-05</td>\n",
       "      <td>0.484023</td>\n",
       "      <td>0.416496</td>\n",
       "      <td>6.770474e-01</td>\n",
       "      <td>6331.5</td>\n",
       "      <td>3.366042e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>S3-&gt;REM</td>\n",
       "      <td>0.604675</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.013940</td>\n",
       "      <td>0.287374</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>0.013182</td>\n",
       "      <td>0.317301</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>0.014809</td>\n",
       "      <td>6.598637e-01</td>\n",
       "      <td>0.501567</td>\n",
       "      <td>0.040852</td>\n",
       "      <td>9.674135e-01</td>\n",
       "      <td>6520.0</td>\n",
       "      <td>4.705984e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>S4-&gt;W</td>\n",
       "      <td>0.893508</td>\n",
       "      <td>0.003885</td>\n",
       "      <td>0.009809</td>\n",
       "      <td>0.453102</td>\n",
       "      <td>0.003568</td>\n",
       "      <td>0.011202</td>\n",
       "      <td>0.440406</td>\n",
       "      <td>0.004276</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>5.881061e-01</td>\n",
       "      <td>0.391981</td>\n",
       "      <td>-2.815829</td>\n",
       "      <td>4.865151e-03</td>\n",
       "      <td>5127.5</td>\n",
       "      <td>2.576184e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>S4-&gt;S1</td>\n",
       "      <td>3.261657</td>\n",
       "      <td>0.014181</td>\n",
       "      <td>0.099853</td>\n",
       "      <td>0.453834</td>\n",
       "      <td>0.003573</td>\n",
       "      <td>0.019148</td>\n",
       "      <td>2.807823</td>\n",
       "      <td>0.027260</td>\n",
       "      <td>0.146637</td>\n",
       "      <td>7.418514e-02</td>\n",
       "      <td>0.519838</td>\n",
       "      <td>-0.517132</td>\n",
       "      <td>6.050639e-01</td>\n",
       "      <td>6281.0</td>\n",
       "      <td>2.044395e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>S4-&gt;S2</td>\n",
       "      <td>3.678762</td>\n",
       "      <td>0.015995</td>\n",
       "      <td>0.079995</td>\n",
       "      <td>2.514545</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.097977</td>\n",
       "      <td>1.164217</td>\n",
       "      <td>0.011303</td>\n",
       "      <td>0.049125</td>\n",
       "      <td>4.253375e-01</td>\n",
       "      <td>0.602248</td>\n",
       "      <td>-2.665373</td>\n",
       "      <td>7.690303e-03</td>\n",
       "      <td>5203.0</td>\n",
       "      <td>1.544964e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>S4-&gt;S3</td>\n",
       "      <td>22.586227</td>\n",
       "      <td>0.098201</td>\n",
       "      <td>0.215609</td>\n",
       "      <td>21.093086</td>\n",
       "      <td>0.166087</td>\n",
       "      <td>0.271468</td>\n",
       "      <td>1.493141</td>\n",
       "      <td>0.014497</td>\n",
       "      <td>0.015860</td>\n",
       "      <td>5.144747e-08</td>\n",
       "      <td>0.528362</td>\n",
       "      <td>0.739330</td>\n",
       "      <td>4.597069e-01</td>\n",
       "      <td>6169.5</td>\n",
       "      <td>2.253108e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>S4-&gt;S4</td>\n",
       "      <td>134.511382</td>\n",
       "      <td>0.584832</td>\n",
       "      <td>0.438940</td>\n",
       "      <td>48.446972</td>\n",
       "      <td>0.381472</td>\n",
       "      <td>0.408706</td>\n",
       "      <td>86.064410</td>\n",
       "      <td>0.835577</td>\n",
       "      <td>0.332269</td>\n",
       "      <td>6.105710e-17</td>\n",
       "      <td>0.813890</td>\n",
       "      <td>-8.182445</td>\n",
       "      <td>2.781413e-16</td>\n",
       "      <td>2434.5</td>\n",
       "      <td>4.354174e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>S4-&gt;REM</td>\n",
       "      <td>1.068465</td>\n",
       "      <td>0.004645</td>\n",
       "      <td>0.065837</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>1.030003</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.098045</td>\n",
       "      <td>2.686266e-01</td>\n",
       "      <td>0.484596</td>\n",
       "      <td>-0.401550</td>\n",
       "      <td>6.880155e-01</td>\n",
       "      <td>6339.0</td>\n",
       "      <td>5.640681e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>REM-&gt;W</td>\n",
       "      <td>4.255135</td>\n",
       "      <td>0.018501</td>\n",
       "      <td>0.033948</td>\n",
       "      <td>2.317329</td>\n",
       "      <td>0.018247</td>\n",
       "      <td>0.021502</td>\n",
       "      <td>1.937806</td>\n",
       "      <td>0.018814</td>\n",
       "      <td>0.044757</td>\n",
       "      <td>9.003139e-01</td>\n",
       "      <td>0.585506</td>\n",
       "      <td>1.447771</td>\n",
       "      <td>1.476812e-01</td>\n",
       "      <td>5814.0</td>\n",
       "      <td>7.188721e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>REM-&gt;S1</td>\n",
       "      <td>5.264456</td>\n",
       "      <td>0.022889</td>\n",
       "      <td>0.039833</td>\n",
       "      <td>3.933551</td>\n",
       "      <td>0.030973</td>\n",
       "      <td>0.048043</td>\n",
       "      <td>1.330904</td>\n",
       "      <td>0.012921</td>\n",
       "      <td>0.022741</td>\n",
       "      <td>5.743439e-04</td>\n",
       "      <td>0.335869</td>\n",
       "      <td>4.278546</td>\n",
       "      <td>1.881181e-05</td>\n",
       "      <td>4393.5</td>\n",
       "      <td>5.780181e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>REM-&gt;S2</td>\n",
       "      <td>3.695046</td>\n",
       "      <td>0.016065</td>\n",
       "      <td>0.017879</td>\n",
       "      <td>1.976139</td>\n",
       "      <td>0.015560</td>\n",
       "      <td>0.020125</td>\n",
       "      <td>1.718908</td>\n",
       "      <td>0.016688</td>\n",
       "      <td>0.014620</td>\n",
       "      <td>6.359048e-01</td>\n",
       "      <td>0.445532</td>\n",
       "      <td>-1.419871</td>\n",
       "      <td>1.556451e-01</td>\n",
       "      <td>5828.0</td>\n",
       "      <td>7.683629e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>REM-&gt;S3</td>\n",
       "      <td>0.276820</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.006982</td>\n",
       "      <td>0.169087</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.007632</td>\n",
       "      <td>0.107733</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>7.591080e-01</td>\n",
       "      <td>0.495184</td>\n",
       "      <td>-0.125547</td>\n",
       "      <td>9.000909e-01</td>\n",
       "      <td>6477.5</td>\n",
       "      <td>3.732689e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>REM-&gt;S4</td>\n",
       "      <td>0.015619</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>6.503802e-01</td>\n",
       "      <td>0.500879</td>\n",
       "      <td>-0.022917</td>\n",
       "      <td>9.817163e-01</td>\n",
       "      <td>6529.0</td>\n",
       "      <td>4.457874e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>REM-&gt;REM</td>\n",
       "      <td>215.492923</td>\n",
       "      <td>0.936926</td>\n",
       "      <td>0.087101</td>\n",
       "      <td>118.592530</td>\n",
       "      <td>0.933799</td>\n",
       "      <td>0.062999</td>\n",
       "      <td>96.900394</td>\n",
       "      <td>0.940781</td>\n",
       "      <td>0.109638</td>\n",
       "      <td>5.475819e-01</td>\n",
       "      <td>0.361517</td>\n",
       "      <td>-3.609961</td>\n",
       "      <td>3.062432e-04</td>\n",
       "      <td>4729.0</td>\n",
       "      <td>1.536876e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Features         Sum      Mean       STD  Healthy_Sum  Healthy_Mean  \\\n",
       "0       W->W  200.023853  0.869669  0.111130   108.678140      0.855733   \n",
       "1      W->S1   24.969913  0.108565  0.094053    15.941897      0.125527   \n",
       "2      W->S2    4.074743  0.017716  0.030857     1.629452      0.012830   \n",
       "3      W->S3    0.219641  0.000955  0.004911     0.108293      0.000853   \n",
       "4      W->S4    0.125777  0.000547  0.007757     0.117647      0.000926   \n",
       "5     W->REM    0.586073  0.002548  0.009081     0.524571      0.004130   \n",
       "6      S1->W   15.246232  0.066288  0.067213     9.969784      0.078502   \n",
       "7     S1->S1  164.464191  0.715062  0.143797    85.942910      0.676716   \n",
       "8     S1->S2   44.629370  0.194041  0.112006    27.225130      0.214371   \n",
       "9     S1->S3    1.493990  0.006496  0.026515     0.857565      0.006752   \n",
       "10    S1->S4    0.065311  0.000284  0.002354     0.028199      0.000222   \n",
       "11   S1->REM    4.100905  0.017830  0.026896     2.976412      0.023436   \n",
       "12     S2->W    4.539591  0.019737  0.054688     1.833698      0.014439   \n",
       "13    S2->S1    9.884910  0.042978  0.103662     6.406378      0.050444   \n",
       "14    S2->S2  204.594912  0.889543  0.157759   111.333292      0.876640   \n",
       "15    S2->S3    8.390459  0.036480  0.042502     5.940230      0.046773   \n",
       "16    S2->S4    0.092039  0.000400  0.001844     0.076477      0.000602   \n",
       "17   S2->REM    2.498089  0.010861  0.011107     1.409925      0.011102   \n",
       "18     S3->W    2.630026  0.011435  0.043096     1.622712      0.012777   \n",
       "19    S3->S1    4.076654  0.017725  0.078417     2.016563      0.015878   \n",
       "20    S3->S2   42.811394  0.186136  0.250092    38.829390      0.305743   \n",
       "21    S3->S3  157.753562  0.685885  0.304326    66.038094      0.519985   \n",
       "22    S3->S4   12.123690  0.052712  0.079878     9.205868      0.072487   \n",
       "23   S3->REM    0.604675  0.002629  0.013940     0.287374      0.002263   \n",
       "24     S4->W    0.893508  0.003885  0.009809     0.453102      0.003568   \n",
       "25    S4->S1    3.261657  0.014181  0.099853     0.453834      0.003573   \n",
       "26    S4->S2    3.678762  0.015995  0.079995     2.514545      0.019800   \n",
       "27    S4->S3   22.586227  0.098201  0.215609    21.093086      0.166087   \n",
       "28    S4->S4  134.511382  0.584832  0.438940    48.446972      0.381472   \n",
       "29   S4->REM    1.068465  0.004645  0.065837     0.038462      0.000303   \n",
       "30    REM->W    4.255135  0.018501  0.033948     2.317329      0.018247   \n",
       "31   REM->S1    5.264456  0.022889  0.039833     3.933551      0.030973   \n",
       "32   REM->S2    3.695046  0.016065  0.017879     1.976139      0.015560   \n",
       "33   REM->S3    0.276820  0.001204  0.006982     0.169087      0.001331   \n",
       "34   REM->S4    0.015619  0.000068  0.000797     0.011364      0.000089   \n",
       "35  REM->REM  215.492923  0.936926  0.087101   118.592530      0.933799   \n",
       "\n",
       "    Healthy_STD  Disorder_Sum  Disorder_Mean  Disorder_STD       P_Value  \\\n",
       "0      0.120193     91.345712       0.886852      0.096074  3.481301e-02   \n",
       "1      0.099497      9.028017       0.087651      0.082186  2.277298e-03   \n",
       "2      0.027523      2.445291       0.023741      0.033563  7.522503e-03   \n",
       "3      0.004927      0.111348       0.001081      0.004890  7.272645e-01   \n",
       "4      0.010398      0.008130       0.000079      0.000797  4.121993e-01   \n",
       "5      0.011837      0.061502       0.000597      0.002123  3.217895e-03   \n",
       "6      0.053088      5.276448       0.051228      0.078752  2.102759e-03   \n",
       "7      0.142570     78.521282       0.762343      0.130624  4.897664e-06   \n",
       "8      0.116463     17.404240       0.168973      0.100755  2.128943e-03   \n",
       "9      0.024687      0.636425       0.006179      0.028607  8.711021e-01   \n",
       "10     0.002267      0.037113       0.000360      0.002456  6.595031e-01   \n",
       "11     0.030251      1.124493       0.010917      0.020013  4.013109e-04   \n",
       "12     0.010992      2.705894       0.026271      0.080325  1.036297e-01   \n",
       "13     0.112111      3.478532       0.033772      0.091348  2.269664e-01   \n",
       "14     0.138031     93.261620       0.905453      0.177832  1.698499e-01   \n",
       "15     0.044281      2.450230       0.023789      0.036392  3.588028e-05   \n",
       "16     0.002351      0.015561       0.000151      0.000815  6.547331e-02   \n",
       "17     0.007044      1.088163       0.010565      0.014633  7.167950e-01   \n",
       "18     0.031093      1.007314       0.009780      0.054316  6.017798e-01   \n",
       "19     0.046974      2.060091       0.020001      0.104887  6.933089e-01   \n",
       "20     0.282573      3.982004       0.038660      0.042719  3.887191e-18   \n",
       "21     0.288398     91.715468       0.890441      0.168750  2.200777e-24   \n",
       "22     0.101889      2.917822       0.028328      0.019256  2.356107e-05   \n",
       "23     0.013182      0.317301       0.003081      0.014809  6.598637e-01   \n",
       "24     0.011202      0.440406       0.004276      0.007737  5.881061e-01   \n",
       "25     0.019148      2.807823       0.027260      0.146637  7.418514e-02   \n",
       "26     0.097977      1.164217       0.011303      0.049125  4.253375e-01   \n",
       "27     0.271468      1.493141       0.014497      0.015860  5.144747e-08   \n",
       "28     0.408706     86.064410       0.835577      0.332269  6.105710e-17   \n",
       "29     0.003399      1.030003       0.010000      0.098045  2.686266e-01   \n",
       "30     0.021502      1.937806       0.018814      0.044757  9.003139e-01   \n",
       "31     0.048043      1.330904       0.012921      0.022741  5.743439e-04   \n",
       "32     0.020125      1.718908       0.016688      0.014620  6.359048e-01   \n",
       "33     0.007632      0.107733       0.001046      0.006083  7.591080e-01   \n",
       "34     0.001004      0.004255       0.000041      0.000417  6.503802e-01   \n",
       "35     0.062999     96.900394       0.940781      0.109638  5.475819e-01   \n",
       "\n",
       "         AUC  Wilcoxon_zscore  Wilcoxon_pvalue  MannWhitney_statistic  \\\n",
       "0   0.403218        -2.522887     1.163957e-02                 5274.5   \n",
       "1   0.347374         3.978629     6.931372e-05                 4544.0   \n",
       "2   0.653811        -4.009518     6.084289e-05                 4528.5   \n",
       "3   0.511199        -0.291945     7.703283e-01                 6394.0   \n",
       "4   0.500879        -0.022917     9.817163e-01                 6529.0   \n",
       "5   0.422139         2.029669     4.239021e-02                 5522.0   \n",
       "6   0.700673         5.231105     1.684994e-07                 3915.5   \n",
       "7   0.686301        -4.854466     1.207117e-06                 4104.5   \n",
       "8   0.381011         3.101796     1.923502e-03                 4984.0   \n",
       "9   0.505695         0.148464     8.819768e-01                 6466.0   \n",
       "10  0.488457        -0.300913     7.634808e-01                 6389.5   \n",
       "11  0.329600         4.445941     8.750782e-06                 4309.5   \n",
       "12  0.616467         3.036034     2.397124e-03                 5017.0   \n",
       "13  0.732436         6.059115     1.368728e-09                 3500.0   \n",
       "14  0.240081        -6.775527     1.239536e-11                 3140.5   \n",
       "15  0.689626         4.943145     7.687208e-07                 4060.0   \n",
       "16  0.464185         0.933628     3.504959e-01                 6072.0   \n",
       "17  0.639248         3.625903     2.879531e-04                 4721.0   \n",
       "18  0.442665         1.494602     1.350185e-01                 5790.5   \n",
       "19  0.559399         1.548407     1.215243e-01                 5763.5   \n",
       "20  0.831206         8.633815     5.933901e-18                 2208.0   \n",
       "21  0.886821       -10.083578     6.530362e-24                 1480.5   \n",
       "22  0.484023         0.416496     6.770474e-01                 6331.5   \n",
       "23  0.501567         0.040852     9.674135e-01                 6520.0   \n",
       "24  0.391981        -2.815829     4.865151e-03                 5127.5   \n",
       "25  0.519838        -0.517132     6.050639e-01                 6281.0   \n",
       "26  0.602248        -2.665373     7.690303e-03                 5203.0   \n",
       "27  0.528362         0.739330     4.597069e-01                 6169.5   \n",
       "28  0.813890        -8.182445     2.781413e-16                 2434.5   \n",
       "29  0.484596        -0.401550     6.880155e-01                 6339.0   \n",
       "30  0.585506         1.447771     1.476812e-01                 5814.0   \n",
       "31  0.335869         4.278546     1.881181e-05                 4393.5   \n",
       "32  0.445532        -1.419871     1.556451e-01                 5828.0   \n",
       "33  0.495184        -0.125547     9.000909e-01                 6477.5   \n",
       "34  0.500879        -0.022917     9.817163e-01                 6529.0   \n",
       "35  0.361517        -3.609961     3.062432e-04                 4729.0   \n",
       "\n",
       "    MannWhitney_pvalue  \n",
       "0         5.836077e-03  \n",
       "1         3.479935e-05  \n",
       "2         1.431124e-05  \n",
       "3         2.483123e-01  \n",
       "4         4.457874e-01  \n",
       "5         1.180169e-03  \n",
       "6         8.178827e-08  \n",
       "7         6.063551e-07  \n",
       "8         9.646592e-04  \n",
       "9         3.975669e-01  \n",
       "10        1.386924e-01  \n",
       "11        1.435113e-06  \n",
       "12        1.201852e-03  \n",
       "13        6.512661e-10  \n",
       "14        6.240471e-12  \n",
       "15        3.857922e-07  \n",
       "16        2.817489e-02  \n",
       "17        1.445188e-04  \n",
       "18        4.815483e-02  \n",
       "19        2.506618e-02  \n",
       "20        2.969660e-18  \n",
       "21        3.203071e-24  \n",
       "22        3.366042e-01  \n",
       "23        4.705984e-01  \n",
       "24        2.576184e-04  \n",
       "25        2.044395e-01  \n",
       "26        1.544964e-03  \n",
       "27        2.253108e-01  \n",
       "28        4.354174e-17  \n",
       "29        5.640681e-02  \n",
       "30        7.188721e-02  \n",
       "31        5.780181e-06  \n",
       "32        7.683629e-02  \n",
       "33        3.732689e-01  \n",
       "34        4.457874e-01  \n",
       "35        1.536876e-04  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39f51517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features_baseOn_pValue(tstat_dataset, pval=0.05): \n",
    "    tsdf = tstat_dataset[(tstat_dataset['P_Value']<=pval)] \n",
    "    return tsdf \n",
    "\n",
    "def get_top_features_baseOn_AUC(tstat_dataset, auc=0.5): \n",
    "    tsdf = tstat_dataset[(tstat_dataset['AUC']>auc)] \n",
    "    return tsdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cbfc6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_featute_names_baseOn_pValue(tstat_dataset, pval=0.05): \n",
    "    top_feat_df = get_top_features_baseOn_pValue(tstat_dataset, pval=pval)\n",
    "    top_feat_df\n",
    "    return top_feat_df['Features'].values.tolist() \n",
    "\n",
    "def get_top_featute_names_baseOn_AUC(tstat_dataset, auc=0.5): \n",
    "    top_feat_df = get_top_features_baseOn_AUC(tstat_dataset, auc=auc)\n",
    "    top_feat_df\n",
    "    return top_feat_df['Features'].values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f953c3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,\n",
       " ['W->W',\n",
       "  'W->S1',\n",
       "  'W->S2',\n",
       "  'W->REM',\n",
       "  'S1->W',\n",
       "  'S1->S1',\n",
       "  'S1->S2',\n",
       "  'S1->REM',\n",
       "  'S2->S3',\n",
       "  'S3->S2',\n",
       "  'S3->S3',\n",
       "  'S3->S4',\n",
       "  'S4->S3',\n",
       "  'S4->S4',\n",
       "  'REM->S1'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feats_p = get_top_featute_names_baseOn_pValue(stat_dataset, pval=0.05)\n",
    "len(top_feats_p), top_feats_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d82c9b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,\n",
       " ['W->S2',\n",
       "  'S1->W',\n",
       "  'S1->S1',\n",
       "  'S2->W',\n",
       "  'S2->S1',\n",
       "  'S2->S3',\n",
       "  'S2->REM',\n",
       "  'S3->S2',\n",
       "  'S3->S3',\n",
       "  'S4->S2',\n",
       "  'S4->S4'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feats_a = get_top_featute_names_baseOn_AUC(stat_dataset, auc=0.6)\n",
    "len(top_feats_a), top_feats_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c65e7ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['Dataset'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "381020e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subject_Name</th>\n",
       "      <th>W-&gt;W</th>\n",
       "      <th>W-&gt;S1</th>\n",
       "      <th>W-&gt;S2</th>\n",
       "      <th>W-&gt;S3</th>\n",
       "      <th>W-&gt;S4</th>\n",
       "      <th>W-&gt;REM</th>\n",
       "      <th>S1-&gt;W</th>\n",
       "      <th>...</th>\n",
       "      <th>S4-&gt;S2</th>\n",
       "      <th>S4-&gt;S3</th>\n",
       "      <th>S4-&gt;S4</th>\n",
       "      <th>S4-&gt;REM</th>\n",
       "      <th>REM-&gt;W</th>\n",
       "      <th>REM-&gt;S1</th>\n",
       "      <th>REM-&gt;S2</th>\n",
       "      <th>REM-&gt;S3</th>\n",
       "      <th>REM-&gt;S4</th>\n",
       "      <th>REM-&gt;REM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux1</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.970443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.966480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux2</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.984940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb1</td>\n",
       "      <td>0.876147</td>\n",
       "      <td>0.123853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.949239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.962617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb2</td>\n",
       "      <td>0.948387</td>\n",
       "      <td>0.045161</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.974359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb3</td>\n",
       "      <td>0.934272</td>\n",
       "      <td>0.046948</td>\n",
       "      <td>0.018779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.935065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7191</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7201</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.932836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7211</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7221</td>\n",
       "      <td>0.818792</td>\n",
       "      <td>0.181208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021645</td>\n",
       "      <td>0.043290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.935065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7241</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dataset Category Subject_Name      W->W     W->S1     W->S2  W->S3  \\\n",
       "0     CAP_Sleep     brux        brux1  0.798077  0.192308  0.009615    0.0   \n",
       "1     CAP_Sleep     brux        brux2  0.824000  0.176000  0.000000    0.0   \n",
       "2     CAP_Sleep      sdb         sdb1  0.876147  0.123853  0.000000    0.0   \n",
       "3     CAP_Sleep      sdb         sdb2  0.948387  0.045161  0.006452    0.0   \n",
       "4     CAP_Sleep      sdb         sdb3  0.934272  0.046948  0.018779    0.0   \n",
       "..          ...      ...          ...       ...       ...       ...    ...   \n",
       "225  Sleep_EDFX        n       ST7191  0.739130  0.246377  0.014493    0.0   \n",
       "226  Sleep_EDFX        n       ST7201  0.294118  0.588235  0.058824    0.0   \n",
       "227  Sleep_EDFX        n       ST7211  0.894118  0.100000  0.000000    0.0   \n",
       "228  Sleep_EDFX        n       ST7221  0.818792  0.181208  0.000000    0.0   \n",
       "229  Sleep_EDFX        n       ST7241  0.607143  0.285714  0.035714    0.0   \n",
       "\n",
       "     W->S4    W->REM     S1->W  ...    S4->S2    S4->S3    S4->S4  S4->REM  \\\n",
       "0      0.0  0.000000  0.030769  ...  0.004926  0.024631  0.970443      0.0   \n",
       "1      0.0  0.000000  0.061728  ...  0.000000  0.012048  0.984940      0.0   \n",
       "2      0.0  0.000000  0.099237  ...  0.005076  0.030457  0.949239      0.0   \n",
       "3      0.0  0.000000  0.011364  ...  0.008475  0.016949  0.966102      0.0   \n",
       "4      0.0  0.000000  0.108696  ...  0.025974  0.038961  0.935065      0.0   \n",
       "..     ...       ...       ...  ...       ...       ...       ...      ...   \n",
       "225    0.0  0.000000  0.044118  ...  0.013889  0.166667  0.805556      0.0   \n",
       "226    0.0  0.058824  0.014493  ...  0.038462  0.615385  0.346154      0.0   \n",
       "227    0.0  0.005882  0.116667  ...  0.016949  0.254237  0.728814      0.0   \n",
       "228    0.0  0.000000  0.052133  ...  0.000000  0.000000  0.000000      0.0   \n",
       "229    0.0  0.071429  0.047619  ...  0.142857  0.571429  0.142857      0.0   \n",
       "\n",
       "       REM->W   REM->S1   REM->S2  REM->S3  REM->S4  REM->REM  \n",
       "0    0.033520  0.000000  0.000000      0.0      0.0  0.966480  \n",
       "1    0.019324  0.004831  0.000000      0.0      0.0  0.975845  \n",
       "2    0.009346  0.000000  0.028037      0.0      0.0  0.962617  \n",
       "3    0.000000  0.000000  0.025641      0.0      0.0  0.974359  \n",
       "4    0.062500  0.000000  0.000000      0.0      0.0  0.937500  \n",
       "..        ...       ...       ...      ...      ...       ...  \n",
       "225  0.000000  0.020000  0.015000      0.0      0.0  0.965000  \n",
       "226  0.044776  0.007463  0.014925      0.0      0.0  0.932836  \n",
       "227  0.025641  0.000000  0.019231      0.0      0.0  0.955128  \n",
       "228  0.021645  0.043290  0.000000      0.0      0.0  0.935065  \n",
       "229  0.016878  0.016878  0.008439      0.0      0.0  0.957806  \n",
       "\n",
       "[230 rows x 39 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset[~(dataset['Subject_Name'].str.startswith('SC'))]\n",
    "# dataset[~(dataset['Dataset']=='Sleep_EDFX')]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "74636023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CAP_Sleep', 'SDRC', 'Sleep_EDFX'], dtype=object)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Dataset'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfb17349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Annot_Proba_Transition2.csv\n",
      "./Results//_Combined/STP_From_Same_Stages/Subject_One_Night_TrimW/Annot_Proba_Transition2_STAT_bin.csv\n"
     ]
    }
   ],
   "source": [
    "print( f\"{data_directory}/{result_subdirectory}/{annot_type.capitalize()}_{tran_mat_type[mat_info_type]}_Transition{tran_step}.csv\" ) \n",
    "print( f\"{data_directory}/{result_subdirectory}/{annot_type.capitalize()}_{tran_mat_type[mat_info_type]}_Transition{tran_step}_STAT_bin.csv\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f773868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb6ecc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_name</th>\n",
       "      <th>stp_from</th>\n",
       "      <th>exp_description</th>\n",
       "      <th>datasets</th>\n",
       "      <th>feature_selection</th>\n",
       "      <th>special_consideration</th>\n",
       "      <th>classification_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ML0011</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>2 datasets- CAP_Sleep, Sleep_EDFX</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, subject balanced over the fol...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ML0012</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>2 datasets- CAP_Sleep, Sleep_EDFX</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, subject balanced over the fol...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ML0013</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>2 datasets- CAP_Sleep, Sleep_EDFX</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, subject balanced over the fol...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ML10001</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>2 datasets- All</td>\n",
       "      <td>AUC&gt;0.5</td>\n",
       "      <td>Remove all zero transition and W-&gt;W stage, sub...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ML10002</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>2 datasets- All</td>\n",
       "      <td>AUC&gt;0.5</td>\n",
       "      <td>Remove all zero, subject balanced over the fol...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>ML6005</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>3 datasets- All</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, AUC&gt;0.5, subject balanced ove...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>ML6006</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>3 datasets- All</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, AUC&gt;0.6, subject balanced ove...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>ML6007</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>3 datasets- All</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, pval&lt;=0.05, subject balanced ...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>ML6008</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>3 datasets- All</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, AUC&gt;0.5, subject balanced ove...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>ML6009</td>\n",
       "      <td>Same</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "      <td>3 datasets- All</td>\n",
       "      <td>No feature selection</td>\n",
       "      <td>Remove all zero, AUC&gt;0.6, subject balanced ove...</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    exp_name stp_from                                    exp_description  \\\n",
       "0     ML0011     Same  Data classification using sleep transition mat...   \n",
       "1     ML0012     Same  Data classification using sleep transition mat...   \n",
       "2     ML0013     Same  Data classification using sleep transition mat...   \n",
       "3    ML10001     Same  Data classification using sleep transition mat...   \n",
       "4    ML10002     Same  Data classification using sleep transition mat...   \n",
       "..       ...      ...                                                ...   \n",
       "103   ML6005     Same  Data classification using sleep transition mat...   \n",
       "104   ML6006     Same  Data classification using sleep transition mat...   \n",
       "105   ML6007     Same  Data classification using sleep transition mat...   \n",
       "106   ML6008     Same  Data classification using sleep transition mat...   \n",
       "107   ML6009     Same  Data classification using sleep transition mat...   \n",
       "\n",
       "                              datasets     feature_selection  \\\n",
       "0    2 datasets- CAP_Sleep, Sleep_EDFX  No feature selection   \n",
       "1    2 datasets- CAP_Sleep, Sleep_EDFX  No feature selection   \n",
       "2    2 datasets- CAP_Sleep, Sleep_EDFX  No feature selection   \n",
       "3                      2 datasets- All               AUC>0.5   \n",
       "4                      2 datasets- All               AUC>0.5   \n",
       "..                                 ...                   ...   \n",
       "103                    3 datasets- All  No feature selection   \n",
       "104                    3 datasets- All  No feature selection   \n",
       "105                    3 datasets- All  No feature selection   \n",
       "106                    3 datasets- All  No feature selection   \n",
       "107                    3 datasets- All  No feature selection   \n",
       "\n",
       "                                 special_consideration  \\\n",
       "0    Remove all zero, subject balanced over the fol...   \n",
       "1    Remove all zero, subject balanced over the fol...   \n",
       "2    Remove all zero, subject balanced over the fol...   \n",
       "3    Remove all zero transition and W->W stage, sub...   \n",
       "4    Remove all zero, subject balanced over the fol...   \n",
       "..                                                 ...   \n",
       "103  Remove all zero, AUC>0.5, subject balanced ove...   \n",
       "104  Remove all zero, AUC>0.6, subject balanced ove...   \n",
       "105  Remove all zero, pval<=0.05, subject balanced ...   \n",
       "106  Remove all zero, AUC>0.5, subject balanced ove...   \n",
       "107  Remove all zero, AUC>0.6, subject balanced ove...   \n",
       "\n",
       "                              classification_type  \n",
       "0    Binary classification- Healthy vs disordered  \n",
       "1    Binary classification- Healthy vs disordered  \n",
       "2    Binary classification- Healthy vs disordered  \n",
       "3    Binary classification- Healthy vs disordered  \n",
       "4    Binary classification- Healthy vs disordered  \n",
       "..                                            ...  \n",
       "103  Binary classification- Healthy vs disordered  \n",
       "104  Binary classification- Healthy vs disordered  \n",
       "105  Binary classification- Healthy vs disordered  \n",
       "106  Binary classification- Healthy vs disordered  \n",
       "107  Binary classification- Healthy vs disordered  \n",
       "\n",
       "[108 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_sum_df = modify_experiment_information_summarry(result_directory) \n",
    "exp_sum_df \n",
    "\n",
    "# exp_sum_df = modify_experiment_information_summarry(result_directory, dict_dat=exp_detail) \n",
    "# exp_sum_df \n",
    "\n",
    "# exp_sum_df = modify_experiment_information_summarry(result_directory) \n",
    "# exp_sum_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6cb1a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'logger' is not defined\n"
     ]
    }
   ],
   "source": [
    "# pd.read_csv?\n",
    "# exp_detail\n",
    "# exp_name\n",
    "try:\n",
    "    stop_logger(logger)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    logger = None\n",
    "    \n",
    "# logger = None\n",
    "\n",
    "# del logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "081d4f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tran_type\n",
    "tran_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "99cf5dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists at path: ./Results//_Classification/ML10008/\n",
      "./Results//_Classification/ML10008/\n",
      "./Results//_Classification/ML10008/all_exp_info.csv\n",
      "['W->S1', 'W->S2', 'W->S3', 'W->S4', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->S3', 'S1->S4', 'S1->REM', 'S2->W', 'S2->S1', 'S2->S2', 'S2->S3', 'S2->S4', 'S2->REM', 'S3->W', 'S3->S1', 'S3->S2', 'S3->S3', 'S3->S4', 'S3->REM', 'S4->W', 'S4->S1', 'S4->S2', 'S4->S3', 'S4->S4', 'S4->REM', 'REM->W', 'REM->S1', 'REM->S2', 'REM->S3', 'REM->S4', 'REM->REM']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Information</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logger</td>\n",
       "      <td>&lt;Logger Sleep ML Model Analysis (DEBUG)&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>directory</td>\n",
       "      <td>./Results//_Classification/ML10008/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_size</td>\n",
       "      <td>(47, 39)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_columns</td>\n",
       "      <td>['Dataset', 'Category', 'Subject_Name', 'Class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>metadata_column</td>\n",
       "      <td>['Dataset', 'Category', 'Subject_Name']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>class_name</td>\n",
       "      <td>Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>label_map</td>\n",
       "      <td>{'n': 0, 'ins': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>split_column</td>\n",
       "      <td>Subject_Name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>total_unique_classes</td>\n",
       "      <td>[27, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>exp_name</td>\n",
       "      <td>ML10008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stp_from</td>\n",
       "      <td>Same</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>exp_description</td>\n",
       "      <td>Data classification using sleep transition mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>datasets</td>\n",
       "      <td>2 datasets- SDRC, CAP (ins only)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>feature_selection</td>\n",
       "      <td>no selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>special_consideration</td>\n",
       "      <td>Remove all zero transition and W-&gt;W stage, sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>classification_type</td>\n",
       "      <td>Binary classification- Healthy vs disordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>apply_feature_selection</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>is_multiclass_classification</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>model_list</td>\n",
       "      <td>[&lt;ML_Classifiers.LogReg: 'logistic_regression'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>should_use_params</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>is_validate_models</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>result_save_path</td>\n",
       "      <td>./Results//_Classification/ML10008/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>random_state_value</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>custom_splitter</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>split_balance_pattern</td>\n",
       "      <td>[['n'], ['SC', 'ST'], ['brux'], ['sdb'], ['ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>test_split_crieteria</td>\n",
       "      <td>(5, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>test_split_details</td>\n",
       "      <td>5-fold cross validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>training_split_crieteria</td>\n",
       "      <td>(2, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>training_split_details</td>\n",
       "      <td>2-fold 5% random test splitting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>model_selection_matrics</td>\n",
       "      <td>[&lt;ML_Performace_Metrics.RECL: 'recall'&gt;, &lt;ML_P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Information  \\\n",
       "0                         logger   \n",
       "1                      directory   \n",
       "2                   dataset_size   \n",
       "3                dataset_columns   \n",
       "4                metadata_column   \n",
       "5                     class_name   \n",
       "6                      label_map   \n",
       "7                   split_column   \n",
       "8           total_unique_classes   \n",
       "9                       exp_name   \n",
       "10                      stp_from   \n",
       "11               exp_description   \n",
       "12                      datasets   \n",
       "13             feature_selection   \n",
       "14         special_consideration   \n",
       "15           classification_type   \n",
       "16       apply_feature_selection   \n",
       "17  is_multiclass_classification   \n",
       "18                    model_list   \n",
       "19             should_use_params   \n",
       "20            is_validate_models   \n",
       "21              result_save_path   \n",
       "22            random_state_value   \n",
       "23               custom_splitter   \n",
       "24         split_balance_pattern   \n",
       "25          test_split_crieteria   \n",
       "26            test_split_details   \n",
       "27      training_split_crieteria   \n",
       "28        training_split_details   \n",
       "29       model_selection_matrics   \n",
       "\n",
       "                                          Description  \n",
       "0            <Logger Sleep ML Model Analysis (DEBUG)>  \n",
       "1                 ./Results//_Classification/ML10008/  \n",
       "2                                            (47, 39)  \n",
       "3   ['Dataset', 'Category', 'Subject_Name', 'Class...  \n",
       "4             ['Dataset', 'Category', 'Subject_Name']  \n",
       "5                                               Class  \n",
       "6                                  {'n': 0, 'ins': 1}  \n",
       "7                                        Subject_Name  \n",
       "8                                            [27, 20]  \n",
       "9                                             ML10008  \n",
       "10                                               Same  \n",
       "11  Data classification using sleep transition mat...  \n",
       "12                   2 datasets- SDRC, CAP (ins only)  \n",
       "13                                       no selection  \n",
       "14  Remove all zero transition and W->W stage, sub...  \n",
       "15       Binary classification- Healthy vs disordered  \n",
       "16                                              False  \n",
       "17                                              False  \n",
       "18  [<ML_Classifiers.LogReg: 'logistic_regression'...  \n",
       "19                                               True  \n",
       "20                                               True  \n",
       "21                ./Results//_Classification/ML10008/  \n",
       "22                                                312  \n",
       "23                                               True  \n",
       "24  [['n'], ['SC', 'ST'], ['brux'], ['sdb'], ['ins...  \n",
       "25                                             (5, 0)  \n",
       "26                            5-fold cross validation  \n",
       "27                                             (2, 5)  \n",
       "28                    2-fold 5% random test splitting  \n",
       "29  [<ML_Performace_Metrics.RECL: 'recall'>, <ML_P...  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'ML10001'\n",
    "exp_name = 'ML4001'\n",
    "exp_name = 'ML10008'\n",
    "result_save_path = create_experiment_directory(result_directory, exp_name) \n",
    "print(result_save_path) \n",
    "\n",
    "exp_info_path = f\"{result_save_path}all_exp_info.csv\" \n",
    "print(exp_info_path)\n",
    "\n",
    "exp_info_df = pd.read_csv(exp_info_path) \n",
    "exp_info_df \n",
    "\n",
    "top_feats = exp_info_df[(exp_info_df['Information']=='dataset_columns')]['Description'].values[0] \n",
    "top_feats = top_feats.replace('[', '').replace(']', '').split(', ') \n",
    "top_feats = [c.replace('\\'', '') for c in top_feats] \n",
    "# print(top_feats) \n",
    "sel_cols = top_feats[top_feats.index(class_name)+1:] \n",
    "print(sel_cols) \n",
    "exp_info_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "93a0a420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<ML_Classifiers.SVC: 'support_vector_classifier'>,\n",
       " <ML_Classifiers.RF: 'random_forest'>,\n",
       " <ML_Classifiers.XGBoost: 'xtreme_gradient_boosting'>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logger.info(\"Hello\")\n",
    "ML_Classifiers.SVC, ML_Classifiers.RF, ML_Classifiers.XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "bbba1846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Class',\n",
       " ['Dataset', 'Category', 'Subject_Name'],\n",
       " ['Dataset', 'Category', 'Subject_Name', 'Class'],\n",
       " './Results//_Classification/ML10008/')"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state_value = 312\n",
    "class_name = \"Class\" \n",
    "metadata_column = [\"Dataset\", \"Category\", \"Subject_Name\"] \n",
    "all_metadata_columns = metadata_column+[class_name]\n",
    "# # ### #Binary/Multi-class healthy vs disorders \n",
    "# split_column = \"Subject_Name\"  #\"Subject_Name\" for binary or multi-class \n",
    "# split_balance_pattern = [['n'], ['SC', 'ST'], ['brux'], ['sdb'], ['ins'], ['narco'], ['nfle'], ['plm'], ['rbd']] #[['n'], ['SC', 'ST']]  for binary || [['n'], ['SC', 'ST'], ['brux'], ['sdb'], ['ins'], ['narco'], ['nfle'], ['plm'], ['rbd']] for multi-class \n",
    "# # split_balance_pattern = [['n'], ['ST'], ['brux'], ['sdb'], ['ins'], ['narco'], ['nfle'], ['plm'], ['rbd']] #[['n'], ['SC', 'ST']]  for binary || [['n'], ['SC', 'ST'], ['brux'], ['sdb'], ['ins'], ['narco'], ['nfle'], ['plm'], ['rbd']] for multi-class \n",
    "# # split_balance_pattern = [['n'], ['brux'], ['sdb'], ['ins'], ['narco'], ['nfle'], ['plm'], ['rbd']] #[['n'], ['SC', 'ST']]  for binary || [['n'], ['SC', 'ST'], ['brux'], ['sdb'], ['ins'], ['narco'], ['nfle'], ['plm'], ['rbd']] for multi-class \n",
    "# ### #Binary/Multi-class age-group detection  \n",
    "# # split_column = class_name\n",
    "# # split_balance_pattern = [[1]]\n",
    "class_name, metadata_column, all_metadata_columns, result_save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "46301139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### 2 step transition feature selection \n",
    "# dataset\n",
    "# OLD ['S3->S3','S4->S4','S3->S2','W->S1','W->S2','S2->S2','REM->REM','S4->S3','S3->S4','S2->S1','S2->S3','S2->W']\n",
    "\n",
    "### From 1031 \n",
    "# AUC_0_7 = ['S3->S3','W->S1','S3->S4','S3->S2','S4->S4','S2->S2','S4->S3','W->S2','S2->S3','S1->W','S2->S1'] \n",
    "# AUC_0_5 = ['S3->S3','W->S1','S3->S4','S3->S2','S4->S4','S2->S2','S4->S3','W->S2','S2->S3','S1->W','S2->S1','S3->W','S1->REM','S1->S1','REM->REM','S2->REM','S4->S2','REM->W',\n",
    "#            'REM->S1','S2->W','S1->S2','S4->W','S3->S1','W->S3','W->REM','S2->S4','S3->REM']\n",
    "# RF_FI_0_2 = ['S3->S3','S4->S4','S3->S2','W->S1','S2->S2','W->S2','S4->S3','S2->S1','S3->S4','S1->S1','S2->S3','REM->REM','S2->W','S2->REM','REM->W'] \n",
    "\n",
    "# ### Overall AUC\n",
    "# AUC_0_7 = ['W->W', 'S1->W', 'S1->REM', 'S2->S1', 'S2->S2', 'S3->S3', 'S4->S4']\n",
    "# AUC_0_5 = ['W->W', 'W->S2', 'S1->W', 'S1->REM', 'S2->S1', 'S2->S2', 'S2->S4', 'S2->REM', 'S3->S2', 'S3->S3', 'S4->W', 'S4->S2', 'S4->S3', 'S4->S4', 'REM->S2']\n",
    "\n",
    "### From 1135  \n",
    "AUC_0_7 = ['S3->S3', 'S4->S4', 'S2->S2', 'S4->S3', 'REM->REM', 'S3->S4', 'S2->REM', 'S3->S2', 'S2->S1', 'W->S2', 'S1->REM'] \n",
    "AUC_0_5 = ['S3->S3', 'S4->S4', 'S2->S2', 'S4->S3', 'REM->REM', 'S3->S4', 'S2->REM', 'S3->S2', 'S2->S1', 'W->S2', 'S1->REM', 'S4->S2', 'S1->W', 'S4->W', 'REM->S2', \n",
    "           'S2->W', 'S2->S3', 'REM->S1', 'S3->W', 'S2->S4', 'S1->S1', 'W->S3'] \n",
    "RF_FI_0_2 = ['S3->S3','S4->S4','S2->S2','REM->REM','S3->S2','S3->S4','S2->S1','S1->REM','S4->S3','S1->S2','S2->S3','W->S2','W->S1','S4->S2','S2->REM','REM->S2','S1->W','S1->S1','S4->W'] \n",
    "\n",
    "theoritically_feasible_transitions = ['W->S1', 'W->S2', 'S1->W', 'S1->S1', 'S1->S2', 'S2->W', 'S2->S1', 'S2->S2', 'S2->S3', 'S2->S4', 'S2->REM', 'S3->W', 'S3->S2', 'S3->S3', 'S3->S4',\n",
    "                                     'S4->W', 'S4->S2', 'S4->S3', 'S4->S4', 'REM->W', 'REM->S2', 'REM->REM'] \n",
    "feats_cant_seperate_both_healthy = ['W->W', 'W->S1', 'W->REM', 'S1->W', 'S1->S1', 'S1->REM', 'S2->W', 'S2->S1', 'S2->S2', 'S2->S3', 'S2->REM', 'S3->S2', 'S3->S3', 'S4->S2', 'S4->S4', \n",
    "                                    'REM->W', 'REM->S1', 'REM->S2', 'REM->REM']\n",
    "# feats_cant_seperate_both_healthy2 = ['W->REM', 'S1->W', 'S1->S2', 'S2->S1', 'S2->REM', 'S3->S2', 'S3->S4', 'S4->S1', 'S4->S2', 'REM->W', 'REM->S1', 'REM->S2']\n",
    "\n",
    "theoritically_infeasible_transitions = ['W->S3', 'W->S4', 'W->REM', 'S1->S3', 'S1->S4', 'S1->REM', 'S3->S1', 'S3->REM', 'S4->S1', 'S4->REM', 'REM->S1', 'REM->S3', 'REM->S4'] \n",
    "\n",
    "len(AUC_0_7),len(AUC_0_5),len(RF_FI_0_2) \n",
    "\n",
    "\n",
    "top_feats = sel_cols \n",
    "# top_feats = get_top_featute_names_baseOn_pValue(stat_dataset, pval=0.05)\n",
    "# top_feats = get_top_featute_names_baseOn_AUC(stat_dataset, auc=0.6)\n",
    "len(top_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "06efc193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature values greater than or equal to 1% of the transitions\n",
    "# processed_dataset = dataset.copy()\n",
    "# dd = ((processed_dataset.mean()/processed_dataset.mean().sum())>=0.01) \n",
    "# dd = dd[dd==True]\n",
    "# type(dd), dd.index.tolist(), dd\n",
    "# take_feats = dd.index.tolist() \n",
    "# take_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "db248760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CAP_Sleep'], dtype=object)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset['Dataset'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "447e48d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n', 'brux', 'sdb', 'ins', 'narco', 'nfle', 'plm', 'rbd']\n",
      "['n', 'brux', 'sdb', 'ins', 'narco', 'nfle', 'plm', 'rbd']\n",
      "{'n': 0, 'brux': 1, 'sdb': 1, 'ins': 1, 'narco': 1, 'nfle': 1, 'plm': 1, 'rbd': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subject_Name</th>\n",
       "      <th>Class</th>\n",
       "      <th>W-&gt;S1</th>\n",
       "      <th>W-&gt;S2</th>\n",
       "      <th>W-&gt;S3</th>\n",
       "      <th>W-&gt;S4</th>\n",
       "      <th>W-&gt;REM</th>\n",
       "      <th>S1-&gt;W</th>\n",
       "      <th>...</th>\n",
       "      <th>S4-&gt;S2</th>\n",
       "      <th>S4-&gt;S3</th>\n",
       "      <th>S4-&gt;S4</th>\n",
       "      <th>S4-&gt;REM</th>\n",
       "      <th>REM-&gt;W</th>\n",
       "      <th>REM-&gt;S1</th>\n",
       "      <th>REM-&gt;S2</th>\n",
       "      <th>REM-&gt;S3</th>\n",
       "      <th>REM-&gt;S4</th>\n",
       "      <th>REM-&gt;REM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.970443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.966480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>brux</td>\n",
       "      <td>brux2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.984940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.123853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.949239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.962617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045161</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.974359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAP_Sleep</td>\n",
       "      <td>sdb</td>\n",
       "      <td>sdb3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.046948</td>\n",
       "      <td>0.018779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.935065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7191</td>\n",
       "      <td>0</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7201</td>\n",
       "      <td>0</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.932836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7211</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7221</td>\n",
       "      <td>0</td>\n",
       "      <td>0.181208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021645</td>\n",
       "      <td>0.043290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.935065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Sleep_EDFX</td>\n",
       "      <td>n</td>\n",
       "      <td>ST7241</td>\n",
       "      <td>0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dataset Category Subject_Name  Class     W->S1     W->S2  W->S3  \\\n",
       "0     CAP_Sleep     brux        brux1      1  0.192308  0.009615    0.0   \n",
       "1     CAP_Sleep     brux        brux2      1  0.176000  0.000000    0.0   \n",
       "2     CAP_Sleep      sdb         sdb1      1  0.123853  0.000000    0.0   \n",
       "3     CAP_Sleep      sdb         sdb2      1  0.045161  0.006452    0.0   \n",
       "4     CAP_Sleep      sdb         sdb3      1  0.046948  0.018779    0.0   \n",
       "..          ...      ...          ...    ...       ...       ...    ...   \n",
       "178  Sleep_EDFX        n       ST7191      0  0.246377  0.014493    0.0   \n",
       "179  Sleep_EDFX        n       ST7201      0  0.588235  0.058824    0.0   \n",
       "180  Sleep_EDFX        n       ST7211      0  0.100000  0.000000    0.0   \n",
       "181  Sleep_EDFX        n       ST7221      0  0.181208  0.000000    0.0   \n",
       "182  Sleep_EDFX        n       ST7241      0  0.285714  0.035714    0.0   \n",
       "\n",
       "     W->S4    W->REM     S1->W  ...    S4->S2    S4->S3    S4->S4  S4->REM  \\\n",
       "0      0.0  0.000000  0.030769  ...  0.004926  0.024631  0.970443      0.0   \n",
       "1      0.0  0.000000  0.061728  ...  0.000000  0.012048  0.984940      0.0   \n",
       "2      0.0  0.000000  0.099237  ...  0.005076  0.030457  0.949239      0.0   \n",
       "3      0.0  0.000000  0.011364  ...  0.008475  0.016949  0.966102      0.0   \n",
       "4      0.0  0.000000  0.108696  ...  0.025974  0.038961  0.935065      0.0   \n",
       "..     ...       ...       ...  ...       ...       ...       ...      ...   \n",
       "178    0.0  0.000000  0.044118  ...  0.013889  0.166667  0.805556      0.0   \n",
       "179    0.0  0.058824  0.014493  ...  0.038462  0.615385  0.346154      0.0   \n",
       "180    0.0  0.005882  0.116667  ...  0.016949  0.254237  0.728814      0.0   \n",
       "181    0.0  0.000000  0.052133  ...  0.000000  0.000000  0.000000      0.0   \n",
       "182    0.0  0.071429  0.047619  ...  0.142857  0.571429  0.142857      0.0   \n",
       "\n",
       "       REM->W   REM->S1   REM->S2  REM->S3  REM->S4  REM->REM  \n",
       "0    0.033520  0.000000  0.000000      0.0      0.0  0.966480  \n",
       "1    0.019324  0.004831  0.000000      0.0      0.0  0.975845  \n",
       "2    0.009346  0.000000  0.028037      0.0      0.0  0.962617  \n",
       "3    0.000000  0.000000  0.025641      0.0      0.0  0.974359  \n",
       "4    0.062500  0.000000  0.000000      0.0      0.0  0.937500  \n",
       "..        ...       ...       ...      ...      ...       ...  \n",
       "178  0.000000  0.020000  0.015000      0.0      0.0  0.965000  \n",
       "179  0.044776  0.007463  0.014925      0.0      0.0  0.932836  \n",
       "180  0.025641  0.000000  0.019231      0.0      0.0  0.955128  \n",
       "181  0.021645  0.043290  0.000000      0.0      0.0  0.935065  \n",
       "182  0.016878  0.016878  0.008439      0.0      0.0  0.957806  \n",
       "\n",
       "[183 rows x 39 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processed_dataset = dataset.copy() if not random_state_value else dataset.copy().sample(frac=1, random_state=random_state_value).reset_index(drop=True) \n",
    "processed_dataset = dataset.copy()\n",
    "all_cols = processed_dataset.columns.values.tolist() \n",
    "last_metadata_col_indx = all_cols.index('Subject_Name')+1 \n",
    "\n",
    "# ### Select some features based on dataset \n",
    "# processed_dataset = processed_dataset[~(processed_dataset['Subject_Name'].str.startswith('SC'))] #dataset[~(dataset['Subject_Name'].str.startswith('SC'))]\n",
    "# processed_dataset = processed_dataset[~(processed_dataset['Dataset']=='Sleep_EDFX')] #dataset[~(dataset['Subject_Name'].str.startswith('SC'))]\n",
    "\n",
    "### Select specific group of features from % of transitions\n",
    "# selected_feats = all_cols[:last_metadata_col_indx]+take_feats\n",
    "selected_feats = all_cols[:last_metadata_col_indx]+top_feats\n",
    "selected_feats \n",
    "processed_dataset = processed_dataset.loc[:, selected_feats]\n",
    "\n",
    "# ### Remove zero and wake-to-wake and specific group \n",
    "# removable_feats = []\n",
    "# zero_feats = get_features_with_zero_values(processed_dataset)\n",
    "# zero_feats\n",
    "# removable_feats.extend(zero_feats)\n",
    "# removable_feats.extend(['W->W'])\n",
    "# removable_feats.extend(feats_cant_seperate_both_healthy) \n",
    "# removable_feats.extend(theoritically_infeasible_transitions) \n",
    "# print(f'Removed: {removable_feats}')\n",
    "# removable_feats \n",
    "# processed_dataset = processed_dataset.drop(removable_feats, axis=1, errors='ignore')\n",
    "\n",
    "### Remove features with <5% transitions\n",
    "\n",
    "\n",
    "### Select specific group of features \n",
    "# # # processed_dataset = processed_dataset[all_metadata_columns+sorted_PAUC_df]\n",
    "# # processed_dataset = processed_dataset.drop(removable_feats, axis=1)\n",
    "# # # select features from manual feature selection list: from previous AUC or RF \n",
    "# selected_feats = all_cols[:last_metadata_col_indx]+theoritically_feasible_transitions\n",
    "# selected_feats = all_cols[:last_metadata_col_indx]+feats_cant_seperate_both_healthy\n",
    "# selected_feats \n",
    "# processed_dataset = processed_dataset.loc[:, selected_feats]\n",
    "\n",
    "### #Binary/Multi-class healthy vs disorders \n",
    "# processed_dataset = processed_dataset[~processed_dataset['Category'].isin(['brux', 'sdb'])]### Brux and sdb is cancelled coz of low number to fit in 5 fold\n",
    "label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=None, multi_class=False)\n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=None, multi_class=True)\n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=['brux', 'sdb', 'narco', 'ins', 'plm'], multi_class=True) \n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=['brux', 'sdb', 'narco', 'ins', 'plm'], multi_class=True) \n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=['brux', 'sdb', 'narco', 'ins', 'plm'], multi_class=True) \n",
    "\n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=None, multi_class=False, age_data=all_demography_detail_df.copy(), age_col = 'Age', source_cols=['Subject_Name', 'File_Name'])\n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=None, multi_class=False, age_data=None, age_col = 'Age', source_cols=['Subject_Name', 'File_Name'])\n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=['brux', 'sdb', 'narco', 'ins', 'plm', 'nfle'], multi_class=False, age_data=None, age_col = 'Age', source_cols=['Subject_Name', 'File_Name'])\n",
    "# label_map, processed_dataset = map_category_to_class(processed_dataset.copy(), source_col='Category', class_name='Class', removable_cats=['brux', 'sdb', 'narco', 'ins', 'rbd', 'nfle'], multi_class=False, age_data=None, age_col = 'Age', source_cols=['Subject_Name', 'File_Name'])\n",
    "\n",
    "\n",
    "print(label_map)\n",
    "# processed_dataset = processed_dataset[~processed_dataset['Subject_Name'].str.startswith('SC')] \n",
    "# processed_dataset = processed_dataset[~processed_dataset['Subject_Name'].str.startswith('ST')] \n",
    "# processed_dataset = processed_dataset[~(processed_dataset['Dataset']=='Sleep_EDFX')]\n",
    "# processed_dataset = processed_dataset[~(processed_dataset['Dataset']=='SDRC')]\n",
    "processed_dataset = processed_dataset[~((processed_dataset['Dataset'].isin(['CAP_Sleep','SDRC'])) & (processed_dataset['Category'].isin(['n','ins'])))]\n",
    "\n",
    "# ### #Binary/Multi-class age-group detection  \n",
    "# label_map, processed_dataset = map_age_category_to_class(processed_dataset.copy(), all_demography_detail_df.copy(), source_cols=['Subject_Name', 'File_Name'], age_col ='Age', age_ranges = [[0, 30], [40, 200]], class_name='Class', multi_class=True, healthy_only=True) ##19-101 \n",
    "# label_map, processed_dataset = map_age_category_to_class(processed_dataset.copy(), all_demography_detail_df.copy(), source_cols=['Subject_Name', 'File_Name'], age_col ='Age', age_ranges = [[0, 30], [40, 60], [70, 200]], class_name='Class', multi_class=True, healthy_only=True) ##19-101 \n",
    "# label_map, processed_dataset = map_age_category_to_class(processed_dataset.copy(), all_demography_detail_df.copy(), source_cols=['Subject_Name', 'File_Name'], age_col ='Age', age_ranges = [[0, 35], [35, 55], [55, 200]], class_name='Class', multi_class=True, healthy_only=True) ##19-101  \n",
    "# label_map, processed_dataset = map_age_category_to_class(processed_dataset.copy(), all_demography_detail_df.copy(), source_cols=['Subject_Name', 'File_Name'], age_col ='Age', age_ranges = [[0, 35], [55, 200]], class_name='Class', multi_class=True, healthy_only=True) ##19-101 \n",
    "# print(label_map)\n",
    "# processed_dataset\n",
    "\n",
    "processed_dataset = processed_dataset.reset_index(drop=True) \n",
    "# processed_dataset = processed_dataset.reset_index() \n",
    "processed_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "15880a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class       0.453552\n",
       "W->S1       0.101843\n",
       "W->S2       0.013957\n",
       "W->S3       0.000739\n",
       "W->S4       0.000044\n",
       "W->REM      0.003159\n",
       "S1->W       0.064089\n",
       "S1->S1      0.722919\n",
       "S1->S2      0.194690\n",
       "S1->S3      0.000816\n",
       "S1->S4      0.000130\n",
       "S1->REM     0.017354\n",
       "S2->W       0.011961\n",
       "S2->S1      0.015392\n",
       "S2->S2      0.930085\n",
       "S2->S3      0.032220\n",
       "S2->S4      0.000326\n",
       "S2->REM     0.010017\n",
       "S3->W       0.007834\n",
       "S3->S1      0.004172\n",
       "S3->S2      0.217264\n",
       "S3->S3      0.658983\n",
       "S3->S4      0.061760\n",
       "S3->REM     0.000807\n",
       "S4->W       0.004381\n",
       "S4->S1      0.001950\n",
       "S4->S2      0.008586\n",
       "S4->S3      0.121993\n",
       "S4->S4      0.611426\n",
       "S4->REM     0.000298\n",
       "REM->W      0.015896\n",
       "REM->S1     0.016441\n",
       "REM->S2     0.015615\n",
       "REM->S3     0.000075\n",
       "REM->S4     0.000000\n",
       "REM->REM    0.951972\n",
       "dtype: float64"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processed_dataset[~processed_dataset['Subject_Name'].str.startswith('SC')].groupby('Category')['Class'].value_counts() \n",
    "processed_dataset.mean()\n",
    "# processed_dataset.isna().sum()\n",
    "# label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4039bb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAP_Sleep' 'Sleep_EDFX'] [1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Category  Class\n",
       "brux      1          2\n",
       "n         0        100\n",
       "narco     1          5\n",
       "nfle      1         40\n",
       "plm       1         10\n",
       "rbd       1         22\n",
       "sdb       1          4\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(processed_dataset['Dataset'].unique(), processed_dataset['Class'].unique()) \n",
    "processed_dataset.groupby('Category')['Class'].value_counts()\n",
    "# processed_dataset['Category'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9cba9af9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HumachLab_ML_CLassifiers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-203-e599fc4589f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# classifier_obj = HumachLab_ML_CLassifiers(logger=logger, directory=result_save_path, dataset=dataset.copy(), class_name=class_name, metadata_column=metadata_column, split_column=split_column)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mclassifier_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHumachLab_ML_CLassifiers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresult_save_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprocessed_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata_column\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetadata_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_column\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_balance_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit_balance_pattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mclassifier_obj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HumachLab_ML_CLassifiers' is not defined"
     ]
    }
   ],
   "source": [
    "if logger:\n",
    "    stop_logger(logger) \n",
    "util, logger = start_logger(result_save_path, exp_name)\n",
    "\n",
    "# classifier_obj = HumachLab_ML_CLassifiers(logger=logger, directory=result_save_path, dataset=dataset.copy(), class_name=class_name, metadata_column=metadata_column, split_column=split_column) \n",
    "classifier_obj = HumachLab_ML_CLassifiers(logger=logger, directory=result_save_path, dataset=processed_dataset.copy(), class_name=class_name, label_map=label_map, metadata_column=metadata_column, split_column=split_column, random_state_value=random_state_value, split_balance_pattern=split_balance_pattern) \n",
    "\n",
    "classifier_obj \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3c232d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from HumachLab_ML_CLassifiers class\n"
     ]
    }
   ],
   "source": [
    "classifier_obj.print_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8df4b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(5, 0), (5, 20)],\n",
       " [<ML_Classifiers.LogReg: 'logistic_regression'>,\n",
       "  <ML_Classifiers.SVC: 'support_vector_classifier'>,\n",
       "  <ML_Classifiers.NB: 'naive_bayes'>,\n",
       "  <ML_Classifiers.kNN: 'k_nearest_neighbors'>,\n",
       "  <ML_Classifiers.DT: 'decision_tree'>,\n",
       "  <ML_Classifiers.RF: 'random_forest'>,\n",
       "  <ML_Classifiers.GBoost: 'gradient_boosting'>,\n",
       "  <ML_Classifiers.XGBoost: 'xtreme_gradient_boosting'>],\n",
       " './Results//_Classification/ML10001/',\n",
       " True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting_crieteria = [(10, 0), (5, 20)]     ### for test & training (validation) splitting_crieteria (m, n)-m folds, n%:  m=0: loso, m>0: m-fold, (n>0 given m>0) -shuffled random splitting with m-fold, n% testing \n",
    "splitting_crieteria = [(5, 0), (5, 20)]     ### for test & training (validation) splitting_crieteria (m, n)-m folds, n%:  m=0: loso, m>0: m-fold, (n>0 given m>0) -shuffled random splitting with m-fold, n% testing \n",
    "# splitting_crieteria = [(1, 0), (1, 20)]   ## for small data  ### for test & training (validation) splitting_crieteria (m, n)-m folds, n%:  m=0: loso, m>0: m-fold, (n>0 given m>0) -shuffled random splitting with m-fold, n% testing \n",
    "# splitting_crieteria = [(5, 0), (2, 20)]   ## for small data  ### for test & training (validation) splitting_crieteria (m, n)-m folds, n%:  m=0: loso, m>0: m-fold, (n>0 given m>0) -shuffled random splitting with m-fold, n% testing \n",
    "model_list = [ML_Classifiers.LogReg, ML_Classifiers.SVC, ML_Classifiers.NB, ML_Classifiers.kNN, ML_Classifiers.DT, ML_Classifiers.RF, ML_Classifiers.GBoost, ML_Classifiers.XGBoost] # [ML_Classifiers.LogReg, ML_Classifiers.SVC, ML_Classifiers.NB, ML_Classifiers.kNN, ML_Classifiers.DT, ML_Classifiers.RF, ML_Classifiers.GBoost] \n",
    "should_use_params = True \n",
    "is_validate_models = True\n",
    "# is_binary_classification = False \n",
    "apply_feature_selection = False\n",
    "custom_splitter = True\n",
    "exp_name = exp_name\n",
    "\n",
    "splitting_crieteria, model_list, result_save_path, should_use_params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ccaf84d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier_obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-a8e00f28b7f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier_obj' is not defined"
     ]
    }
   ],
   "source": [
    "classifier_obj.class_name, classifier_obj.split_column, result_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "daecf3ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'handlers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-d6472b175b03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# best_tr_model, tr_model, tr_model_scores_df, tr_target_and_prediction_df, ts_model, ts_model_scores_df, ts_target_and_prediction_df, ts_fold_info_df, exp_info_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mstop_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# exp_sum_df = modify_experiment_information_summarry(result_directory, dict_dat=exp_detail)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-e390b0a6c632>\u001b[0m in \u001b[0;36mstop_logger\u001b[1;34m(logger)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstop_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mhandlers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremoveHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mhandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'handlers'"
     ]
    }
   ],
   "source": [
    "### Set the classifier parameters in the \"HumachLab_ML_CLassifiers\" class file to run with the parameter \n",
    "# best_tr_model, tr_model, tr_model_scores_df, tr_target_and_prediction_df, ts_model, ts_model_scores_df, ts_target_and_prediction_df, ts_fold_info_df, exp_info_df\n",
    "\n",
    "stop_logger(logger) \n",
    "\n",
    "# exp_sum_df = modify_experiment_information_summarry(result_directory, dict_dat=exp_detail) \n",
    "# exp_sum_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9546d31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b6b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1dde6e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Results//_Classification/ML10008/Models/'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = f\"{result_save_path}Models/\" \n",
    "model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "07e4945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier \n",
    "\n",
    "\n",
    "class HumachLab_ML_Predictors:\n",
    "    def __init__(self, logger, result_save_path, dataset, class_name, label_map, metadata_column):\n",
    "        self.logger = logger \n",
    "        self.result_save_path = result_save_path \n",
    "        self.dataset = dataset  \n",
    "        self.class_name = class_name  \n",
    "        self.label_map = label_map \n",
    "        self.metadata_column = metadata_column \n",
    "        \n",
    "        \n",
    "    def get_trained_models(self): \n",
    "        self.model_save_path = f\"{self.result_save_path}Models/\"  \n",
    "        \n",
    "        all_model_files = glob.glob(f\"{self.model_save_path}best*\") \n",
    "#         print(all_model_files)\n",
    "        \n",
    "        save_path = f\"{self.model_save_path}best_tr_model\" \n",
    "        all_best_tr_model = self.load_models_from_file(save_path, 'Best Training Models')        \n",
    "        \n",
    "        return all_model_files, all_best_tr_model     \n",
    "\n",
    "    \n",
    "    def load_models_from_file(self, save_path, model_type):\n",
    "        models_dict = {} \n",
    "        \n",
    "        save_path = f'{save_path}*'\n",
    "        files = self.sort_string_list(glob.glob(save_path)) \n",
    "        files\n",
    "        selected_files = [[int(fn) for fn in f[len(save_path):].split('.')[0].split('_')] for f in files]\n",
    "        selected_files\n",
    "        \n",
    "        self.logger.info(f'Start retrieving {model_type} model from file...')\n",
    "        model_dict = {}  \n",
    "        for i, (ind, fl) in enumerate(zip(selected_files, files)):\n",
    "            if len(ind)==3:\n",
    "                tsi, tri, modi = ind\n",
    "                mod = None \n",
    "                with open(fl, \"rb\") as f:\n",
    "                    mod = pickle.load(f) \n",
    "                if tsi not in model_dict.keys(): \n",
    "                    model_dict[tsi] = {}\n",
    "                if tri not in model_dict[tsi].keys(): \n",
    "                    model_dict[tsi][tri] = {} \n",
    "                if modi not in model_dict[tsi][tri].keys(): \n",
    "                    model_dict[tsi][tri][modi] = mod \n",
    "            elif len(ind)==2:\n",
    "                tsi, modi = ind\n",
    "                mod = None \n",
    "                with open(fl, \"rb\") as f:\n",
    "                    mod = pickle.load(f) \n",
    "                if tsi not in model_dict.keys(): \n",
    "                    model_dict[tsi] = {}\n",
    "                if modi not in model_dict[tsi].keys(): \n",
    "                    model_dict[tsi][modi] = mod \n",
    "            else:\n",
    "                self.logger.info(f'Doesn\\'t identify {model_type} model file to retrieve...')\n",
    "        \n",
    "        model_dict\n",
    "        self.logger.info(f'Finish retrieving {model_type} model from file...')\n",
    "        return model_dict     \n",
    "    \n",
    "    \n",
    "        \n",
    "    def save_results(self, save_directory, all_pred_scores_df):\n",
    "        # './Results/_Classification/ML001/'\n",
    "        save_path = f\"{save_directory}all_pred_scores.csv\" \n",
    "        all_pred_scores_df.to_csv(save_path, index=False) \n",
    "        print(f\"Saving prediction result to: {save_path}\") \n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def sort_string_list(self, string_list):\n",
    "        ## ref: https://blog.codinghorror.com/sorting-for-humans-natural-sort-order/\n",
    "        \"\"\" Sort the given list in the way that humans expect.\n",
    "        \"\"\"\n",
    "        convert = lambda text: int(text) if text.isdigit() else text\n",
    "        alphanum_key = lambda key: [ convert(c.replace(\"_\",\"\")) for c in re.split('([0-9]+)', key) ]\n",
    "        string_list.sort( key=alphanum_key )\n",
    "        return string_list\n",
    "    \n",
    "    \n",
    "    def prediction(self, model_list, all_best_tr_model):\n",
    "        all_pred_score_df = self.do_prediction(model_list, all_best_tr_model) \n",
    "        self.save_results(self.result_save_path, all_pred_score_df)  \n",
    "        return all_pred_score_df \n",
    "    \n",
    "    \n",
    "    def do_prediction(self, model_list, all_best_tr_model):\n",
    "        all_pred_score_df = pd.DataFrame() \n",
    "        all_cols = self.dataset.columns.values.tolist() \n",
    "        feat_names = all_cols[all_cols.index(self.class_name)+1:] \n",
    "        all_feats = self.dataset[feat_names] \n",
    "        labels = self.dataset[self.class_name].values.tolist()  \n",
    "        mod_act_name_list = [ML_Classifiers.get_actual_name(str(tmp_name._name_)) for tmp_name in model_list] \n",
    "\n",
    "#         model_ids = range(1, len(model_list)+1) #[1] #range(1, len(model_list)+1)\n",
    "\n",
    "        for iter_key, iter_val in all_best_tr_model.items(): \n",
    "            for model_key, model_val in iter_val.items(): \n",
    "#                 print('###===>', iter_key, model_key) \n",
    "                print('###===>', iter_key, model_key, model_val.best_estimator_.__class__.__name__)\n",
    "                mod_feats = model_val.feature_names \n",
    "        #         if 'index' in mod_feats:\n",
    "        #             mod_feats.remove('index')\n",
    "                feat_names.sort()\n",
    "                mod_feats.sort()\n",
    "#                 print('===>', model_key, model_val, model_list[model_key-1], mod_act_name_list[model_key-1]) \n",
    "                mod_act_name = model_val.best_estimator_.__class__.__name__                 \n",
    "                \n",
    "#                 ML_Classifiers.get_actual_name(str(ML_Classifiers.LogReg._name_)) \n",
    "                if (feat_names == mod_feats) and (mod_act_name in mod_act_name_list):\n",
    "                    print('###===>', iter_key, model_key, model_val.best_estimator_.__class__.__name__, mod_act_name_list)\n",
    "#                     print('***===>', iter_key, model_key, model_val, model_val.__dict__)\n",
    "                    prediction = model_val.predict(all_feats) \n",
    "#                     prediction = prediction.tolist() \n",
    "                    prediction_proba = model_val.predict_proba(all_feats) \n",
    "#                     prediction_proba = prediction_proba.tolist() \n",
    "                    \n",
    "                    scores_df = self.calculate_model_scores(model_val, labels, prediction, prediction_proba)\n",
    "                    scores_df.insert(0, \"Model_No\", model_key) \n",
    "                    scores_df.insert(0, \"Test_No\", iter_key) \n",
    "                    all_pred_score_df = pd.concat([all_pred_score_df, scores_df]) \n",
    "                else:\n",
    "                    if (feat_names == mod_feats): \n",
    "                        print('Model is not found.')  \n",
    "                    else:\n",
    "                        print('Data feature and model features are not same: \\n', feat_names, '\\n', mod_feats) \n",
    "        if len(all_pred_score_df)>0:\n",
    "            all_pred_score_df = all_pred_score_df.sort_values(by=[\"Model_No\", \"Test_No\"])\n",
    "        return all_pred_score_df\n",
    "    \n",
    "\n",
    "    \n",
    "    # #########################################################################\n",
    "    # Calculate and save classification details and model scores\n",
    "    # #########################################################################\n",
    "    #############################\n",
    "\n",
    "    def calculate_model_scores(self, mods, y_test, y_pred, y_pred_proba): \n",
    "#         print(y_test, '\\n', y_pred, '\\n', y_pred_proba, '\\n')\n",
    "#         target_labels = np.unique(np.array(y_test)).tolist() \n",
    "        target_labels = sorted( self.dataset[self.class_name].unique().tolist() )\n",
    "        \n",
    "        y_pred = y_pred.tolist() \n",
    "        perf_scores = self.calculate_performance_scores(y_test, y_pred, y_pred_proba, labels=target_labels)  # average = 'weighted', 'macro', 'micro' \n",
    "        confMat = perf_scores['Conf_Mat']\n",
    "\n",
    "        acc = round(perf_scores['ACC'], 3) #Accuracy score or total correct prediction rate \n",
    "        prec = round(perf_scores['PREC'], 3) #precision or positive predictive value (PPV)\n",
    "        reca_sens = round(perf_scores['REC'], 3) #sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "        spec = round(perf_scores['SPE'], 3) #specificity, selectivity or true negative rate (TNR)\n",
    "        f1sc = round(perf_scores['F1SCR'], 3)\n",
    "        auc_s = round(perf_scores['AUC'], 3) \n",
    "        \n",
    "        scr_dict = {'method': str(mods), 'model': mods.best_estimator_, 'model_parameters': mods.best_params_, \n",
    "                    'model_scores': round(mods.best_score_*100,2),\n",
    "                    ML_Performace_Metrics.CONF_MAT.value: confMat, ML_Performace_Metrics.ACC.value: acc, ML_Performace_Metrics.PREC.value: prec,\n",
    "                    ML_Performace_Metrics.RECL.value: reca_sens, ML_Performace_Metrics.SEN.value: reca_sens, ML_Performace_Metrics.SPEC.value: spec,\n",
    "                    ML_Performace_Metrics.F1SCR.value: f1sc, ML_Performace_Metrics.ROC_AUC.value: auc_s}\n",
    "        \n",
    "#         scr_dict = {'method_class': str(mods.__class__.__name__), 'model_name': mods.best_estimator_, 'model_parameters': mods.best_params_, \n",
    "#                     'model_scores': round(mods.best_score_*100,2),\n",
    "#                     ML_Performace_Metrics.CONF_MAT.value: confMat, ML_Performace_Metrics.ACC.value: acc, ML_Performace_Metrics.PREC.value: prec,\n",
    "#                     ML_Performace_Metrics.RECL.value: reca_sens, ML_Performace_Metrics.SEN.value: reca_sens, ML_Performace_Metrics.SPEC.value: spec,\n",
    "#                     ML_Performace_Metrics.F1SCR.value: f1sc, ML_Performace_Metrics.ROC_AUC.value: auc_s}\n",
    "#         scr_dict = {'model_class': str(mods.__class__.__name__), 'method_name': mods.estimator, 'method_parameters': mods.best_params_, \n",
    "#                     'method_scores': round(mods.best_score_*100,2),\n",
    "#                     ML_Performace_Metrics.CONF_MAT.value: confMat, ML_Performace_Metrics.ACC.value: acc, ML_Performace_Metrics.PREC.value: prec,\n",
    "#                     ML_Performace_Metrics.RECL.value: reca_sens, ML_Performace_Metrics.SEN.value: reca_sens, ML_Performace_Metrics.SPEC.value: spec,\n",
    "#                     ML_Performace_Metrics.FPR.value: fpr, ML_Performace_Metrics.FNR.value: fnr, ML_Performace_Metrics.F1SCR.value: f1sc, ML_Performace_Metrics.ROC_AUC.value: auc_s}\n",
    "#         scr_dict = {'model_class': str(mods.__class__.__name__), 'method_name': str(mods), 'method_parameters': mods.best_params_, \n",
    "#                     'method_scores': round(mods.best_score_*100,2),\n",
    "#                     ML_Performace_Metrics.CONF_MAT.value: confMat, ML_Performace_Metrics.ACC.value: acc, ML_Performace_Metrics.PREC.value: prec,\n",
    "#                     ML_Performace_Metrics.RECL.value: reca_sens, ML_Performace_Metrics.SEN.value: reca_sens, ML_Performace_Metrics.SPEC.value: spec,\n",
    "#                     ML_Performace_Metrics.FPR.value: fpr, ML_Performace_Metrics.FNR.value: fnr, ML_Performace_Metrics.F1SCR.value: f1sc, ML_Performace_Metrics.ROC_AUC.value: auc_s}\n",
    "        scr_df = pd.DataFrame([list( scr_dict.values() )], columns=list( scr_dict.keys() )) \n",
    "        self.logger.info(f\"\"\"Score columns: {scr_df.shape} {scr_df.columns.values.tolist()}\"\"\") \n",
    "\n",
    "        return scr_df\n",
    "    \n",
    "    \n",
    "    def calculate_performance_scores(self, y_true, y_pred, y_pred_proba, labels=[0, 1], verbose=2, average='weighted'): # average = 'macro', 'micro', 'weighted' \n",
    "        #### SOURCES: https://www.youtube.com/watch?v=PCHf_7jBor8 \n",
    "        # https://www.mariakhalusova.com/posts/2019-04-17-ml-model-evaluation-metrics-p2/ \n",
    "        # https://www.mariakhalusova.com/posts/2019-04-11-ml-model-evaluation-metrics-p1/\n",
    "        # https://www.evidentlyai.com/classification-metrics/multi-class-metrics \n",
    "        # https://www.kaggle.com/code/nkitgupta/evaluation-metrics-for-multi-class-classification \n",
    "\n",
    "        model_scores = []\n",
    "        true_label_uniq = np.unique(np.array(y_true)).tolist() \n",
    "        print(np.unique(np.array(y_true)), np.unique(np.array(y_pred)))\n",
    "        print(y_true, y_pred) \n",
    "        conf_matrix = confusion_matrix(y_true, y_pred, labels=labels).tolist()\n",
    "        print(np.array(conf_matrix) )\n",
    "\n",
    "        ### For micro averaging and binary class \n",
    "        conf_matrix_arr = np.array(conf_matrix) \n",
    "        one_vs_all_confMat = []     \n",
    "        for label in labels:\n",
    "            tp_lbl = conf_matrix_arr[label, label] \n",
    "            fp_lbl = np.sum(conf_matrix_arr[:, label])-tp_lbl \n",
    "            fn_lbl = np.sum(conf_matrix_arr[label, :])-tp_lbl \n",
    "            tn_lbl = np.sum(conf_matrix_arr)-(tp_lbl+fp_lbl+fn_lbl) \n",
    "            one_vs_all_confMat.append([tn_lbl, fp_lbl, fn_lbl, tp_lbl]) \n",
    "        print(np.array(one_vs_all_confMat)) \n",
    "\n",
    "        tn_tot = np.sum( np.array(one_vs_all_confMat)[:, 0] ) \n",
    "        fp_tot = np.sum( np.array(one_vs_all_confMat)[:, 1] )  \n",
    "        fn_tot = np.sum( np.array(one_vs_all_confMat)[:, 2] )  \n",
    "        tp_tot = np.sum( np.array(one_vs_all_confMat)[:, 3] )\n",
    "\n",
    "        conf_matrix_tol = [[tn_tot, fp_tot], [fn_tot, tp_tot]] \n",
    "        print(np.array(conf_matrix_tol)) \n",
    "\n",
    "        if len(labels)==2:\n",
    "            tn_tot = one_vs_all_confMat[1][0] \n",
    "            fp_tot = one_vs_all_confMat[1][1] \n",
    "            fn_tot = one_vs_all_confMat[1][2] \n",
    "            tp_tot = one_vs_all_confMat[1][3] \n",
    "            average = \"micro\"\n",
    "\n",
    "        result = [] \n",
    "        for label in labels:\n",
    "            precision, recall, fscore, support = precision_recall_fscore_support( np.array(y_true)==label, np.array(y_pred)==label ) \n",
    "            # tmp_fpr, tmp_tpr, tmp_thresholds = roc_curve(np.array(y_true)==label, np.array(y_pred)==label, pos_label=label) \n",
    "            tmp_fpr, tmp_tpr, tmp_thresholds = roc_curve(np.array(y_true)==label, np.array(y_pred)==label) \n",
    "            auc_score = auc(tmp_fpr, tmp_tpr)*100 \n",
    "\n",
    "            if label in true_label_uniq: \n",
    "                result.append( [label, precision[1], recall[1], recall[1], recall[0], fscore[1], auc_score, support[1]] ) \n",
    "            else:\n",
    "                result.append( [label, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0] ) \n",
    "\n",
    "            accuracy = accuracy_score(np.array(y_true)==label, np.array(y_pred)==label)*100 \n",
    "            if verbose>1:\n",
    "                print(\n",
    "                    f'Class-wise info: For multilevel internal scores fo label {label}: \\n', \n",
    "                    f'Accuracy = {accuracy}\\n', \n",
    "                    f'Precision = {precision}\\n', \n",
    "                    f'Recall = {recall}\\n', \n",
    "                    f'F1 score = {fscore}\\n', \n",
    "                    f'AUC score = {auc_score}\\n', \n",
    "                    f'Support = {support}\\n', \n",
    "                )\n",
    "        tdf = pd.DataFrame(result, columns=['Label', 'Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', 'AUC', 'Support']) \n",
    "\n",
    "        if average=='macro': #average = \"weighted\", \"macro\", \"micro\" \n",
    "            tdf = tdf[['Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', 'AUC']].apply(lambda col:np.mean(col), axis=0) \n",
    "        elif average=='micro':\n",
    "            prc = (tp_tot / (tp_tot+fp_tot))*100 if (tp_tot+fp_tot)!=0 else 0.0 #precision or positive predictive value (PPV)\n",
    "            rec = (tp_tot / (tp_tot+fn_tot))*100 if (tp_tot+fn_tot)!=0 else 0.0 #sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "            sns = rec #sensitivity same as recall \n",
    "            spc = (tn_tot / (tn_tot+fp_tot))*100 if (tn_tot+fp_tot)!=0 else 0.0 #specificity, selectivity or true negative rate (TNR)\n",
    "            f1s = (2*tp_tot / (2*tp_tot+fp_tot+fn_tot))*100 if (2*tp_tot+fp_tot+fn_tot)!=0 else 0.0 #specificity, selectivity or true negative rate (TNR)  \n",
    "            auc_s = roc_auc_score(y_true, y_pred) if len(labels)==2 else roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average=average) #multi_class='ovo', 'ovr' \n",
    "#             auc_s = 0.5\n",
    "#             if len(labels)==2:\n",
    "#                 auc_s = roc_auc_score(y_true, y_pred)  \n",
    "#             else:\n",
    "#                 auc_s = roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average=average) #multi_class='ovo', 'ovr' \n",
    "            tdf = pd.Series([prc, rec, sns, spc, f1s, auc_s], index=['Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', 'AUC'])  \n",
    "        else: ## Default = weighted\n",
    "            class_weights = tdf['Support']/tdf['Support'].sum() \n",
    "            tdf = tdf[['Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', 'AUC']].apply(lambda col:np.sum(col*class_weights), axis=0) \n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)*100 \n",
    "        tdf_summary = pd.Series([conf_matrix, acc, tdf['Precision'], tdf['Recall'], tdf['Sensitivity'], tdf['Specificity'], tdf['F1 Score'], tdf['AUC']],\n",
    "                               index=['Conf_Mat', 'ACC', 'PREC', 'REC', 'SEN', 'SPE', 'F1SCR', 'AUC'])\n",
    "\n",
    "        if verbose>1:\n",
    "            confMat = tdf_summary['Conf_Mat']\n",
    "            acc = round(tdf_summary['ACC'], 3) #Accuracy score or total correct prediction rate \n",
    "            prec = round(tdf_summary['PREC'], 3) #precision or positive predictive value (PPV)\n",
    "            reca_sens = round(tdf_summary['REC'], 3) #sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "            spec = round(tdf_summary['SPE'], 3) #specificity, selectivity or true negative rate (TNR)\n",
    "            f1sc = round(tdf_summary['F1SCR'], 3)\n",
    "            auc_s = round(tdf_summary['AUC'], 3) \n",
    "            print(\n",
    "                f'CLASSIFICATION MERICS:\\n',\n",
    "                f'{\"_\"*55}\\n',\n",
    "                f'Confusion Matrix: \\n{np.array(conf_matrix)}\\n',\n",
    "                f'Accuracy (acc): {acc}\\n',\n",
    "                f'Precision (prc): {prec}\\n',\n",
    "                f'Recall (rec): {reca_sens}\\n',\n",
    "                f'Sensitivity (sns): {reca_sens}\\n',\n",
    "                f'Specificity (spc): {spec}\\n',\n",
    "                f'F1 Score (f1s): {f1sc}\\n',\n",
    "                f'ROC AUC (AUC): {auc_s}',\n",
    "            )\n",
    "\n",
    "        return tdf_summary\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5f684e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.HumachLab_ML_Predictors at 0x219ffd50a88>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if logger:\n",
    "    stop_logger(logger) \n",
    "util, logger = start_logger(result_save_path, exp_name)\n",
    "\n",
    "predictor_obj = HumachLab_ML_Predictors(logger=logger, result_save_path=result_save_path, dataset=processed_dataset.copy(), class_name=class_name, label_map=label_map, metadata_column=metadata_column) \n",
    "\n",
    "predictor_obj "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "85ecd9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start retrieving Best Training Models model from file...\n",
      "Finish retrieving Best Training Models model from file...\n"
     ]
    }
   ],
   "source": [
    "all_model_files, all_best_tr_model = predictor_obj.get_trained_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "df8f4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_model_files), all_best_tr_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c8a36508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###===> 1 1 LogisticRegression\n",
      "###===> 1 1 LogisticRegression ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[[78 22]\n",
      " [83  0]]\n",
      "[[ 0 83 22 78]\n",
      " [78 22 83  0]]\n",
      "[[ 78 105]\n",
      " [105  78]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 42.62295081967213\n",
      " Precision = [0.         0.48447205]\n",
      " Recall = [0.   0.78]\n",
      " F1 score = [0.         0.59770115]\n",
      " AUC score = 39.0\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 42.62295081967213\n",
      " Precision = [0.48447205 0.        ]\n",
      " Recall = [0.78 0.  ]\n",
      " F1 score = [0.59770115 0.        ]\n",
      " AUC score = 39.0\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[78 22]\n",
      " [83  0]]\n",
      " Accuracy (acc): 42.623\n",
      " Precision (prc): 0.0\n",
      " Recall (rec): 0.0\n",
      " Sensitivity (sns): 0.0\n",
      " Specificity (spc): 78.0\n",
      " F1 Score (f1s): 0.0\n",
      " ROC AUC (AUC): 0.39\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 1 2 SVC\n",
      "###===> 1 2 SVC ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1]\n",
      "[[41 59]\n",
      " [83  0]]\n",
      "[[ 0 83 59 41]\n",
      " [41 59 83  0]]\n",
      "[[ 41 142]\n",
      " [142  41]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 22.404371584699454\n",
      " Precision = [0.         0.33064516]\n",
      " Recall = [0.   0.41]\n",
      " F1 score = [0.         0.36607143]\n",
      " AUC score = 20.5\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 22.404371584699454\n",
      " Precision = [0.33064516 0.        ]\n",
      " Recall = [0.41 0.  ]\n",
      " F1 score = [0.36607143 0.        ]\n",
      " AUC score = 20.5\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[41 59]\n",
      " [83  0]]\n",
      " Accuracy (acc): 22.404\n",
      " Precision (prc): 0.0\n",
      " Recall (rec): 0.0\n",
      " Sensitivity (sns): 0.0\n",
      " Specificity (spc): 41.0\n",
      " F1 Score (f1s): 0.0\n",
      " ROC AUC (AUC): 0.205\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 1 3 GaussianNB\n",
      "###===> 1 3 GaussianNB ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
      "[[61 39]\n",
      " [82  1]]\n",
      "[[ 1 82 39 61]\n",
      " [61 39 82  1]]\n",
      "[[ 62 121]\n",
      " [121  62]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 33.87978142076503\n",
      " Precision = [0.025      0.42657343]\n",
      " Recall = [0.01204819 0.61      ]\n",
      " F1 score = [0.01626016 0.50205761]\n",
      " AUC score = 31.102409638554217\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 33.87978142076503\n",
      " Precision = [0.42657343 0.025     ]\n",
      " Recall = [0.61       0.01204819]\n",
      " F1 score = [0.50205761 0.01626016]\n",
      " AUC score = 31.102409638554214\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[61 39]\n",
      " [82  1]]\n",
      " Accuracy (acc): 33.88\n",
      " Precision (prc): 2.5\n",
      " Recall (rec): 1.205\n",
      " Sensitivity (sns): 1.205\n",
      " Specificity (spc): 61.0\n",
      " F1 Score (f1s): 1.626\n",
      " ROC AUC (AUC): 0.311\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 1 4 KNeighborsClassifier\n",
      "###===> 1 4 KNeighborsClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[100   0]\n",
      " [ 83   0]]\n",
      "[[  0  83   0 100]\n",
      " [100   0  83   0]]\n",
      "[[100  83]\n",
      " [ 83 100]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 54.644808743169406\n",
      " Precision = [0.         0.54644809]\n",
      " Recall = [0. 1.]\n",
      " F1 score = [0.         0.70671378]\n",
      " AUC score = 50.0\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 54.644808743169406\n",
      " Precision = [0.54644809 0.        ]\n",
      " Recall = [1. 0.]\n",
      " F1 score = [0.70671378 0.        ]\n",
      " AUC score = 50.0\n",
      " Support = [100  83]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GaussianNB was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GaussianNB was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[100   0]\n",
      " [ 83   0]]\n",
      " Accuracy (acc): 54.645\n",
      " Precision (prc): 0.0\n",
      " Recall (rec): 0.0\n",
      " Sensitivity (sns): 0.0\n",
      " Specificity (spc): 100.0\n",
      " F1 Score (f1s): 0.0\n",
      " ROC AUC (AUC): 0.5\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 1 5 DecisionTreeClassifier\n",
      "###===> 1 5 DecisionTreeClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1]\n",
      "[[20 80]\n",
      " [31 52]]\n",
      "[[52 31 80 20]\n",
      " [20 80 31 52]]\n",
      "[[ 72 111]\n",
      " [111  72]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 39.34426229508197\n",
      " Precision = [0.39393939 0.39215686]\n",
      " Recall = [0.62650602 0.2       ]\n",
      " F1 score = [0.48372093 0.26490066]\n",
      " AUC score = 41.325301204819276\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 39.34426229508197\n",
      " Precision = [0.39215686 0.39393939]\n",
      " Recall = [0.2        0.62650602]\n",
      " F1 score = [0.26490066 0.48372093]\n",
      " AUC score = 41.325301204819276\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[20 80]\n",
      " [31 52]]\n",
      " Accuracy (acc): 39.344\n",
      " Precision (prc): 39.394\n",
      " Recall (rec): 62.651\n",
      " Sensitivity (sns): 62.651\n",
      " Specificity (spc): 20.0\n",
      " F1 Score (f1s): 48.372\n",
      " ROC AUC (AUC): 0.413\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 1 6 RandomForestClassifier\n",
      "###===> 1 6 RandomForestClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1]\n",
      "[[35 65]\n",
      " [60 23]]\n",
      "[[23 60 65 35]\n",
      " [35 65 60 23]]\n",
      "[[ 58 125]\n",
      " [125  58]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 31.693989071038253\n",
      " Precision = [0.26136364 0.36842105]\n",
      " Recall = [0.27710843 0.35      ]\n",
      " F1 score = [0.26900585 0.35897436]\n",
      " AUC score = 31.355421686746986\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 31.693989071038253\n",
      " Precision = [0.36842105 0.26136364]\n",
      " Recall = [0.35       0.27710843]\n",
      " F1 score = [0.35897436 0.26900585]\n",
      " AUC score = 31.355421686746986\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[35 65]\n",
      " [60 23]]\n",
      " Accuracy (acc): 31.694\n",
      " Precision (prc): 26.136\n",
      " Recall (rec): 27.711\n",
      " Sensitivity (sns): 27.711\n",
      " Specificity (spc): 35.0\n",
      " F1 Score (f1s): 26.901\n",
      " ROC AUC (AUC): 0.314\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 1 7 GradientBoostingClassifier\n",
      "###===> 1 7 GradientBoostingClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1]\n",
      "[[21 79]\n",
      " [34 49]]\n",
      "[[49 34 79 21]\n",
      " [21 79 34 49]]\n",
      "[[ 70 113]\n",
      " [113  70]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 38.25136612021858\n",
      " Precision = [0.3828125  0.38181818]\n",
      " Recall = [0.59036145 0.21      ]\n",
      " F1 score = [0.46445498 0.27096774]\n",
      " AUC score = 40.01807228915663\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 38.25136612021858\n",
      " Precision = [0.38181818 0.3828125 ]\n",
      " Recall = [0.21       0.59036145]\n",
      " F1 score = [0.27096774 0.46445498]\n",
      " AUC score = 40.01807228915663\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[21 79]\n",
      " [34 49]]\n",
      " Accuracy (acc): 38.251\n",
      " Precision (prc): 38.281\n",
      " Recall (rec): 59.036\n",
      " Sensitivity (sns): 59.036\n",
      " Specificity (spc): 21.0\n",
      " F1 Score (f1s): 46.445\n",
      " ROC AUC (AUC): 0.4\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 1 8 XGBClassifier\n",
      "###===> 1 8 XGBClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1]\n",
      "[[25 75]\n",
      " [34 49]]\n",
      "[[49 34 75 25]\n",
      " [25 75 34 49]]\n",
      "[[ 74 109]\n",
      " [109  74]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 40.43715846994536\n",
      " Precision = [0.39516129 0.42372881]\n",
      " Recall = [0.59036145 0.25      ]\n",
      " F1 score = [0.47342995 0.31446541]\n",
      " AUC score = 42.01807228915663\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 40.43715846994536\n",
      " Precision = [0.42372881 0.39516129]\n",
      " Recall = [0.25       0.59036145]\n",
      " F1 score = [0.31446541 0.47342995]\n",
      " AUC score = 42.01807228915663\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[25 75]\n",
      " [34 49]]\n",
      " Accuracy (acc): 40.437\n",
      " Precision (prc): 39.516\n",
      " Recall (rec): 59.036\n",
      " Sensitivity (sns): 59.036\n",
      " Specificity (spc): 25.0\n",
      " F1 Score (f1s): 47.343\n",
      " ROC AUC (AUC): 0.42\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 2 1 LogisticRegression\n",
      "###===> 2 1 LogisticRegression ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
      "[[55 45]\n",
      " [83  0]]\n",
      "[[ 0 83 45 55]\n",
      " [55 45 83  0]]\n",
      "[[ 55 128]\n",
      " [128  55]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 30.05464480874317\n",
      " Precision = [0.         0.39855072]\n",
      " Recall = [0.   0.55]\n",
      " F1 score = [0.         0.46218487]\n",
      " AUC score = 27.500000000000004\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 30.05464480874317\n",
      " Precision = [0.39855072 0.        ]\n",
      " Recall = [0.55 0.  ]\n",
      " F1 score = [0.46218487 0.        ]\n",
      " AUC score = 27.500000000000004\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[55 45]\n",
      " [83  0]]\n",
      " Accuracy (acc): 30.055\n",
      " Precision (prc): 0.0\n",
      " Recall (rec): 0.0\n",
      " Sensitivity (sns): 0.0\n",
      " Specificity (spc): 55.0\n",
      " F1 Score (f1s): 0.0\n",
      " ROC AUC (AUC): 0.275\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 2 2 SVC\n",
      "###===> 2 2 SVC ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1]\n",
      "[[32 68]\n",
      " [82  1]]\n",
      "[[ 1 82 68 32]\n",
      " [32 68 82  1]]\n",
      "[[ 33 150]\n",
      " [150  33]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 18.0327868852459\n",
      " Precision = [0.01449275 0.28070175]\n",
      " Recall = [0.01204819 0.32      ]\n",
      " F1 score = [0.01315789 0.29906542]\n",
      " AUC score = 16.60240963855422\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 18.0327868852459\n",
      " Precision = [0.28070175 0.01449275]\n",
      " Recall = [0.32       0.01204819]\n",
      " F1 score = [0.29906542 0.01315789]\n",
      " AUC score = 16.602409638554214\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[32 68]\n",
      " [82  1]]\n",
      " Accuracy (acc): 18.033\n",
      " Precision (prc): 1.449\n",
      " Recall (rec): 1.205\n",
      " Sensitivity (sns): 1.205\n",
      " Specificity (spc): 32.0\n",
      " F1 Score (f1s): 1.316\n",
      " ROC AUC (AUC): 0.166\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 2 3 GaussianNB\n",
      "###===> 2 3 GaussianNB ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[14 86]\n",
      " [82  1]]\n",
      "[[ 1 82 86 14]\n",
      " [14 86 82  1]]\n",
      "[[ 15 168]\n",
      " [168  15]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 8.19672131147541\n",
      " Precision = [0.01149425 0.14583333]\n",
      " Recall = [0.01204819 0.14      ]\n",
      " F1 score = [0.01176471 0.14285714]\n",
      " AUC score = 7.602409638554219\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 8.19672131147541\n",
      " Precision = [0.14583333 0.01149425]\n",
      " Recall = [0.14       0.01204819]\n",
      " F1 score = [0.14285714 0.01176471]\n",
      " AUC score = 7.602409638554216\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[14 86]\n",
      " [82  1]]\n",
      " Accuracy (acc): 8.197\n",
      " Precision (prc): 1.149\n",
      " Recall (rec): 1.205\n",
      " Sensitivity (sns): 1.205\n",
      " Specificity (spc): 14.0\n",
      " F1 Score (f1s): 1.176\n",
      " ROC AUC (AUC): 0.076\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 2 4 KNeighborsClassifier\n",
      "###===> 2 4 KNeighborsClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[100   0]\n",
      " [ 83   0]]\n",
      "[[  0  83   0 100]\n",
      " [100   0  83   0]]\n",
      "[[100  83]\n",
      " [ 83 100]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 54.644808743169406\n",
      " Precision = [0.         0.54644809]\n",
      " Recall = [0. 1.]\n",
      " F1 score = [0.         0.70671378]\n",
      " AUC score = 50.0\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 54.644808743169406\n",
      " Precision = [0.54644809 0.        ]\n",
      " Recall = [1. 0.]\n",
      " F1 score = [0.70671378 0.        ]\n",
      " AUC score = 50.0\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[100   0]\n",
      " [ 83   0]]\n",
      " Accuracy (acc): 54.645\n",
      " Precision (prc): 0.0\n",
      " Recall (rec): 0.0\n",
      " Sensitivity (sns): 0.0\n",
      " Specificity (spc): 100.0\n",
      " F1 Score (f1s): 0.0\n",
      " ROC AUC (AUC): 0.5\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 2 5 DecisionTreeClassifier\n",
      "###===> 2 5 DecisionTreeClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1]\n",
      "[[20 80]\n",
      " [32 51]]\n",
      "[[51 32 80 20]\n",
      " [20 80 32 51]]\n",
      "[[ 71 112]\n",
      " [112  71]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 38.79781420765027\n",
      " Precision = [0.38931298 0.38461538]\n",
      " Recall = [0.61445783 0.2       ]\n",
      " F1 score = [0.47663551 0.26315789]\n",
      " AUC score = 40.72289156626506\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 38.79781420765027\n",
      " Precision = [0.38461538 0.38931298]\n",
      " Recall = [0.2        0.61445783]\n",
      " F1 score = [0.26315789 0.47663551]\n",
      " AUC score = 40.72289156626506\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[20 80]\n",
      " [32 51]]\n",
      " Accuracy (acc): 38.798\n",
      " Precision (prc): 38.931\n",
      " Recall (rec): 61.446\n",
      " Sensitivity (sns): 61.446\n",
      " Specificity (spc): 20.0\n",
      " F1 Score (f1s): 47.664\n",
      " ROC AUC (AUC): 0.407\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GaussianNB was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GaussianNB was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 6 RandomForestClassifier\n",
      "###===> 2 6 RandomForestClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1]\n",
      "[[22 78]\n",
      " [70 13]]\n",
      "[[13 70 78 22]\n",
      " [22 78 70 13]]\n",
      "[[ 35 148]\n",
      " [148  35]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 19.12568306010929\n",
      " Precision = [0.14285714 0.23913043]\n",
      " Recall = [0.15662651 0.22      ]\n",
      " F1 score = [0.14942529 0.22916667]\n",
      " AUC score = 18.831325301204814\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 19.12568306010929\n",
      " Precision = [0.23913043 0.14285714]\n",
      " Recall = [0.22       0.15662651]\n",
      " F1 score = [0.22916667 0.14942529]\n",
      " AUC score = 18.831325301204814\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[22 78]\n",
      " [70 13]]\n",
      " Accuracy (acc): 19.126\n",
      " Precision (prc): 14.286\n",
      " Recall (rec): 15.663\n",
      " Sensitivity (sns): 15.663\n",
      " Specificity (spc): 22.0\n",
      " F1 Score (f1s): 14.943\n",
      " ROC AUC (AUC): 0.188\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 2 7 GradientBoostingClassifier\n",
      "###===> 2 7 GradientBoostingClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1]\n",
      "[[22 78]\n",
      " [60 23]]\n",
      "[[23 60 78 22]\n",
      " [22 78 60 23]]\n",
      "[[ 45 138]\n",
      " [138  45]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 24.59016393442623\n",
      " Precision = [0.22772277 0.26829268]\n",
      " Recall = [0.27710843 0.22      ]\n",
      " F1 score = [0.25       0.24175824]\n",
      " AUC score = 24.855421686746986\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 24.59016393442623\n",
      " Precision = [0.26829268 0.22772277]\n",
      " Recall = [0.22       0.27710843]\n",
      " F1 score = [0.24175824 0.25      ]\n",
      " AUC score = 24.855421686746983\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[22 78]\n",
      " [60 23]]\n",
      " Accuracy (acc): 24.59\n",
      " Precision (prc): 22.772\n",
      " Recall (rec): 27.711\n",
      " Sensitivity (sns): 27.711\n",
      " Specificity (spc): 22.0\n",
      " F1 Score (f1s): 25.0\n",
      " ROC AUC (AUC): 0.249\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 2 8 XGBClassifier\n",
      "###===> 2 8 XGBClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1]\n",
      "[[30 70]\n",
      " [61 22]]\n",
      "[[22 61 70 30]\n",
      " [30 70 61 22]]\n",
      "[[ 52 131]\n",
      " [131  52]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 28.415300546448087\n",
      " Precision = [0.23913043 0.32967033]\n",
      " Recall = [0.26506024 0.3       ]\n",
      " F1 score = [0.25142857 0.31413613]\n",
      " AUC score = 28.253012048192772\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 28.415300546448087\n",
      " Precision = [0.32967033 0.23913043]\n",
      " Recall = [0.3        0.26506024]\n",
      " F1 score = [0.31413613 0.25142857]\n",
      " AUC score = 28.253012048192772\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[30 70]\n",
      " [61 22]]\n",
      " Accuracy (acc): 28.415\n",
      " Precision (prc): 23.913\n",
      " Recall (rec): 26.506\n",
      " Sensitivity (sns): 26.506\n",
      " Specificity (spc): 30.0\n",
      " F1 Score (f1s): 25.143\n",
      " ROC AUC (AUC): 0.283\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 3 1 LogisticRegression\n",
      "###===> 3 1 LogisticRegression ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[81 19]\n",
      " [83  0]]\n",
      "[[ 0 83 19 81]\n",
      " [81 19 83  0]]\n",
      "[[ 81 102]\n",
      " [102  81]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 44.26229508196721\n",
      " Precision = [0.         0.49390244]\n",
      " Recall = [0.   0.81]\n",
      " F1 score = [0.         0.61363636]\n",
      " AUC score = 40.5\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 44.26229508196721\n",
      " Precision = [0.49390244 0.        ]\n",
      " Recall = [0.81 0.  ]\n",
      " F1 score = [0.61363636 0.        ]\n",
      " AUC score = 40.5\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[81 19]\n",
      " [83  0]]\n",
      " Accuracy (acc): 44.262\n",
      " Precision (prc): 0.0\n",
      " Recall (rec): 0.0\n",
      " Sensitivity (sns): 0.0\n",
      " Specificity (spc): 81.0\n",
      " F1 Score (f1s): 0.0\n",
      " ROC AUC (AUC): 0.405\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 3 2 SVC\n",
      "###===> 3 2 SVC ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[76 24]\n",
      " [82  1]]\n",
      "[[ 1 82 24 76]\n",
      " [76 24 82  1]]\n",
      "[[ 77 106]\n",
      " [106  77]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 42.07650273224044\n",
      " Precision = [0.04       0.48101266]\n",
      " Recall = [0.01204819 0.76      ]\n",
      " F1 score = [0.01851852 0.58914729]\n",
      " AUC score = 38.60240963855422\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 42.07650273224044\n",
      " Precision = [0.48101266 0.04      ]\n",
      " Recall = [0.76       0.01204819]\n",
      " F1 score = [0.58914729 0.01851852]\n",
      " AUC score = 38.602409638554214\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[76 24]\n",
      " [82  1]]\n",
      " Accuracy (acc): 42.077\n",
      " Precision (prc): 4.0\n",
      " Recall (rec): 1.205\n",
      " Sensitivity (sns): 1.205\n",
      " Specificity (spc): 76.0\n",
      " F1 Score (f1s): 1.852\n",
      " ROC AUC (AUC): 0.386\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 3 3 GaussianNB\n",
      "###===> 3 3 GaussianNB ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[11 89]\n",
      " [68 15]]\n",
      "[[15 68 89 11]\n",
      " [11 89 68 15]]\n",
      "[[ 26 157]\n",
      " [157  26]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 14.207650273224044\n",
      " Precision = [0.14423077 0.13924051]\n",
      " Recall = [0.18072289 0.11      ]\n",
      " F1 score = [0.16042781 0.12290503]\n",
      " AUC score = 14.536144578313257\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 14.207650273224044\n",
      " Precision = [0.13924051 0.14423077]\n",
      " Recall = [0.11       0.18072289]\n",
      " F1 score = [0.12290503 0.16042781]\n",
      " AUC score = 14.536144578313253\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[11 89]\n",
      " [68 15]]\n",
      " Accuracy (acc): 14.208\n",
      " Precision (prc): 14.423\n",
      " Recall (rec): 18.072\n",
      " Sensitivity (sns): 18.072\n",
      " Specificity (spc): 11.0\n",
      " F1 Score (f1s): 16.043\n",
      " ROC AUC (AUC): 0.145\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 3 4 KNeighborsClassifier\n",
      "###===> 3 4 KNeighborsClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[100   0]\n",
      " [ 83   0]]\n",
      "[[  0  83   0 100]\n",
      " [100   0  83   0]]\n",
      "[[100  83]\n",
      " [ 83 100]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 54.644808743169406\n",
      " Precision = [0.         0.54644809]\n",
      " Recall = [0. 1.]\n",
      " F1 score = [0.         0.70671378]\n",
      " AUC score = 50.0\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 54.644808743169406\n",
      " Precision = [0.54644809 0.        ]\n",
      " Recall = [1. 0.]\n",
      " F1 score = [0.70671378 0.        ]\n",
      " AUC score = 50.0\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[100   0]\n",
      " [ 83   0]]\n",
      " Accuracy (acc): 54.645\n",
      " Precision (prc): 0.0\n",
      " Recall (rec): 0.0\n",
      " Sensitivity (sns): 0.0\n",
      " Specificity (spc): 100.0\n",
      " F1 Score (f1s): 0.0\n",
      " ROC AUC (AUC): 0.5\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GaussianNB was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GaussianNB was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###===> 3 5 DecisionTreeClassifier\n",
      "###===> 3 5 DecisionTreeClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[[38 62]\n",
      " [33 50]]\n",
      "[[50 33 62 38]\n",
      " [38 62 33 50]]\n",
      "[[88 95]\n",
      " [95 88]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 48.08743169398907\n",
      " Precision = [0.44642857 0.53521127]\n",
      " Recall = [0.60240964 0.38      ]\n",
      " F1 score = [0.51282051 0.44444444]\n",
      " AUC score = 49.12048192771084\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 48.08743169398907\n",
      " Precision = [0.53521127 0.44642857]\n",
      " Recall = [0.38       0.60240964]\n",
      " F1 score = [0.44444444 0.51282051]\n",
      " AUC score = 49.12048192771085\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[38 62]\n",
      " [33 50]]\n",
      " Accuracy (acc): 48.087\n",
      " Precision (prc): 44.643\n",
      " Recall (rec): 60.241\n",
      " Sensitivity (sns): 60.241\n",
      " Specificity (spc): 38.0\n",
      " F1 Score (f1s): 51.282\n",
      " ROC AUC (AUC): 0.491\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 3 6 RandomForestClassifier\n",
      "###===> 3 6 RandomForestClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[28 72]\n",
      " [43 40]]\n",
      "[[40 43 72 28]\n",
      " [28 72 43 40]]\n",
      "[[ 68 115]\n",
      " [115  68]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 37.15846994535519\n",
      " Precision = [0.35714286 0.3943662 ]\n",
      " Recall = [0.48192771 0.28      ]\n",
      " F1 score = [0.41025641 0.32748538]\n",
      " AUC score = 38.096385542168676\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 37.15846994535519\n",
      " Precision = [0.3943662  0.35714286]\n",
      " Recall = [0.28       0.48192771]\n",
      " F1 score = [0.32748538 0.41025641]\n",
      " AUC score = 38.096385542168676\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[28 72]\n",
      " [43 40]]\n",
      " Accuracy (acc): 37.158\n",
      " Precision (prc): 35.714\n",
      " Recall (rec): 48.193\n",
      " Sensitivity (sns): 48.193\n",
      " Specificity (spc): 28.0\n",
      " F1 Score (f1s): 41.026\n",
      " ROC AUC (AUC): 0.381\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 3 7 GradientBoostingClassifier\n",
      "###===> 3 7 GradientBoostingClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1]\n",
      "[[23 77]\n",
      " [36 47]]\n",
      "[[47 36 77 23]\n",
      " [23 77 36 47]]\n",
      "[[ 70 113]\n",
      " [113  70]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 38.25136612021858\n",
      " Precision = [0.37903226 0.38983051]\n",
      " Recall = [0.56626506 0.23      ]\n",
      " F1 score = [0.45410628 0.28930818]\n",
      " AUC score = 39.813253012048186\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 38.25136612021858\n",
      " Precision = [0.38983051 0.37903226]\n",
      " Recall = [0.23       0.56626506]\n",
      " F1 score = [0.28930818 0.45410628]\n",
      " AUC score = 39.81325301204819\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[23 77]\n",
      " [36 47]]\n",
      " Accuracy (acc): 38.251\n",
      " Precision (prc): 37.903\n",
      " Recall (rec): 56.627\n",
      " Sensitivity (sns): 56.627\n",
      " Specificity (spc): 23.0\n",
      " F1 Score (f1s): 45.411\n",
      " ROC AUC (AUC): 0.398\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 3 8 XGBClassifier\n",
      "###===> 3 8 XGBClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1]\n",
      "[[18 82]\n",
      " [42 41]]\n",
      "[[41 42 82 18]\n",
      " [18 82 42 41]]\n",
      "[[ 59 124]\n",
      " [124  59]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 32.240437158469945\n",
      " Precision = [0.33333333 0.3       ]\n",
      " Recall = [0.4939759 0.18     ]\n",
      " F1 score = [0.39805825 0.225     ]\n",
      " AUC score = 33.69879518072289\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 32.240437158469945\n",
      " Precision = [0.3        0.33333333]\n",
      " Recall = [0.18      0.4939759]\n",
      " F1 score = [0.225      0.39805825]\n",
      " AUC score = 33.6987951807229\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[18 82]\n",
      " [42 41]]\n",
      " Accuracy (acc): 32.24\n",
      " Precision (prc): 33.333\n",
      " Recall (rec): 49.398\n",
      " Sensitivity (sns): 49.398\n",
      " Specificity (spc): 18.0\n",
      " F1 Score (f1s): 39.806\n",
      " ROC AUC (AUC): 0.337\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 4 1 LogisticRegression\n",
      "###===> 4 1 LogisticRegression ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1]\n",
      "[[51 49]\n",
      " [83  0]]\n",
      "[[ 0 83 49 51]\n",
      " [51 49 83  0]]\n",
      "[[ 51 132]\n",
      " [132  51]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 27.86885245901639\n",
      " Precision = [0.         0.38059701]\n",
      " Recall = [0.   0.51]\n",
      " F1 score = [0.         0.43589744]\n",
      " AUC score = 25.5\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 27.86885245901639\n",
      " Precision = [0.38059701 0.        ]\n",
      " Recall = [0.51 0.  ]\n",
      " F1 score = [0.43589744 0.        ]\n",
      " AUC score = 25.5\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[51 49]\n",
      " [83  0]]\n",
      " Accuracy (acc): 27.869\n",
      " Precision (prc): 0.0\n",
      " Recall (rec): 0.0\n",
      " Sensitivity (sns): 0.0\n",
      " Specificity (spc): 51.0\n",
      " F1 Score (f1s): 0.0\n",
      " ROC AUC (AUC): 0.255\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 4 2 SVC\n",
      "###===> 4 2 SVC ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1]\n",
      "[[31 69]\n",
      " [82  1]]\n",
      "[[ 1 82 69 31]\n",
      " [31 69 82  1]]\n",
      "[[ 32 151]\n",
      " [151  32]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 17.48633879781421\n",
      " Precision = [0.01428571 0.27433628]\n",
      " Recall = [0.01204819 0.31      ]\n",
      " F1 score = [0.0130719  0.29107981]\n",
      " AUC score = 16.102409638554217\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 17.48633879781421\n",
      " Precision = [0.27433628 0.01428571]\n",
      " Recall = [0.31       0.01204819]\n",
      " F1 score = [0.29107981 0.0130719 ]\n",
      " AUC score = 16.102409638554217\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[31 69]\n",
      " [82  1]]\n",
      " Accuracy (acc): 17.486\n",
      " Precision (prc): 1.429\n",
      " Recall (rec): 1.205\n",
      " Sensitivity (sns): 1.205\n",
      " Specificity (spc): 31.0\n",
      " F1 Score (f1s): 1.307\n",
      " ROC AUC (AUC): 0.161\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 4 3 GaussianNB\n",
      "###===> 4 3 GaussianNB ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1]\n",
      "[[26 74]\n",
      " [83  0]]\n",
      "[[ 0 83 74 26]\n",
      " [26 74 83  0]]\n",
      "[[ 26 157]\n",
      " [157  26]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 14.207650273224044\n",
      " Precision = [0.         0.23853211]\n",
      " Recall = [0.   0.26]\n",
      " F1 score = [0.         0.24880383]\n",
      " AUC score = 13.0\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 14.207650273224044\n",
      " Precision = [0.23853211 0.        ]\n",
      " Recall = [0.26 0.  ]\n",
      " F1 score = [0.24880383 0.        ]\n",
      " AUC score = 13.0\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[26 74]\n",
      " [83  0]]\n",
      " Accuracy (acc): 14.208\n",
      " Precision (prc): 0.0\n",
      " Recall (rec): 0.0\n",
      " Sensitivity (sns): 0.0\n",
      " Specificity (spc): 26.0\n",
      " F1 Score (f1s): 0.0\n",
      " ROC AUC (AUC): 0.13\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 4 4 KNeighborsClassifier\n",
      "###===> 4 4 KNeighborsClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
      "[[50 50]\n",
      " [83  0]]\n",
      "[[ 0 83 50 50]\n",
      " [50 50 83  0]]\n",
      "[[ 50 133]\n",
      " [133  50]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 27.322404371584703\n",
      " Precision = [0.         0.37593985]\n",
      " Recall = [0.  0.5]\n",
      " F1 score = [0.         0.42918455]\n",
      " AUC score = 25.0\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 27.322404371584703\n",
      " Precision = [0.37593985 0.        ]\n",
      " Recall = [0.5 0. ]\n",
      " F1 score = [0.42918455 0.        ]\n",
      " AUC score = 25.0\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[50 50]\n",
      " [83  0]]\n",
      " Accuracy (acc): 27.322\n",
      " Precision (prc): 0.0\n",
      " Recall (rec): 0.0\n",
      " Sensitivity (sns): 0.0\n",
      " Specificity (spc): 50.0\n",
      " F1 Score (f1s): 0.0\n",
      " ROC AUC (AUC): 0.25\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 4 5 DecisionTreeClassifier\n",
      "###===> 4 5 DecisionTreeClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0]\n",
      "[[23 77]\n",
      " [42 41]]\n",
      "[[41 42 77 23]\n",
      " [23 77 42 41]]\n",
      "[[ 64 119]\n",
      " [119  64]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 34.97267759562842\n",
      " Precision = [0.34745763 0.35384615]\n",
      " Recall = [0.4939759 0.23     ]\n",
      " F1 score = [0.4079602  0.27878788]\n",
      " AUC score = 36.19879518072289\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 34.97267759562842\n",
      " Precision = [0.35384615 0.34745763]\n",
      " Recall = [0.23      0.4939759]\n",
      " F1 score = [0.27878788 0.4079602 ]\n",
      " AUC score = 36.19879518072289\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[23 77]\n",
      " [42 41]]\n",
      " Accuracy (acc): 34.973\n",
      " Precision (prc): 34.746\n",
      " Recall (rec): 49.398\n",
      " Sensitivity (sns): 49.398\n",
      " Specificity (spc): 23.0\n",
      " F1 Score (f1s): 40.796\n",
      " ROC AUC (AUC): 0.362\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GaussianNB was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GaussianNB was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###===> 4 6 RandomForestClassifier\n",
      "###===> 4 6 RandomForestClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1]\n",
      "[[58 42]\n",
      " [70 13]]\n",
      "[[13 70 42 58]\n",
      " [58 42 70 13]]\n",
      "[[ 71 112]\n",
      " [112  71]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 38.79781420765027\n",
      " Precision = [0.23636364 0.453125  ]\n",
      " Recall = [0.15662651 0.58      ]\n",
      " F1 score = [0.1884058  0.50877193]\n",
      " AUC score = 36.831325301204814\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 38.79781420765027\n",
      " Precision = [0.453125   0.23636364]\n",
      " Recall = [0.58       0.15662651]\n",
      " F1 score = [0.50877193 0.1884058 ]\n",
      " AUC score = 36.83132530120482\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[58 42]\n",
      " [70 13]]\n",
      " Accuracy (acc): 38.798\n",
      " Precision (prc): 23.636\n",
      " Recall (rec): 15.663\n",
      " Sensitivity (sns): 15.663\n",
      " Specificity (spc): 58.0\n",
      " F1 Score (f1s): 18.841\n",
      " ROC AUC (AUC): 0.368\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 4 7 GradientBoostingClassifier\n",
      "###===> 4 7 GradientBoostingClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "[[17 83]\n",
      " [47 36]]\n",
      "[[36 47 83 17]\n",
      " [17 83 47 36]]\n",
      "[[ 53 130]\n",
      " [130  53]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 28.96174863387978\n",
      " Precision = [0.30252101 0.265625  ]\n",
      " Recall = [0.43373494 0.17      ]\n",
      " F1 score = [0.35643564 0.20731707]\n",
      " AUC score = 30.186746987951807\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 28.96174863387978\n",
      " Precision = [0.265625   0.30252101]\n",
      " Recall = [0.17       0.43373494]\n",
      " F1 score = [0.20731707 0.35643564]\n",
      " AUC score = 30.186746987951807\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[17 83]\n",
      " [47 36]]\n",
      " Accuracy (acc): 28.962\n",
      " Precision (prc): 30.252\n",
      " Recall (rec): 43.373\n",
      " Sensitivity (sns): 43.373\n",
      " Specificity (spc): 17.0\n",
      " F1 Score (f1s): 35.644\n",
      " ROC AUC (AUC): 0.302\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 4 8 XGBClassifier\n",
      "###===> 4 8 XGBClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "[[25 75]\n",
      " [46 37]]\n",
      "[[37 46 75 25]\n",
      " [25 75 46 37]]\n",
      "[[ 62 121]\n",
      " [121  62]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 33.87978142076503\n",
      " Precision = [0.33035714 0.35211268]\n",
      " Recall = [0.44578313 0.25      ]\n",
      " F1 score = [0.37948718 0.29239766]\n",
      " AUC score = 34.78915662650602\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 33.87978142076503\n",
      " Precision = [0.35211268 0.33035714]\n",
      " Recall = [0.25       0.44578313]\n",
      " F1 score = [0.29239766 0.37948718]\n",
      " AUC score = 34.78915662650602\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[25 75]\n",
      " [46 37]]\n",
      " Accuracy (acc): 33.88\n",
      " Precision (prc): 33.036\n",
      " Recall (rec): 44.578\n",
      " Sensitivity (sns): 44.578\n",
      " Specificity (spc): 25.0\n",
      " F1 Score (f1s): 37.949\n",
      " ROC AUC (AUC): 0.348\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 5 1 LogisticRegression\n",
      "###===> 5 1 LogisticRegression ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1]\n",
      "[[47 53]\n",
      " [83  0]]\n",
      "[[ 0 83 53 47]\n",
      " [47 53 83  0]]\n",
      "[[ 47 136]\n",
      " [136  47]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 25.683060109289617\n",
      " Precision = [0.         0.36153846]\n",
      " Recall = [0.   0.47]\n",
      " F1 score = [0.         0.40869565]\n",
      " AUC score = 23.5\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 25.683060109289617\n",
      " Precision = [0.36153846 0.        ]\n",
      " Recall = [0.47 0.  ]\n",
      " F1 score = [0.40869565 0.        ]\n",
      " AUC score = 23.5\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[47 53]\n",
      " [83  0]]\n",
      " Accuracy (acc): 25.683\n",
      " Precision (prc): 0.0\n",
      " Recall (rec): 0.0\n",
      " Sensitivity (sns): 0.0\n",
      " Specificity (spc): 47.0\n",
      " F1 Score (f1s): 0.0\n",
      " ROC AUC (AUC): 0.235\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 5 2 SVC\n",
      "###===> 5 2 SVC ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1]\n",
      "[[29 71]\n",
      " [82  1]]\n",
      "[[ 1 82 71 29]\n",
      " [29 71 82  1]]\n",
      "[[ 30 153]\n",
      " [153  30]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 16.39344262295082\n",
      " Precision = [0.01388889 0.26126126]\n",
      " Recall = [0.01204819 0.29      ]\n",
      " F1 score = [0.01290323 0.27488152]\n",
      " AUC score = 15.102409638554217\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 16.39344262295082\n",
      " Precision = [0.26126126 0.01388889]\n",
      " Recall = [0.29       0.01204819]\n",
      " F1 score = [0.27488152 0.01290323]\n",
      " AUC score = 15.102409638554217\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[29 71]\n",
      " [82  1]]\n",
      " Accuracy (acc): 16.393\n",
      " Precision (prc): 1.389\n",
      " Recall (rec): 1.205\n",
      " Sensitivity (sns): 1.205\n",
      " Specificity (spc): 29.0\n",
      " F1 Score (f1s): 1.29\n",
      " ROC AUC (AUC): 0.151\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 5 3 GaussianNB\n",
      "###===> 5 3 GaussianNB ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[ 2 98]\n",
      " [76  7]]\n",
      "[[ 7 76 98  2]\n",
      " [ 2 98 76  7]]\n",
      "[[  9 174]\n",
      " [174   9]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 4.918032786885246\n",
      " Precision = [0.06666667 0.02564103]\n",
      " Recall = [0.08433735 0.02      ]\n",
      " F1 score = [0.07446809 0.02247191]\n",
      " AUC score = 5.216867469879521\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 4.918032786885246\n",
      " Precision = [0.02564103 0.06666667]\n",
      " Recall = [0.02       0.08433735]\n",
      " F1 score = [0.02247191 0.07446809]\n",
      " AUC score = 5.216867469879519\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[ 2 98]\n",
      " [76  7]]\n",
      " Accuracy (acc): 4.918\n",
      " Precision (prc): 6.667\n",
      " Recall (rec): 8.434\n",
      " Sensitivity (sns): 8.434\n",
      " Specificity (spc): 2.0\n",
      " F1 Score (f1s): 7.447\n",
      " ROC AUC (AUC): 0.052\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 5 4 KNeighborsClassifier\n",
      "###===> 5 4 KNeighborsClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1]\n",
      "[[34 66]\n",
      " [81  2]]\n",
      "[[ 2 81 66 34]\n",
      " [34 66 81  2]]\n",
      "[[ 36 147]\n",
      " [147  36]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 19.672131147540984\n",
      " Precision = [0.02941176 0.29565217]\n",
      " Recall = [0.02409639 0.34      ]\n",
      " F1 score = [0.02649007 0.31627907]\n",
      " AUC score = 18.204819277108435\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 19.672131147540984\n",
      " Precision = [0.29565217 0.02941176]\n",
      " Recall = [0.34       0.02409639]\n",
      " F1 score = [0.31627907 0.02649007]\n",
      " AUC score = 18.204819277108435\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[34 66]\n",
      " [81  2]]\n",
      " Accuracy (acc): 19.672\n",
      " Precision (prc): 2.941\n",
      " Recall (rec): 2.41\n",
      " Sensitivity (sns): 2.41\n",
      " Specificity (spc): 34.0\n",
      " F1 Score (f1s): 2.649\n",
      " ROC AUC (AUC): 0.182\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 5 5 DecisionTreeClassifier\n",
      "###===> 5 5 DecisionTreeClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[11 89]\n",
      " [52 31]]\n",
      "[[31 52 89 11]\n",
      " [11 89 52 31]]\n",
      "[[ 42 141]\n",
      " [141  42]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 22.950819672131146\n",
      " Precision = [0.25833333 0.17460317]\n",
      " Recall = [0.37349398 0.11      ]\n",
      " F1 score = [0.30541872 0.13496933]\n",
      " AUC score = 24.174698795180724\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 22.950819672131146\n",
      " Precision = [0.17460317 0.25833333]\n",
      " Recall = [0.11       0.37349398]\n",
      " F1 score = [0.13496933 0.30541872]\n",
      " AUC score = 24.17469879518072\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[11 89]\n",
      " [52 31]]\n",
      " Accuracy (acc): 22.951\n",
      " Precision (prc): 25.833\n",
      " Recall (rec): 37.349\n",
      " Sensitivity (sns): 37.349\n",
      " Specificity (spc): 11.0\n",
      " F1 Score (f1s): 30.542\n",
      " ROC AUC (AUC): 0.242\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GaussianNB was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GaussianNB was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###===> 5 6 RandomForestClassifier\n",
      "###===> 5 6 RandomForestClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[ 3 97]\n",
      " [59 24]]\n",
      "[[24 59 97  3]\n",
      " [ 3 97 59 24]]\n",
      "[[ 27 156]\n",
      " [156  27]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 14.754098360655737\n",
      " Precision = [0.19834711 0.0483871 ]\n",
      " Recall = [0.28915663 0.03      ]\n",
      " F1 score = [0.23529412 0.03703704]\n",
      " AUC score = 15.957831325301209\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 14.754098360655737\n",
      " Precision = [0.0483871  0.19834711]\n",
      " Recall = [0.03       0.28915663]\n",
      " F1 score = [0.03703704 0.23529412]\n",
      " AUC score = 15.957831325301205\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[ 3 97]\n",
      " [59 24]]\n",
      " Accuracy (acc): 14.754\n",
      " Precision (prc): 19.835\n",
      " Recall (rec): 28.916\n",
      " Sensitivity (sns): 28.916\n",
      " Specificity (spc): 3.0\n",
      " F1 Score (f1s): 23.529\n",
      " ROC AUC (AUC): 0.16\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 5 7 GradientBoostingClassifier\n",
      "###===> 5 7 GradientBoostingClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[ 4 96]\n",
      " [43 40]]\n",
      "[[40 43 96  4]\n",
      " [ 4 96 43 40]]\n",
      "[[ 44 139]\n",
      " [139  44]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 24.043715846994534\n",
      " Precision = [0.29411765 0.08510638]\n",
      " Recall = [0.48192771 0.04      ]\n",
      " F1 score = [0.3652968  0.05442177]\n",
      " AUC score = 26.096385542168676\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 24.043715846994534\n",
      " Precision = [0.08510638 0.29411765]\n",
      " Recall = [0.04       0.48192771]\n",
      " F1 score = [0.05442177 0.3652968 ]\n",
      " AUC score = 26.096385542168676\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[ 4 96]\n",
      " [43 40]]\n",
      " Accuracy (acc): 24.044\n",
      " Precision (prc): 29.412\n",
      " Recall (rec): 48.193\n",
      " Sensitivity (sns): 48.193\n",
      " Specificity (spc): 4.0\n",
      " F1 Score (f1s): 36.53\n",
      " ROC AUC (AUC): 0.261\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "###===> 5 8 XGBClassifier\n",
      "###===> 5 8 XGBClassifier ['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']\n",
      "[0 1] [0 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\aliem\\.conda\\envs\\Python37Work\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19 81]\n",
      " [39 44]]\n",
      "[[44 39 81 19]\n",
      " [19 81 39 44]]\n",
      "[[ 63 120]\n",
      " [120  63]]\n",
      "Class-wise info: For multilevel internal scores fo label 0: \n",
      " Accuracy = 34.42622950819672\n",
      " Precision = [0.352      0.32758621]\n",
      " Recall = [0.53012048 0.19      ]\n",
      " F1 score = [0.42307692 0.24050633]\n",
      " AUC score = 36.00602409638554\n",
      " Support = [ 83 100]\n",
      "\n",
      "Class-wise info: For multilevel internal scores fo label 1: \n",
      " Accuracy = 34.42622950819672\n",
      " Precision = [0.32758621 0.352     ]\n",
      " Recall = [0.19       0.53012048]\n",
      " F1 score = [0.24050633 0.42307692]\n",
      " AUC score = 36.006024096385545\n",
      " Support = [100  83]\n",
      "\n",
      "CLASSIFICATION MERICS:\n",
      " _______________________________________________________\n",
      " Confusion Matrix: \n",
      "[[19 81]\n",
      " [39 44]]\n",
      " Accuracy (acc): 34.426\n",
      " Precision (prc): 35.2\n",
      " Recall (rec): 53.012\n",
      " Sensitivity (sns): 53.012\n",
      " Specificity (spc): 19.0\n",
      " F1 Score (f1s): 42.308\n",
      " ROC AUC (AUC): 0.36\n",
      "Score columns: (1, 12) ['method', 'model', 'model_parameters', 'model_scores', 'confusion_matrix', 'accuracy', 'precision', 'recall', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']\n",
      "Saving prediction result to: ./Results//_Classification/ML10008/all_pred_scores.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_No</th>\n",
       "      <th>Model_No</th>\n",
       "      <th>method</th>\n",
       "      <th>model</th>\n",
       "      <th>model_parameters</th>\n",
       "      <th>model_scores</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=LogisticRegressio...</td>\n",
       "      <td>LogisticRegression(C=0.5, max_iter=50)</td>\n",
       "      <td>{'C': 0.5, 'max_iter': 50, 'penalty': 'l2'}</td>\n",
       "      <td>26.67</td>\n",
       "      <td>[[78, 22], [83, 0]]</td>\n",
       "      <td>42.623</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=LogisticRegressio...</td>\n",
       "      <td>LogisticRegression(max_iter=50)</td>\n",
       "      <td>{'C': 1.0, 'max_iter': 50, 'penalty': 'l2'}</td>\n",
       "      <td>46.67</td>\n",
       "      <td>[[55, 45], [83, 0]]</td>\n",
       "      <td>30.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=LogisticRegressio...</td>\n",
       "      <td>LogisticRegression(C=0.5, max_iter=50)</td>\n",
       "      <td>{'C': 0.5, 'max_iter': 50, 'penalty': 'l2'}</td>\n",
       "      <td>13.33</td>\n",
       "      <td>[[81, 19], [83, 0]]</td>\n",
       "      <td>44.262</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=LogisticRegressio...</td>\n",
       "      <td>LogisticRegression(C=0.5, max_iter=50)</td>\n",
       "      <td>{'C': 0.5, 'max_iter': 50, 'penalty': 'l2'}</td>\n",
       "      <td>33.33</td>\n",
       "      <td>[[51, 49], [83, 0]]</td>\n",
       "      <td>27.869</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=LogisticRegressio...</td>\n",
       "      <td>LogisticRegression(C=0.5, max_iter=50)</td>\n",
       "      <td>{'C': 0.5, 'max_iter': 50, 'penalty': 'l2'}</td>\n",
       "      <td>40.00</td>\n",
       "      <td>[[47, 53], [83, 0]]</td>\n",
       "      <td>25.683</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...</td>\n",
       "      <td>SVC(C=0.5, kernel='poly', probability=True)</td>\n",
       "      <td>{'C': 0.5, 'kernel': 'poly', 'probability': True}</td>\n",
       "      <td>53.33</td>\n",
       "      <td>[[41, 59], [83, 0]]</td>\n",
       "      <td>22.404</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...</td>\n",
       "      <td>SVC(C=0.5, kernel='poly', probability=True)</td>\n",
       "      <td>{'C': 0.5, 'kernel': 'poly', 'probability': True}</td>\n",
       "      <td>73.33</td>\n",
       "      <td>[[32, 68], [82, 1]]</td>\n",
       "      <td>18.033</td>\n",
       "      <td>1.449</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.205</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.316</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...</td>\n",
       "      <td>SVC(C=0.5, kernel='poly', probability=True)</td>\n",
       "      <td>{'C': 0.5, 'kernel': 'poly', 'probability': True}</td>\n",
       "      <td>33.33</td>\n",
       "      <td>[[76, 24], [82, 1]]</td>\n",
       "      <td>42.077</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.205</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1.852</td>\n",
       "      <td>0.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...</td>\n",
       "      <td>SVC(kernel='poly', probability=True)</td>\n",
       "      <td>{'C': 1.0, 'kernel': 'poly', 'probability': True}</td>\n",
       "      <td>46.67</td>\n",
       "      <td>[[31, 69], [82, 1]]</td>\n",
       "      <td>17.486</td>\n",
       "      <td>1.429</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.205</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.307</td>\n",
       "      <td>0.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...</td>\n",
       "      <td>SVC(C=0.5, kernel='poly', probability=True)</td>\n",
       "      <td>{'C': 0.5, 'kernel': 'poly', 'probability': True}</td>\n",
       "      <td>58.33</td>\n",
       "      <td>[[29, 71], [82, 1]]</td>\n",
       "      <td>16.393</td>\n",
       "      <td>1.389</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.205</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.290</td>\n",
       "      <td>0.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GaussianNB(), n_j...</td>\n",
       "      <td>GaussianNB(var_smoothing=1.7782794100389227e-07)</td>\n",
       "      <td>{'var_smoothing': 1.7782794100389227e-07}</td>\n",
       "      <td>60.00</td>\n",
       "      <td>[[61, 39], [82, 1]]</td>\n",
       "      <td>33.880</td>\n",
       "      <td>2.500</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.205</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.626</td>\n",
       "      <td>0.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GaussianNB(), n_j...</td>\n",
       "      <td>GaussianNB(var_smoothing=3.1622776601683795e-05)</td>\n",
       "      <td>{'var_smoothing': 3.1622776601683795e-05}</td>\n",
       "      <td>60.00</td>\n",
       "      <td>[[14, 86], [82, 1]]</td>\n",
       "      <td>8.197</td>\n",
       "      <td>1.149</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.205</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.176</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GaussianNB(), n_j...</td>\n",
       "      <td>GaussianNB(var_smoothing=1.7782794100389227e-07)</td>\n",
       "      <td>{'var_smoothing': 1.7782794100389227e-07}</td>\n",
       "      <td>25.00</td>\n",
       "      <td>[[11, 89], [68, 15]]</td>\n",
       "      <td>14.208</td>\n",
       "      <td>14.423</td>\n",
       "      <td>18.072</td>\n",
       "      <td>18.072</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.043</td>\n",
       "      <td>0.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GaussianNB(), n_j...</td>\n",
       "      <td>GaussianNB(var_smoothing=0.005623413251903491)</td>\n",
       "      <td>{'var_smoothing': 0.005623413251903491}</td>\n",
       "      <td>40.00</td>\n",
       "      <td>[[26, 74], [83, 0]]</td>\n",
       "      <td>14.208</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GaussianNB(), n_j...</td>\n",
       "      <td>GaussianNB(var_smoothing=3.1622776601683795e-05)</td>\n",
       "      <td>{'var_smoothing': 3.1622776601683795e-05}</td>\n",
       "      <td>58.33</td>\n",
       "      <td>[[2, 98], [76, 7]]</td>\n",
       "      <td>4.918</td>\n",
       "      <td>6.667</td>\n",
       "      <td>8.434</td>\n",
       "      <td>8.434</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.447</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=KNeighborsClassif...</td>\n",
       "      <td>KNeighborsClassifier(n_neighbors=15)</td>\n",
       "      <td>{'metric': 'minkowski', 'n_neighbors': 15}</td>\n",
       "      <td>40.00</td>\n",
       "      <td>[[100, 0], [83, 0]]</td>\n",
       "      <td>54.645</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=KNeighborsClassif...</td>\n",
       "      <td>KNeighborsClassifier(n_neighbors=15)</td>\n",
       "      <td>{'metric': 'minkowski', 'n_neighbors': 15}</td>\n",
       "      <td>53.33</td>\n",
       "      <td>[[100, 0], [83, 0]]</td>\n",
       "      <td>54.645</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=KNeighborsClassif...</td>\n",
       "      <td>KNeighborsClassifier(n_neighbors=15)</td>\n",
       "      <td>{'metric': 'minkowski', 'n_neighbors': 15}</td>\n",
       "      <td>26.67</td>\n",
       "      <td>[[100, 0], [83, 0]]</td>\n",
       "      <td>54.645</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=KNeighborsClassif...</td>\n",
       "      <td>KNeighborsClassifier(n_neighbors=15)</td>\n",
       "      <td>{'metric': 'minkowski', 'n_neighbors': 15}</td>\n",
       "      <td>40.00</td>\n",
       "      <td>[[50, 50], [83, 0]]</td>\n",
       "      <td>27.322</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=KNeighborsClassif...</td>\n",
       "      <td>KNeighborsClassifier(metric='manhattan', n_nei...</td>\n",
       "      <td>{'metric': 'manhattan', 'n_neighbors': 3}</td>\n",
       "      <td>45.00</td>\n",
       "      <td>[[34, 66], [81, 2]]</td>\n",
       "      <td>19.672</td>\n",
       "      <td>2.941</td>\n",
       "      <td>2.410</td>\n",
       "      <td>2.410</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2.649</td>\n",
       "      <td>0.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=DecisionTreeClass...</td>\n",
       "      <td>DecisionTreeClassifier(criterion='entropy', ma...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5}</td>\n",
       "      <td>78.33</td>\n",
       "      <td>[[20, 80], [31, 52]]</td>\n",
       "      <td>39.344</td>\n",
       "      <td>39.394</td>\n",
       "      <td>62.651</td>\n",
       "      <td>62.651</td>\n",
       "      <td>20.0</td>\n",
       "      <td>48.372</td>\n",
       "      <td>0.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=DecisionTreeClass...</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=2)</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2}</td>\n",
       "      <td>66.67</td>\n",
       "      <td>[[20, 80], [32, 51]]</td>\n",
       "      <td>38.798</td>\n",
       "      <td>38.931</td>\n",
       "      <td>61.446</td>\n",
       "      <td>61.446</td>\n",
       "      <td>20.0</td>\n",
       "      <td>47.664</td>\n",
       "      <td>0.407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=DecisionTreeClass...</td>\n",
       "      <td>DecisionTreeClassifier(criterion='entropy', ma...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 30}</td>\n",
       "      <td>53.33</td>\n",
       "      <td>[[38, 62], [33, 50]]</td>\n",
       "      <td>48.087</td>\n",
       "      <td>44.643</td>\n",
       "      <td>60.241</td>\n",
       "      <td>60.241</td>\n",
       "      <td>38.0</td>\n",
       "      <td>51.282</td>\n",
       "      <td>0.491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=DecisionTreeClass...</td>\n",
       "      <td>DecisionTreeClassifier(criterion='entropy', ma...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3}</td>\n",
       "      <td>55.00</td>\n",
       "      <td>[[23, 77], [42, 41]]</td>\n",
       "      <td>34.973</td>\n",
       "      <td>34.746</td>\n",
       "      <td>49.398</td>\n",
       "      <td>49.398</td>\n",
       "      <td>23.0</td>\n",
       "      <td>40.796</td>\n",
       "      <td>0.362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=DecisionTreeClass...</td>\n",
       "      <td>DecisionTreeClassifier(criterion='entropy', ma...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 25}</td>\n",
       "      <td>81.67</td>\n",
       "      <td>[[11, 89], [52, 31]]</td>\n",
       "      <td>22.951</td>\n",
       "      <td>25.833</td>\n",
       "      <td>37.349</td>\n",
       "      <td>37.349</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.542</td>\n",
       "      <td>0.242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>(DecisionTreeClassifier(criterion='entropy', m...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7, 'n_es...</td>\n",
       "      <td>78.33</td>\n",
       "      <td>[[35, 65], [60, 23]]</td>\n",
       "      <td>31.694</td>\n",
       "      <td>26.136</td>\n",
       "      <td>27.711</td>\n",
       "      <td>27.711</td>\n",
       "      <td>35.0</td>\n",
       "      <td>26.901</td>\n",
       "      <td>0.314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=5, max_featu...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'n_estim...</td>\n",
       "      <td>73.33</td>\n",
       "      <td>[[22, 78], [70, 13]]</td>\n",
       "      <td>19.126</td>\n",
       "      <td>14.286</td>\n",
       "      <td>15.663</td>\n",
       "      <td>15.663</td>\n",
       "      <td>22.0</td>\n",
       "      <td>14.943</td>\n",
       "      <td>0.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=15, max_feat...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 15, 'n_esti...</td>\n",
       "      <td>61.67</td>\n",
       "      <td>[[28, 72], [43, 40]]</td>\n",
       "      <td>37.158</td>\n",
       "      <td>35.714</td>\n",
       "      <td>48.193</td>\n",
       "      <td>48.193</td>\n",
       "      <td>28.0</td>\n",
       "      <td>41.026</td>\n",
       "      <td>0.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=20, max_feat...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 20, 'n_esti...</td>\n",
       "      <td>75.00</td>\n",
       "      <td>[[58, 42], [70, 13]]</td>\n",
       "      <td>38.798</td>\n",
       "      <td>23.636</td>\n",
       "      <td>15.663</td>\n",
       "      <td>15.663</td>\n",
       "      <td>58.0</td>\n",
       "      <td>18.841</td>\n",
       "      <td>0.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=7, max_featu...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 7, 'n_estim...</td>\n",
       "      <td>75.00</td>\n",
       "      <td>[[3, 97], [59, 24]]</td>\n",
       "      <td>14.754</td>\n",
       "      <td>19.835</td>\n",
       "      <td>28.916</td>\n",
       "      <td>28.916</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.529</td>\n",
       "      <td>0.160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GradientBoostingC...</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 2, 'n_est...</td>\n",
       "      <td>73.33</td>\n",
       "      <td>[[21, 79], [34, 49]]</td>\n",
       "      <td>38.251</td>\n",
       "      <td>38.281</td>\n",
       "      <td>59.036</td>\n",
       "      <td>59.036</td>\n",
       "      <td>21.0</td>\n",
       "      <td>46.445</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GradientBoostingC...</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 2, 'n_est...</td>\n",
       "      <td>66.67</td>\n",
       "      <td>[[22, 78], [60, 23]]</td>\n",
       "      <td>24.590</td>\n",
       "      <td>22.772</td>\n",
       "      <td>27.711</td>\n",
       "      <td>27.711</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.000</td>\n",
       "      <td>0.249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GradientBoostingC...</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'learning_rate': 1.0, 'max_depth': 5, 'n_esti...</td>\n",
       "      <td>53.33</td>\n",
       "      <td>[[23, 77], [36, 47]]</td>\n",
       "      <td>38.251</td>\n",
       "      <td>37.903</td>\n",
       "      <td>56.627</td>\n",
       "      <td>56.627</td>\n",
       "      <td>23.0</td>\n",
       "      <td>45.411</td>\n",
       "      <td>0.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GradientBoostingC...</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'learning_rate': 1.0, 'max_depth': 2, 'n_esti...</td>\n",
       "      <td>41.67</td>\n",
       "      <td>[[17, 83], [47, 36]]</td>\n",
       "      <td>28.962</td>\n",
       "      <td>30.252</td>\n",
       "      <td>43.373</td>\n",
       "      <td>43.373</td>\n",
       "      <td>17.0</td>\n",
       "      <td>35.644</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=GradientBoostingC...</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'learning_rate': 0.5, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>81.67</td>\n",
       "      <td>[[4, 96], [43, 40]]</td>\n",
       "      <td>24.044</td>\n",
       "      <td>29.412</td>\n",
       "      <td>48.193</td>\n",
       "      <td>48.193</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.530</td>\n",
       "      <td>0.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=XGB...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'eta': 1.0, 'max_depth': 3}</td>\n",
       "      <td>78.33</td>\n",
       "      <td>[[25, 75], [34, 49]]</td>\n",
       "      <td>40.437</td>\n",
       "      <td>39.516</td>\n",
       "      <td>59.036</td>\n",
       "      <td>59.036</td>\n",
       "      <td>25.0</td>\n",
       "      <td>47.343</td>\n",
       "      <td>0.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=XGB...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'eta': 0.05, 'max_depth': 3}</td>\n",
       "      <td>73.33</td>\n",
       "      <td>[[30, 70], [61, 22]]</td>\n",
       "      <td>28.415</td>\n",
       "      <td>23.913</td>\n",
       "      <td>26.506</td>\n",
       "      <td>26.506</td>\n",
       "      <td>30.0</td>\n",
       "      <td>25.143</td>\n",
       "      <td>0.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=XGB...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'eta': 0.3, 'max_depth': 3}</td>\n",
       "      <td>48.33</td>\n",
       "      <td>[[18, 82], [42, 41]]</td>\n",
       "      <td>32.240</td>\n",
       "      <td>33.333</td>\n",
       "      <td>49.398</td>\n",
       "      <td>49.398</td>\n",
       "      <td>18.0</td>\n",
       "      <td>39.806</td>\n",
       "      <td>0.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=XGB...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'eta': 0.5, 'max_depth': 3}</td>\n",
       "      <td>68.33</td>\n",
       "      <td>[[25, 75], [46, 37]]</td>\n",
       "      <td>33.880</td>\n",
       "      <td>33.036</td>\n",
       "      <td>44.578</td>\n",
       "      <td>44.578</td>\n",
       "      <td>25.0</td>\n",
       "      <td>37.949</td>\n",
       "      <td>0.348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=XGB...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'eta': 0.3, 'max_depth': 2}</td>\n",
       "      <td>75.00</td>\n",
       "      <td>[[19, 81], [39, 44]]</td>\n",
       "      <td>34.426</td>\n",
       "      <td>35.200</td>\n",
       "      <td>53.012</td>\n",
       "      <td>53.012</td>\n",
       "      <td>19.0</td>\n",
       "      <td>42.308</td>\n",
       "      <td>0.360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Test_No  Model_No                                             method  \\\n",
       "0        1         1  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
       "0        2         1  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
       "0        3         1  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
       "0        4         1  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
       "0        5         1  GridSearchCV(cv=5, estimator=LogisticRegressio...   \n",
       "0        1         2  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
       "0        2         2  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
       "0        3         2  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
       "0        4         2  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
       "0        5         2  GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,...   \n",
       "0        1         3  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
       "0        2         3  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
       "0        3         3  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
       "0        4         3  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
       "0        5         3  GridSearchCV(cv=5, estimator=GaussianNB(), n_j...   \n",
       "0        1         4  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
       "0        2         4  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
       "0        3         4  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
       "0        4         4  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
       "0        5         4  GridSearchCV(cv=5, estimator=KNeighborsClassif...   \n",
       "0        1         5  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
       "0        2         5  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
       "0        3         5  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
       "0        4         5  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
       "0        5         5  GridSearchCV(cv=5, estimator=DecisionTreeClass...   \n",
       "0        1         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "0        2         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "0        3         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "0        4         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "0        5         6  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "0        1         7  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
       "0        2         7  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
       "0        3         7  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
       "0        4         7  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
       "0        5         7  GridSearchCV(cv=5, estimator=GradientBoostingC...   \n",
       "0        1         8  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
       "0        2         8  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
       "0        3         8  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
       "0        4         8  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
       "0        5         8  GridSearchCV(cv=5,\\n             estimator=XGB...   \n",
       "\n",
       "                                               model  \\\n",
       "0             LogisticRegression(C=0.5, max_iter=50)   \n",
       "0                    LogisticRegression(max_iter=50)   \n",
       "0             LogisticRegression(C=0.5, max_iter=50)   \n",
       "0             LogisticRegression(C=0.5, max_iter=50)   \n",
       "0             LogisticRegression(C=0.5, max_iter=50)   \n",
       "0        SVC(C=0.5, kernel='poly', probability=True)   \n",
       "0        SVC(C=0.5, kernel='poly', probability=True)   \n",
       "0        SVC(C=0.5, kernel='poly', probability=True)   \n",
       "0               SVC(kernel='poly', probability=True)   \n",
       "0        SVC(C=0.5, kernel='poly', probability=True)   \n",
       "0   GaussianNB(var_smoothing=1.7782794100389227e-07)   \n",
       "0   GaussianNB(var_smoothing=3.1622776601683795e-05)   \n",
       "0   GaussianNB(var_smoothing=1.7782794100389227e-07)   \n",
       "0     GaussianNB(var_smoothing=0.005623413251903491)   \n",
       "0   GaussianNB(var_smoothing=3.1622776601683795e-05)   \n",
       "0               KNeighborsClassifier(n_neighbors=15)   \n",
       "0               KNeighborsClassifier(n_neighbors=15)   \n",
       "0               KNeighborsClassifier(n_neighbors=15)   \n",
       "0               KNeighborsClassifier(n_neighbors=15)   \n",
       "0  KNeighborsClassifier(metric='manhattan', n_nei...   \n",
       "0  DecisionTreeClassifier(criterion='entropy', ma...   \n",
       "0                DecisionTreeClassifier(max_depth=2)   \n",
       "0  DecisionTreeClassifier(criterion='entropy', ma...   \n",
       "0  DecisionTreeClassifier(criterion='entropy', ma...   \n",
       "0  DecisionTreeClassifier(criterion='entropy', ma...   \n",
       "0  (DecisionTreeClassifier(criterion='entropy', m...   \n",
       "0  (DecisionTreeClassifier(max_depth=5, max_featu...   \n",
       "0  (DecisionTreeClassifier(max_depth=15, max_feat...   \n",
       "0  (DecisionTreeClassifier(max_depth=20, max_feat...   \n",
       "0  (DecisionTreeClassifier(max_depth=7, max_featu...   \n",
       "0  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
       "0  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
       "0  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
       "0  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
       "0  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
       "0  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "0  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "0  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "0  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "0  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "\n",
       "                                    model_parameters  model_scores  \\\n",
       "0        {'C': 0.5, 'max_iter': 50, 'penalty': 'l2'}         26.67   \n",
       "0        {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'}         46.67   \n",
       "0        {'C': 0.5, 'max_iter': 50, 'penalty': 'l2'}         13.33   \n",
       "0        {'C': 0.5, 'max_iter': 50, 'penalty': 'l2'}         33.33   \n",
       "0        {'C': 0.5, 'max_iter': 50, 'penalty': 'l2'}         40.00   \n",
       "0  {'C': 0.5, 'kernel': 'poly', 'probability': True}         53.33   \n",
       "0  {'C': 0.5, 'kernel': 'poly', 'probability': True}         73.33   \n",
       "0  {'C': 0.5, 'kernel': 'poly', 'probability': True}         33.33   \n",
       "0  {'C': 1.0, 'kernel': 'poly', 'probability': True}         46.67   \n",
       "0  {'C': 0.5, 'kernel': 'poly', 'probability': True}         58.33   \n",
       "0          {'var_smoothing': 1.7782794100389227e-07}         60.00   \n",
       "0          {'var_smoothing': 3.1622776601683795e-05}         60.00   \n",
       "0          {'var_smoothing': 1.7782794100389227e-07}         25.00   \n",
       "0            {'var_smoothing': 0.005623413251903491}         40.00   \n",
       "0          {'var_smoothing': 3.1622776601683795e-05}         58.33   \n",
       "0         {'metric': 'minkowski', 'n_neighbors': 15}         40.00   \n",
       "0         {'metric': 'minkowski', 'n_neighbors': 15}         53.33   \n",
       "0         {'metric': 'minkowski', 'n_neighbors': 15}         26.67   \n",
       "0         {'metric': 'minkowski', 'n_neighbors': 15}         40.00   \n",
       "0          {'metric': 'manhattan', 'n_neighbors': 3}         45.00   \n",
       "0           {'criterion': 'entropy', 'max_depth': 5}         78.33   \n",
       "0              {'criterion': 'gini', 'max_depth': 2}         66.67   \n",
       "0          {'criterion': 'entropy', 'max_depth': 30}         53.33   \n",
       "0           {'criterion': 'entropy', 'max_depth': 3}         55.00   \n",
       "0          {'criterion': 'entropy', 'max_depth': 25}         81.67   \n",
       "0  {'criterion': 'entropy', 'max_depth': 7, 'n_es...         78.33   \n",
       "0  {'criterion': 'gini', 'max_depth': 5, 'n_estim...         73.33   \n",
       "0  {'criterion': 'gini', 'max_depth': 15, 'n_esti...         61.67   \n",
       "0  {'criterion': 'gini', 'max_depth': 20, 'n_esti...         75.00   \n",
       "0  {'criterion': 'gini', 'max_depth': 7, 'n_estim...         75.00   \n",
       "0  {'learning_rate': 0.05, 'max_depth': 2, 'n_est...         73.33   \n",
       "0  {'learning_rate': 0.01, 'max_depth': 2, 'n_est...         66.67   \n",
       "0  {'learning_rate': 1.0, 'max_depth': 5, 'n_esti...         53.33   \n",
       "0  {'learning_rate': 1.0, 'max_depth': 2, 'n_esti...         41.67   \n",
       "0  {'learning_rate': 0.5, 'max_depth': 15, 'n_est...         81.67   \n",
       "0                       {'eta': 1.0, 'max_depth': 3}         78.33   \n",
       "0                      {'eta': 0.05, 'max_depth': 3}         73.33   \n",
       "0                       {'eta': 0.3, 'max_depth': 3}         48.33   \n",
       "0                       {'eta': 0.5, 'max_depth': 3}         68.33   \n",
       "0                       {'eta': 0.3, 'max_depth': 2}         75.00   \n",
       "\n",
       "       confusion_matrix  accuracy  precision  recall  sensitivity  \\\n",
       "0   [[78, 22], [83, 0]]    42.623      0.000   0.000        0.000   \n",
       "0   [[55, 45], [83, 0]]    30.055      0.000   0.000        0.000   \n",
       "0   [[81, 19], [83, 0]]    44.262      0.000   0.000        0.000   \n",
       "0   [[51, 49], [83, 0]]    27.869      0.000   0.000        0.000   \n",
       "0   [[47, 53], [83, 0]]    25.683      0.000   0.000        0.000   \n",
       "0   [[41, 59], [83, 0]]    22.404      0.000   0.000        0.000   \n",
       "0   [[32, 68], [82, 1]]    18.033      1.449   1.205        1.205   \n",
       "0   [[76, 24], [82, 1]]    42.077      4.000   1.205        1.205   \n",
       "0   [[31, 69], [82, 1]]    17.486      1.429   1.205        1.205   \n",
       "0   [[29, 71], [82, 1]]    16.393      1.389   1.205        1.205   \n",
       "0   [[61, 39], [82, 1]]    33.880      2.500   1.205        1.205   \n",
       "0   [[14, 86], [82, 1]]     8.197      1.149   1.205        1.205   \n",
       "0  [[11, 89], [68, 15]]    14.208     14.423  18.072       18.072   \n",
       "0   [[26, 74], [83, 0]]    14.208      0.000   0.000        0.000   \n",
       "0    [[2, 98], [76, 7]]     4.918      6.667   8.434        8.434   \n",
       "0   [[100, 0], [83, 0]]    54.645      0.000   0.000        0.000   \n",
       "0   [[100, 0], [83, 0]]    54.645      0.000   0.000        0.000   \n",
       "0   [[100, 0], [83, 0]]    54.645      0.000   0.000        0.000   \n",
       "0   [[50, 50], [83, 0]]    27.322      0.000   0.000        0.000   \n",
       "0   [[34, 66], [81, 2]]    19.672      2.941   2.410        2.410   \n",
       "0  [[20, 80], [31, 52]]    39.344     39.394  62.651       62.651   \n",
       "0  [[20, 80], [32, 51]]    38.798     38.931  61.446       61.446   \n",
       "0  [[38, 62], [33, 50]]    48.087     44.643  60.241       60.241   \n",
       "0  [[23, 77], [42, 41]]    34.973     34.746  49.398       49.398   \n",
       "0  [[11, 89], [52, 31]]    22.951     25.833  37.349       37.349   \n",
       "0  [[35, 65], [60, 23]]    31.694     26.136  27.711       27.711   \n",
       "0  [[22, 78], [70, 13]]    19.126     14.286  15.663       15.663   \n",
       "0  [[28, 72], [43, 40]]    37.158     35.714  48.193       48.193   \n",
       "0  [[58, 42], [70, 13]]    38.798     23.636  15.663       15.663   \n",
       "0   [[3, 97], [59, 24]]    14.754     19.835  28.916       28.916   \n",
       "0  [[21, 79], [34, 49]]    38.251     38.281  59.036       59.036   \n",
       "0  [[22, 78], [60, 23]]    24.590     22.772  27.711       27.711   \n",
       "0  [[23, 77], [36, 47]]    38.251     37.903  56.627       56.627   \n",
       "0  [[17, 83], [47, 36]]    28.962     30.252  43.373       43.373   \n",
       "0   [[4, 96], [43, 40]]    24.044     29.412  48.193       48.193   \n",
       "0  [[25, 75], [34, 49]]    40.437     39.516  59.036       59.036   \n",
       "0  [[30, 70], [61, 22]]    28.415     23.913  26.506       26.506   \n",
       "0  [[18, 82], [42, 41]]    32.240     33.333  49.398       49.398   \n",
       "0  [[25, 75], [46, 37]]    33.880     33.036  44.578       44.578   \n",
       "0  [[19, 81], [39, 44]]    34.426     35.200  53.012       53.012   \n",
       "\n",
       "   specificity  f1_score  roc_auc  \n",
       "0         78.0     0.000    0.390  \n",
       "0         55.0     0.000    0.275  \n",
       "0         81.0     0.000    0.405  \n",
       "0         51.0     0.000    0.255  \n",
       "0         47.0     0.000    0.235  \n",
       "0         41.0     0.000    0.205  \n",
       "0         32.0     1.316    0.166  \n",
       "0         76.0     1.852    0.386  \n",
       "0         31.0     1.307    0.161  \n",
       "0         29.0     1.290    0.151  \n",
       "0         61.0     1.626    0.311  \n",
       "0         14.0     1.176    0.076  \n",
       "0         11.0    16.043    0.145  \n",
       "0         26.0     0.000    0.130  \n",
       "0          2.0     7.447    0.052  \n",
       "0        100.0     0.000    0.500  \n",
       "0        100.0     0.000    0.500  \n",
       "0        100.0     0.000    0.500  \n",
       "0         50.0     0.000    0.250  \n",
       "0         34.0     2.649    0.182  \n",
       "0         20.0    48.372    0.413  \n",
       "0         20.0    47.664    0.407  \n",
       "0         38.0    51.282    0.491  \n",
       "0         23.0    40.796    0.362  \n",
       "0         11.0    30.542    0.242  \n",
       "0         35.0    26.901    0.314  \n",
       "0         22.0    14.943    0.188  \n",
       "0         28.0    41.026    0.381  \n",
       "0         58.0    18.841    0.368  \n",
       "0          3.0    23.529    0.160  \n",
       "0         21.0    46.445    0.400  \n",
       "0         22.0    25.000    0.249  \n",
       "0         23.0    45.411    0.398  \n",
       "0         17.0    35.644    0.302  \n",
       "0          4.0    36.530    0.261  \n",
       "0         25.0    47.343    0.420  \n",
       "0         30.0    25.143    0.283  \n",
       "0         18.0    39.806    0.337  \n",
       "0         25.0    37.949    0.348  \n",
       "0         19.0    42.308    0.360  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_obj.prediction(model_list, all_best_tr_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd01235d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bdff630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W->S1', 'W->S2', 'W->REM', 'S1->W', 'S1->S1', 'S1->S2', 'S1->REM', 'S2->S3', 'S3->S2', 'S3->S3', 'S3->S4', 'S4->S3', 'S4->S4', 'REM->S1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, <ML_Classifiers.LogReg: 'logistic_regression'>),\n",
       " (2, <ML_Classifiers.SVC: 'support_vector_classifier'>),\n",
       " (3, <ML_Classifiers.NB: 'naive_bayes'>),\n",
       " (4, <ML_Classifiers.kNN: 'k_nearest_neighbors'>),\n",
       " (5, <ML_Classifiers.DT: 'decision_tree'>),\n",
       " (6, <ML_Classifiers.RF: 'random_forest'>),\n",
       " (7, <ML_Classifiers.GBoost: 'gradient_boosting'>),\n",
       " (8, <ML_Classifiers.XGBoost: 'xtreme_gradient_boosting'>)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cols = processed_dataset.columns.values.tolist() \n",
    "feat_names = all_cols[all_cols.index(class_name)+1:] \n",
    "all_feats = processed_dataset[feat_names] \n",
    "print(all_feats.columns.values.tolist()) \n",
    "[(i+1, m) for i, m in enumerate(model_list)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8bd05ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression == LogReg || logistic_regression\n",
      "SVC == SVC || support_vector_classifier\n",
      "GaussianNB == NB || naive_bayes\n",
      "KNeighborsClassifier == kNN || k_nearest_neighbors\n",
      "DecisionTreeClassifier == DT || decision_tree\n",
      "RandomForestClassifier == RF || random_forest\n",
      "GradientBoostingClassifier == GBoost || gradient_boosting\n",
      "XGBClassifier == XGBoost || xtreme_gradient_boosting\n",
      "['LogisticRegression', 'SVC', 'GaussianNB', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier'] ['LogReg', 'SVC', 'NB', 'kNN', 'DT', 'RF', 'GBoost', 'XGBoost']\n",
      "XGBClassifier == {'_value_': 'xtreme_gradient_boosting', '_name_': 'XGBoost', '__objclass__': <enum 'ML_Classifiers'>}\n"
     ]
    }
   ],
   "source": [
    "actual_name_list = [] \n",
    "name_list = [] \n",
    "for i in range(len(model_list)): \n",
    "    print(all_best_tr_model[1][i+1].best_estimator_.__class__.__name__, '==',  model_list[i]._name_, '||',  model_list[i]._value_) \n",
    "    actual_name_list.append(all_best_tr_model[1][i+1].best_estimator_.__class__.__name__) \n",
    "    name_list.append(model_list[i]._name_) \n",
    "    \n",
    "print(actual_name_list, name_list) \n",
    "print(all_best_tr_model[1][i+1].best_estimator_.__class__.__name__, '==',  model_list[i].__dict__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e97481f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {1: GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
       "               param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
       "                           'max_iter': [50, 100, 130, 150, 170, 200],\n",
       "                           'penalty': ['l2', 'elasticnet']},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  2: GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
       "               param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
       "                           'kernel': ['linear', 'rbf', 'poly'],\n",
       "                           'probability': [True]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  3: GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
       "               param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
       "                                             3.1622776601683795e-05,\n",
       "                                             1.7782794100389227e-07, 1e-09]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  4: GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
       "               param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
       "                           'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  5: GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
       "               param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                           'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  6: GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
       "               param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                           'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
       "                           'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  7: GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
       "               param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
       "                           'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
       "                           'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  8: GridSearchCV(cv=5,\n",
       "               estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                       callbacks=None, colsample_bylevel=None,\n",
       "                                       colsample_bynode=None,\n",
       "                                       colsample_bytree=None,\n",
       "                                       early_stopping_rounds=None,\n",
       "                                       enable_categorical=False, eval_metric=None,\n",
       "                                       gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                       importance_type=None,\n",
       "                                       interaction_constraints=None,\n",
       "                                       learning_rate=None, max_bin=None,\n",
       "                                       max_ca...\n",
       "                                       max_leaves=None, min_child_weight=None,\n",
       "                                       missing=nan, monotone_constraints=None,\n",
       "                                       n_estimators=100, n_jobs=None,\n",
       "                                       num_parallel_tree=None, predictor=None,\n",
       "                                       random_state=None, reg_alpha=None,\n",
       "                                       reg_lambda=None, ...),\n",
       "               n_jobs=50,\n",
       "               param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
       "                           'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2)},\n",
       " 2: {1: GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
       "               param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
       "                           'max_iter': [50, 100, 130, 150, 170, 200],\n",
       "                           'penalty': ['l2', 'elasticnet']},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  2: GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
       "               param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
       "                           'kernel': ['linear', 'rbf', 'poly'],\n",
       "                           'probability': [True]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  3: GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
       "               param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
       "                                             3.1622776601683795e-05,\n",
       "                                             1.7782794100389227e-07, 1e-09]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  4: GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
       "               param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
       "                           'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  5: GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
       "               param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                           'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  6: GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
       "               param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                           'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
       "                           'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  7: GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
       "               param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
       "                           'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
       "                           'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  8: GridSearchCV(cv=5,\n",
       "               estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                       callbacks=None, colsample_bylevel=None,\n",
       "                                       colsample_bynode=None,\n",
       "                                       colsample_bytree=None,\n",
       "                                       early_stopping_rounds=None,\n",
       "                                       enable_categorical=False, eval_metric=None,\n",
       "                                       gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                       importance_type=None,\n",
       "                                       interaction_constraints=None,\n",
       "                                       learning_rate=None, max_bin=None,\n",
       "                                       max_ca...\n",
       "                                       max_leaves=None, min_child_weight=None,\n",
       "                                       missing=nan, monotone_constraints=None,\n",
       "                                       n_estimators=100, n_jobs=None,\n",
       "                                       num_parallel_tree=None, predictor=None,\n",
       "                                       random_state=None, reg_alpha=None,\n",
       "                                       reg_lambda=None, ...),\n",
       "               n_jobs=50,\n",
       "               param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
       "                           'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2)},\n",
       " 3: {1: GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
       "               param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
       "                           'max_iter': [50, 100, 130, 150, 170, 200],\n",
       "                           'penalty': ['l2', 'elasticnet']},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  2: GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
       "               param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
       "                           'kernel': ['linear', 'rbf', 'poly'],\n",
       "                           'probability': [True]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  3: GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
       "               param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
       "                                             3.1622776601683795e-05,\n",
       "                                             1.7782794100389227e-07, 1e-09]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  4: GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
       "               param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
       "                           'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  5: GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
       "               param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                           'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  6: GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
       "               param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                           'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
       "                           'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  7: GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
       "               param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
       "                           'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
       "                           'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  8: GridSearchCV(cv=5,\n",
       "               estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                       callbacks=None, colsample_bylevel=None,\n",
       "                                       colsample_bynode=None,\n",
       "                                       colsample_bytree=None,\n",
       "                                       early_stopping_rounds=None,\n",
       "                                       enable_categorical=False, eval_metric=None,\n",
       "                                       gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                       importance_type=None,\n",
       "                                       interaction_constraints=None,\n",
       "                                       learning_rate=None, max_bin=None,\n",
       "                                       max_ca...\n",
       "                                       max_leaves=None, min_child_weight=None,\n",
       "                                       missing=nan, monotone_constraints=None,\n",
       "                                       n_estimators=100, n_jobs=None,\n",
       "                                       num_parallel_tree=None, predictor=None,\n",
       "                                       random_state=None, reg_alpha=None,\n",
       "                                       reg_lambda=None, ...),\n",
       "               n_jobs=50,\n",
       "               param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
       "                           'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2)},\n",
       " 4: {1: GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
       "               param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
       "                           'max_iter': [50, 100, 130, 150, 170, 200],\n",
       "                           'penalty': ['l2', 'elasticnet']},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  2: GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
       "               param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
       "                           'kernel': ['linear', 'rbf', 'poly'],\n",
       "                           'probability': [True]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  3: GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
       "               param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
       "                                             3.1622776601683795e-05,\n",
       "                                             1.7782794100389227e-07, 1e-09]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  4: GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
       "               param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
       "                           'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  5: GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
       "               param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                           'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  6: GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
       "               param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                           'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
       "                           'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  7: GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
       "               param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
       "                           'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
       "                           'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  8: GridSearchCV(cv=5,\n",
       "               estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                       callbacks=None, colsample_bylevel=None,\n",
       "                                       colsample_bynode=None,\n",
       "                                       colsample_bytree=None,\n",
       "                                       early_stopping_rounds=None,\n",
       "                                       enable_categorical=False, eval_metric=None,\n",
       "                                       gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                       importance_type=None,\n",
       "                                       interaction_constraints=None,\n",
       "                                       learning_rate=None, max_bin=None,\n",
       "                                       max_ca...\n",
       "                                       max_leaves=None, min_child_weight=None,\n",
       "                                       missing=nan, monotone_constraints=None,\n",
       "                                       n_estimators=100, n_jobs=None,\n",
       "                                       num_parallel_tree=None, predictor=None,\n",
       "                                       random_state=None, reg_alpha=None,\n",
       "                                       reg_lambda=None, ...),\n",
       "               n_jobs=50,\n",
       "               param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
       "                           'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2)},\n",
       " 5: {1: GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=50,\n",
       "               param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
       "                           'max_iter': [50, 100, 130, 150, 170, 200],\n",
       "                           'penalty': ['l2', 'elasticnet']},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  2: GridSearchCV(cv=5, estimator=SVC(), n_jobs=50,\n",
       "               param_grid={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
       "                           'kernel': ['linear', 'rbf', 'poly'],\n",
       "                           'probability': [True]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  3: GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=50,\n",
       "               param_grid={'var_smoothing': [1.0, 0.005623413251903491,\n",
       "                                             3.1622776601683795e-05,\n",
       "                                             1.7782794100389227e-07, 1e-09]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  4: GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=50,\n",
       "               param_grid={'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
       "                           'n_neighbors': [2, 3, 5, 10, 15, 25, 35]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  5: GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=50,\n",
       "               param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                           'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  6: GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=50,\n",
       "               param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                           'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
       "                           'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  7: GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=50,\n",
       "               param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
       "                           'max_depth': [2, 3, 5, 7, 10, 15, 20, 25, 30],\n",
       "                           'n_estimators': [3, 5, 10, 15, 21, 30, 50, 75, 100]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2),\n",
       "  8: GridSearchCV(cv=5,\n",
       "               estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                       callbacks=None, colsample_bylevel=None,\n",
       "                                       colsample_bynode=None,\n",
       "                                       colsample_bytree=None,\n",
       "                                       early_stopping_rounds=None,\n",
       "                                       enable_categorical=False, eval_metric=None,\n",
       "                                       gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                       importance_type=None,\n",
       "                                       interaction_constraints=None,\n",
       "                                       learning_rate=None, max_bin=None,\n",
       "                                       max_ca...\n",
       "                                       max_leaves=None, min_child_weight=None,\n",
       "                                       missing=nan, monotone_constraints=None,\n",
       "                                       n_estimators=100, n_jobs=None,\n",
       "                                       num_parallel_tree=None, predictor=None,\n",
       "                                       random_state=None, reg_alpha=None,\n",
       "                                       reg_lambda=None, ...),\n",
       "               n_jobs=50,\n",
       "               param_grid={'eta': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0],\n",
       "                           'max_depth': [2, 3, 6, 10, 15, 20, 25, 30]},\n",
       "               refit='recall', return_train_score=True, scoring=['recall', 'f1'],\n",
       "               verbose=2)}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_Classifiers.get_actual_name(str(ML_Classifiers.LogReg._name_)) \n",
    "# ML_Classifiers.get_short_form(str(ML_Classifiers.LogReg._value_)) \n",
    "\n",
    "all_best_tr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3274f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57d1adc2",
   "metadata": {},
   "source": [
    "### Test saved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c14f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_name = 'ML3002' \n",
    "# result_save_path = f'./Results/_Classification/{exp_name}/'\n",
    "# result_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b193c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if logger:\n",
    "#     stop_logger(logger) \n",
    "# util, logger = start_logger(result_save_path, exp_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6149537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_save_path2 = result_save_path \n",
    "result_save_path2 = f'./Results/_Classification/{exp_name}/'\n",
    "result_save_path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0febb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0737a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if logger:\n",
    "    stop_logger(logger) \n",
    "util, logger = start_logger('./Results/', '00')\n",
    "\n",
    "classifier_obj2 = HumachLab_ML_CLassifiers(logger=logger, directory=result_save_path2, dataset=pd.DataFrame(columns=[class_name]), class_name=class_name, label_map={}, metadata_column=metadata_column, split_column=split_column, random_state_value=0, split_balance_pattern=[], check_result=True) \n",
    "\n",
    "classifier_obj2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tr_model2, tr_model2, tr_model_scores_df2, tr_target_and_prediction_df2, ts_model2, ts_model_scores_df2, ts_target_and_prediction_df2, ts_fold_info_df2, exp_info_df2  = classifier_obj2.load_results(result_save_path2)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_tr_model, tr_model, tr_model_scores_df, tr_target_and_prediction_df, ts_model, ts_model_scores_df, ts_target_and_prediction_df, ts_fold_info_df, exp_info_df = best_tr_model2, tr_model2, tr_model_scores_df2, tr_target_and_prediction_df2, ts_model2, ts_model_scores_df2, ts_target_and_prediction_df2, ts_fold_info_df2, exp_info_df2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c4e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_fold_info_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5895766",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = ts_fold_info_df2[ ts_fold_info_df2['Model_No']==1 ] ['Selected_Features'] \n",
    "tt\n",
    "\n",
    "for t in tt:\n",
    "    print(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e72df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model_scores_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ab555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd3f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb2807d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "079bb659",
   "metadata": {},
   "source": [
    "##### Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "395b9b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_model_scores_df = ts_model_scores_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea5a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_map = {'n': 0, 'ins': 1, 'narco': 2, 'nfle': 3, 'plm': 4, 'rbd': 5}\n",
    "\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47606a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.array(list(label_map.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map\n",
    "\n",
    "label_map = {'n': 0, 'dis': 1}\n",
    "\n",
    "label_map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4957eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model_scores_df[(ts_model_scores_df['Model_No']==6)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0950e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conf_mat = ts_model_scores_df[(ts_model_scores_df['Model_No']==6)]['confusion_matrix'].values \n",
    "# all_conf_mat = [eval(cf) for cf in all_conf_mat] \n",
    "all_conf_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_conf_mat[0]), len(all_conf_mat[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_conf_mat = np.zeros( ( len(all_conf_mat[0]), len(all_conf_mat[0][0]) ) ) \n",
    "for i, arr in enumerate(all_conf_mat):\n",
    "    arr = np.array(arr) \n",
    "    if i==0:\n",
    "        final_conf_mat = np.zeros( (arr.shape[0], arr.shape[1]) ) \n",
    "#     print(type(arr), arr)\n",
    "    final_conf_mat += arr\n",
    "    \n",
    "# tot_counts = np.array( np.sum(final_conf_mat, axis=1) ) \n",
    "tot_counts = np.sum(final_conf_mat, axis=1).reshape(1,final_conf_mat.shape[1]) \n",
    "print(type(tot_counts), tot_counts) \n",
    "final_conf_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63fe856",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_conf = np.concatenate( (tot_counts, final_conf_mat), axis=0).astype(int) \n",
    "tmp_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dffc060",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_conf_df = pd.DataFrame(tmp_conf, columns=list(label_map.keys()), index=['Total']+list(label_map.keys())) \n",
    "tmp_conf_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55f6f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_save_path, f'{result_save_path}combined_conf_mat.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_conf_df.to_csv(f'{result_save_path}combined_conf_mat.csv')  \n",
    "# tmp_conf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deac131",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_conf_df = pd.read_csv(f'{result_save_path}combined_conf_mat.csv', index_col='Unnamed: 0') \n",
    "tmp_conf_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3bf37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd3310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1eb1bfe",
   "metadata": {},
   "source": [
    "# ML Classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "93d6b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data preprocessor class \n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class DataPreprocessor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def calculate_p_and_auc_for_feature(self, feat_data, label_data, binary_class=True): \n",
    "        # Extract the independent variable and dependent variable as dataframe and series \n",
    "        X = feat_data.copy()  # Replace 'independent_variable' with your column name\n",
    "        y = label_data.copy()  # Replace 'dependent_variable' with your column name\n",
    "        # print(X, y) \n",
    "        #print(\"111 Binary classification?\", binary_class)\n",
    "\n",
    "        # Perform a one-way ANOVA and calculate the p-value\n",
    "        p_value = 1.0\n",
    "        if binary_class:\n",
    "            _, p_value = ttest_ind(X[y==0], X[y==1])  # Assuming binary classification \n",
    "            #print(\"222 Binary classification?\", binary_class)\n",
    "        else: \n",
    "            groups = [X[y == label] for label in np.unique(y)] # For multiclass classification \n",
    "            _, p_value = f_oneway(*groups)\n",
    "            #print(\"222 Not binary classification?\", binary_class)\n",
    "        p_value = p_value[0] \n",
    "\n",
    "        # Display the p-value\n",
    "        #print(\"P-value:\", p_value)\n",
    "\n",
    "        # Encode the target variable - For multiclass \n",
    "        if not binary_class: \n",
    "            label_encoder = LabelEncoder()\n",
    "            y = label_encoder.fit_transform(y)\n",
    "            #print(\"333 Not binary classification?\", binary_class)\n",
    "\n",
    "        # Fit a logistic regression model and calculate the AUC\n",
    "        model = None \n",
    "        #if binary_class:\n",
    "        #    model = LogisticRegression()\n",
    "        #else:\n",
    "        #    model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "        # model = LogisticRegression()\n",
    "        # model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "        model = SVC(probability=True)\n",
    "        # model = SVC(C=1.0, random_state=1, kernel='linear', probability=True)\n",
    "        model.fit(X, y)\n",
    "        y_pred_proba = model.predict_proba(X)\n",
    "        if binary_class: \n",
    "            y_pred_proba = y_pred_proba[:, 1]\n",
    "            #print(\"444 Binary classification?\", binary_class)\n",
    "\n",
    "        # print(y_pred_proba) \n",
    "        auc = 0.0 \n",
    "        if binary_class:\n",
    "            auc = roc_auc_score(y, y_pred_proba)\n",
    "            #print(\"555 Binary classification?\", binary_class)\n",
    "        else:\n",
    "            auc = roc_auc_score(y, y_pred_proba, multi_class='ovr')\n",
    "            #print(\"555 Not binary classification?\", binary_class)\n",
    "\n",
    "        # Display the AUC\n",
    "        #print(\"AUC:\", auc)\n",
    "        return p_value, auc \n",
    "    \n",
    "    def calculate_p_and_auc_for_dataset(self, all_feats_df, label_df, binary_class=True): \n",
    "        feat_cols = all_feats_df.columns.values.tolist() \n",
    "\n",
    "        all_p_list = [] \n",
    "        all_auc_list = [] \n",
    "        for ft in feat_cols:\n",
    "            feat_data = all_feats_df[[ft]].copy() \n",
    "            label_data = label_df.copy() \n",
    "            # print(\"HHHHHH\", feat_data.shape, type(feat_data), label_data.shape, type(label_data), binary_class)\n",
    "            p, auc = self.calculate_p_and_auc_for_feature(feat_data, label_data, binary_class=binary_class) \n",
    "            all_p_list.append(p) \n",
    "            all_auc_list.append(auc) \n",
    "\n",
    "        all_p_and_auc_df = pd.DataFrame( {\"Features\": feat_cols, f\"P_Value_{'bin' if binary_class else 'multi'}\": all_p_list, f\"AUC_{'bin' if binary_class else 'multi'}\": all_auc_list} )    \n",
    "        return all_p_and_auc_df \n",
    "    \n",
    "    def get_selected_feature_list_based_on_PAUC(self, tmp_df, p_threshold=0.05, auc_threshold=0.5, sort=False, binary_class=True): \n",
    "        cols = tmp_df['Features'].values.tolist() \n",
    "        if p_threshold:\n",
    "            tmp_df = tmp_df[(tmp_df[f\"P_Value_{'bin' if binary_class else 'multi'}\"]<p_threshold)]\n",
    "        if auc_threshold:\n",
    "            tmp_df = tmp_df[(tmp_df[f\"AUC_{'bin' if binary_class else 'multi'}\"]>auc_threshold)]\n",
    "        if sort:\n",
    "            tmp_df = tmp_df.sort_values([f\"P_Value_{'bin' if binary_class else 'multi'}\", f\"AUC_{'bin' if binary_class else 'multi'}\"], ascending = [True, False])\n",
    "        selected_features = tmp_df['Features'].values.tolist() \n",
    "        return selected_features\n",
    "    \n",
    "    def select_pandauc_based_features(self, all_feats_df, label_df, binary_class=True, p_threshold=None, auc_threshold=None, sort=False): \n",
    "        tmp_df = self.calculate_p_and_auc_for_dataset(all_feats_df, label_df, binary_class=binary_class)\n",
    "        selected_features = self.get_selected_feature_list_based_on_PAUC(tmp_df, p_threshold=p_threshold, auc_threshold=auc_threshold, sort=sort, binary_class=binary_class)        \n",
    "        return selected_features, tmp_df \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f61da8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom splitter class \n",
    "import math\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "# class MyCustomSplitter(BaseCrossValidator):\n",
    "class MyCustomSplitter():\n",
    "#     def __init__(self, n_splits):\n",
    "#         self.n_splits = n_splits\n",
    "        \n",
    "#     def set_criteria(self, splt_cri, groups=[['n'], ['SC', 'ST']]):\n",
    "#         self.splt_cri = splt_cri\n",
    "#         self.groups = groups\n",
    "        \n",
    "    def __init__(self, splt_cri, groups=[['n'], ['SC', 'ST']]):\n",
    "        self.splt_cri = splt_cri\n",
    "        self.groups = groups\n",
    "\n",
    "    def split(self, x, y=None):\n",
    "        groups = self.groups \n",
    "        fold = self.splt_cri[0] \n",
    "        percent = self.splt_cri[1]\n",
    "        y = np.array(y) \n",
    "        unique_y = np.unique(y)\n",
    "        num_y = len(unique_y)\n",
    "        y_indices = np.arange(num_y)\n",
    "        # print('AAAA--->> ', unique_y, y_indices)\n",
    "\n",
    "        remain_x = [i for i,v in enumerate(x)]\n",
    "        all_filterred_x = []\n",
    "        for grp in groups:\n",
    "#             print('Group', grp)\n",
    "            tmp_filterred_x = [] \n",
    "            for it in grp: \n",
    "                gg = f\"^{it}\\d\"\n",
    "                r = re.compile(gg) \n",
    "                filterred_x = list(filter(r.match, x))\n",
    "                # print('BBBB--->> ', gg, filterred_x) \n",
    "                filterred_x_ind = [i for i,v in enumerate(x) if v in filterred_x]\n",
    "                tmp_filterred_x.extend(filterred_x_ind) \n",
    "                # tmp_filterred_x.extend(filterred_x) \n",
    "            remain_x = [i for i in remain_x if i not in tmp_filterred_x] \n",
    "            all_filterred_x.append(tmp_filterred_x) \n",
    "            # print('CCCC--->> ', all_filterred_x)\n",
    "            # print('222--->', grp, remain_x) \n",
    "\n",
    "        # print('--->', remain_x) \n",
    "        remain_x_ind = [i for i,v in enumerate(x) if i in remain_x]\n",
    "        all_filterred_x.append(remain_x_ind) \n",
    "        # all_filterred_x.append(remain_x)     \n",
    "        all_dat = [item for row in all_filterred_x for item in row]\n",
    "        # print('DDDD--->> ', all_filterred_x, all_dat)\n",
    "\n",
    "        num_groups = len(all_filterred_x) \n",
    "        groups_item_len = [len(it) for it in all_filterred_x] \n",
    "        groups_item_ratio = [int(it/fold) if (it/fold)==int(it/fold) else int(it/fold+1) for it in groups_item_len] \n",
    "        # print('EEEE--->> ', num_groups, groups_item_len, groups_item_ratio)\n",
    "\n",
    "        main_grps = [it//fold  for it in groups_item_len]  # math.floor(it/fold) \n",
    "        ext_grps = [it%fold for it in groups_item_len] \n",
    "        # print('FFFF--->> ', main_grps, ext_grps)  \n",
    "\n",
    "        all_fold_values = [[] for _ in range(fold)] \n",
    "        for i, dat in enumerate(all_fold_values):\n",
    "            test_dat = [] \n",
    "            for l, (j,k,fd) in enumerate(zip(main_grps, ext_grps, all_filterred_x)): \n",
    "                # print('---->> ', i, j, k, i*j, i*j+j*1, i<k, fold*j+i)\n",
    "                dd = fd[i*j : i*j+j*1]\n",
    "                ex_dd = [fd[fold*j+i]] if i<k else []\n",
    "                dd.extend(ex_dd)\n",
    "                # print('---->> ', i, j, k, i*j, i*j+j*1, i<k, fold*j+i, dd, ex_dd)\n",
    "                test_dat.extend( dd ) \n",
    "            test_dat = list(set(test_dat))\n",
    "            train_dat = list( set(all_dat)-set(test_dat) ) \n",
    "            # print('GGGG--->> ', test_dat, train_dat)\n",
    "            yield train_dat, test_dat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "306a1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ML Classifier class \n",
    "##### import random\n",
    "import multiprocessing as mp\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, ShuffleSplit, LeavePOut, KFold, ParameterGrid\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier \n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# HumachLab_ML_CLassifiers     \n",
    "\n",
    "\n",
    "\n",
    "# ### All models' implementation\n",
    "\n",
    "class HumachLab_ML_CLassifiers:\n",
    "    \n",
    "    def print_message(self):\n",
    "#         ---------------------------------------------------------------------------------------------------\n",
    "#         ===================================================================================================\n",
    "#         ###################################################################################################\n",
    "#         ***************************************************************************************************\n",
    "        self.logger.info(f\"Hello from HumachLab_ML_CLassifiers class\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, logger, directory, dataset, class_name, label_map, metadata_column, split_column, random_state_value, split_balance_pattern, check_result=False): \n",
    "        self.logger = logger \n",
    "        self.directory = directory\n",
    "        self.dataset = dataset \n",
    "        self.class_name = class_name\n",
    "        self.label_map = label_map\n",
    "        self.metadata_column = metadata_column\n",
    "        self.split_column = split_column \n",
    "        self.is_multiclass = True if len(dataset[class_name].unique().tolist())>2 else False\n",
    "        self.random_state_value = random_state_value\n",
    "        self.split_balance_pattern = split_balance_pattern\n",
    "        if not check_result:\n",
    "            self.experiment_info = {\n",
    "                'logger':logger, 'directory':directory, 'dataset_size':dataset.shape, 'dataset_columns':dataset.columns.values.tolist(), 'metadata_column':metadata_column, \n",
    "                'class_name':class_name, 'label_map':label_map, 'split_column':split_column, 'total_unique_classes':dataset[class_name].value_counts().keys().tolist(), \n",
    "                'total_unique_classes':dataset[class_name].value_counts().values.tolist()\n",
    "                }\n",
    "        \n",
    "        self.best_model_scoring_metrics=[ML_Performace_Metrics.RECL, ML_Performace_Metrics.F1SCR] \n",
    "        \n",
    "        self.logger.info(f\"\"\"\n",
    "        Object is initialised with the following properties: \n",
    "        ###################################################################################################\n",
    "        Dataset size: {self.dataset.shape}, Columns: {self.dataset.columns.values.tolist()}\n",
    "        Target class column name: {self.class_name}\n",
    "        Metadata column names: {self.metadata_column}\n",
    "        Dataset split column on which the training and test sets will be devided: {self.split_column}\n",
    "        Is multi-class classification: {self.is_multiclass}\n",
    "        \"\"\") \n",
    "        return  \n",
    "    \n",
    "    \n",
    "    def convert_list_to_string(self, lst):\n",
    "        lst = [str(l) for l in lst]        \n",
    "        return '* '.join(lst) \n",
    "    \n",
    "    \n",
    "    \n",
    "    def classify(self, should_use_params, splitting_crieteria, model_list, is_validate_models, result_save_path, exp_name, exp_detail, apply_feature_selection, custom_splitter):\n",
    "        self.splitting_crieteria = splitting_crieteria    ### for test & training (validation) splitting_crieteria (n): n=0 -loso, n>0 -n-fold, n<0 -shuffled random splitting with n% testing\n",
    "        self.model_list = model_list \n",
    "        self.should_use_params = should_use_params\n",
    "        self.is_validate_models = is_validate_models \n",
    "        self.result_save_path = result_save_path \n",
    "        self.exp_name = exp_name \n",
    "        self.exp_detail = exp_detail \n",
    "        self.is_binary_classification = not self.is_multiclass\n",
    "        self.apply_feature_selection = apply_feature_selection \n",
    "        self.selected_features = None \n",
    "        self.custom_splitter = custom_splitter\n",
    "        \n",
    "        # self.experiment_info['exp_name'] = exp_name \n",
    "        self.experiment_info.update(exp_detail)\n",
    "        self.experiment_info['apply_feature_selection'] = apply_feature_selection\n",
    "        #self.experiment_info['selected_features'] = self.selected_features \n",
    "        self.experiment_info['is_multiclass_classification'] = self.is_multiclass\n",
    "        self.experiment_info['model_list'] = model_list \n",
    "        self.experiment_info['should_use_params'] = should_use_params\n",
    "        self.experiment_info['is_validate_models'] = is_validate_models\n",
    "        self.experiment_info['result_save_path'] = result_save_path\n",
    "        self.experiment_info['random_state_value'] = self.random_state_value \n",
    "        self.experiment_info['custom_splitter'] = self.custom_splitter\n",
    "        self.experiment_info['split_balance_pattern'] = self.split_balance_pattern\n",
    "        tmp = splitting_crieteria[0] \n",
    "        self.experiment_info['test_split_crieteria'] = tmp \n",
    "        self.experiment_info['test_split_details'] = f'Leave-one-out' if tmp[0]==0 else (f'{tmp[0]}-fold cross validation' if (tmp[0]>0 and tmp[1]<=0) else f'{tmp[0]}-fold {tmp[1]}% random test splitting') \n",
    "        tmp = splitting_crieteria[1] \n",
    "        self.experiment_info['training_split_crieteria'] = tmp \n",
    "        self.experiment_info['training_split_details'] = f'Leave-one-out' if tmp[0]==0 else (f'{tmp[0]}-fold cross validation' if (tmp[0]>0 and tmp[1]<=0) else f'{tmp[0]}-fold {tmp[1]}% random test splitting') \n",
    "        self.experiment_info['model_selection_matrics'] = self.best_model_scoring_metrics \n",
    "                                                                                                  \n",
    "                                                                                                                      \n",
    "        self.logger.info(f\"\"\"\n",
    "        Classification is set with the following parameters: \n",
    "        ###################################################################################################\n",
    "        Splitting crieteria: {self.splitting_crieteria}\n",
    "        Test split: {f'Leave-one-out' if splitting_crieteria[0] [0]==0 else (f'{splitting_crieteria[0] [0]}-fold cross validation' if (splitting_crieteria[0] [0]>0 and splitting_crieteria[0] [1]<=0) else f'{splitting_crieteria[0] [0]}-fold {splitting_crieteria[0] [1]}% random test splitting') }\n",
    "        Training split: {f'Leave-one-out' if splitting_crieteria[1] [0]==0 else (f'{splitting_crieteria[1] [0]}-fold cross validation' if (splitting_crieteria[1] [0]>0 and splitting_crieteria[1] [1]<=0) else f'{splitting_crieteria[1] [0]}-fold {splitting_crieteria[1] [1]}% random test splitting') }\n",
    "        List of ML models that will be applied: {[mn.value for mn in self.model_list]}\n",
    "        Use parameters for model: {self.should_use_params}\n",
    "        Is validate the model (or only train): {self.is_validate_models} \n",
    "        Classification results will be saved in the directory: {self.result_save_path}\n",
    "        \"\"\") \n",
    "        all_exp_info_df = pd.DataFrame(self.experiment_info.items(), columns=['Information', 'Description']) \n",
    "        all_best_tr_model, all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_ts_model, all_ts_scores_df, all_ts_prediction_df, all_ts_fold_info_df  = self.test() \n",
    "        \n",
    "        self.save_results(self.directory, all_best_tr_model, all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_ts_model, all_ts_scores_df, all_ts_prediction_df, all_ts_fold_info_df, all_exp_info_df) \n",
    "        \n",
    "        return all_best_tr_model, all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_ts_model, all_ts_scores_df, all_ts_prediction_df, all_ts_fold_info_df, all_exp_info_df \n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_results(self, save_directory):\n",
    "        # './Results/_Classification/ML001/'\n",
    "        self.logger.info(f\"\"\"\n",
    "        Data is being loaded from: {save_directory}\n",
    "        \"\"\") \n",
    "        save_path = f\"{save_directory}all_tr_scores.csv\" \n",
    "        all_tr_scores_df = pd.read_csv(save_path, index_col=False) \n",
    "        \n",
    "        save_path = f\"{save_directory}all_tr_prediction.csv\" \n",
    "        all_tr_prediction_df = pd.read_csv(save_path, index_col=False) \n",
    "        \n",
    "        save_path = f\"{save_directory}all_ts_scores.csv\" \n",
    "        all_ts_scores_df = pd.read_csv(save_path, index_col=False) \n",
    "        \n",
    "        save_path = f\"{save_directory}all_ts_prediction.csv\" \n",
    "        all_ts_prediction_df = pd.read_csv(save_path, index_col=False)         \n",
    "        \n",
    "        save_path = f\"{save_directory}all_ts_fold_info.csv\" \n",
    "        all_ts_fold_info_df = pd.read_csv(save_path, index_col=False)         \n",
    "        \n",
    "        save_path = f\"{save_directory}all_exp_info.csv\" \n",
    "        all_exp_info_df = pd.read_csv(save_path, index_col=False)  \n",
    "        \n",
    "        new_save_directory = f\"{save_directory}/Models/\"\n",
    "        \n",
    "        save_path = f\"{new_save_directory}ts_model\" \n",
    "        all_ts_model = self.load_models_from_file(save_path, 'Test Models')\n",
    "        \n",
    "        save_path = f\"{new_save_directory}tr_model\" \n",
    "        all_tr_model = self.load_models_from_file(save_path, 'Training Models')\n",
    "        \n",
    "        save_path = f\"{new_save_directory}best_tr_model\" \n",
    "        all_best_tr_model = self.load_models_from_file(save_path, 'Best Training Models')\n",
    "        \n",
    "        return all_best_tr_model, all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_ts_model, all_ts_scores_df, all_ts_prediction_df, all_ts_fold_info_df, all_exp_info_df \n",
    "\n",
    "\n",
    "    def load_models_from_file(self, save_path, model_type):\n",
    "        models_dict = {} \n",
    "        \n",
    "        save_path = f'{save_path}*'\n",
    "        files = self.sort_string_list(glob.glob(save_path)) \n",
    "        files\n",
    "        selected_files = [[int(fn) for fn in f[len(save_path):].split('.')[0].split('_')] for f in files]\n",
    "        selected_files\n",
    "        \n",
    "        self.logger.info(f'Start retrieving {model_type} model from file...')\n",
    "        model_dict = {}  \n",
    "        for i, (ind, fl) in enumerate(zip(selected_files, files)):\n",
    "            if len(ind)==3:\n",
    "                tsi, tri, modi = ind\n",
    "                mod = None \n",
    "                with open(fl, \"rb\") as f:\n",
    "                    mod = pickle.load(f) \n",
    "                if tsi not in model_dict.keys(): \n",
    "                    model_dict[tsi] = {}\n",
    "                if tri not in model_dict[tsi].keys(): \n",
    "                    model_dict[tsi][tri] = {} \n",
    "                if modi not in model_dict[tsi][tri].keys(): \n",
    "                    model_dict[tsi][tri][modi] = mod \n",
    "            elif len(ind)==2:\n",
    "                tsi, modi = ind\n",
    "                mod = None \n",
    "                with open(fl, \"rb\") as f:\n",
    "                    mod = pickle.load(f) \n",
    "                if tsi not in model_dict.keys(): \n",
    "                    model_dict[tsi] = {}\n",
    "                if modi not in model_dict[tsi].keys(): \n",
    "                    model_dict[tsi][modi] = mod \n",
    "            else:\n",
    "                self.logger.info(f'Doesn\\'t identify {model_type} model file to retrieve...')\n",
    "        \n",
    "        model_dict\n",
    "        self.logger.info(f'Finish retrieving {model_type} model from file...')\n",
    "        return model_dict \n",
    "    \n",
    "    \n",
    "    \n",
    "    def sort_string_list(self, string_list):\n",
    "        ## ref: https://blog.codinghorror.com/sorting-for-humans-natural-sort-order/\n",
    "        \"\"\" Sort the given list in the way that humans expect.\n",
    "        \"\"\"\n",
    "        convert = lambda text: int(text) if text.isdigit() else text\n",
    "        alphanum_key = lambda key: [ convert(c.replace(\"_\",\"\")) for c in re.split('([0-9]+)', key) ]\n",
    "        string_list.sort( key=alphanum_key )\n",
    "        return string_list\n",
    "    \n",
    "    \n",
    "        \n",
    "    def save_results(self, save_directory, all_best_tr_model, all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_ts_model, all_ts_scores_df, all_ts_prediction_df, all_ts_fold_info_df, all_exp_info_df):\n",
    "        # './Results/_Classification/ML001/'\n",
    "        save_path = f\"{save_directory}all_tr_scores.csv\" \n",
    "        all_tr_scores_df.to_csv(save_path, index=False) \n",
    "        \n",
    "        save_path = f\"{save_directory}all_tr_prediction.csv\" \n",
    "        all_tr_prediction_df.to_csv(save_path, index=False) \n",
    "        \n",
    "        save_path = f\"{save_directory}all_ts_scores.csv\" \n",
    "        all_ts_scores_df.to_csv(save_path, index=False) \n",
    "        \n",
    "        save_path = f\"{save_directory}all_ts_prediction.csv\" \n",
    "        all_ts_prediction_df.to_csv(save_path, index=False) \n",
    "        \n",
    "        save_path = f\"{save_directory}all_ts_fold_info.csv\" \n",
    "        all_ts_fold_info_df.to_csv(save_path, index=False) \n",
    "        \n",
    "        save_path = f\"{save_directory}all_exp_info.csv\" \n",
    "        all_exp_info_df.to_csv(save_path, index=False) \n",
    "        \n",
    "        new_save_directory = self.create_directory(save_directory, 'Models')\n",
    "        \n",
    "        save_path = f\"{new_save_directory}ts_model\" \n",
    "        self.save_models_to_file(save_path, all_ts_model, 'Test Models')\n",
    "        \n",
    "        save_path = f\"{new_save_directory}tr_model\" \n",
    "        self.save_models_to_file(save_path, all_tr_model, 'Training Models')\n",
    "        \n",
    "        save_path = f\"{new_save_directory}best_tr_model\" \n",
    "        self.save_models_to_file(save_path, all_best_tr_model, 'Best Training Models')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def create_directory(self, path, dir_name): \n",
    "        new_directory = f\"{path}/{dir_name}/\"\n",
    "        \n",
    "        if (not os.path.exists(new_directory)):\n",
    "            try:\n",
    "                os.makedirs(new_directory, exist_ok = True)\n",
    "                print(f\"Directory successfully created at path: {new_directory}\") \n",
    "            except OSError as error:\n",
    "                print(f\"Directory cannot be created at path: {new_directory}\") \n",
    "        else:\n",
    "            print(f\"Directory already exists at path: {new_directory}\") \n",
    "            \n",
    "        return new_directory\n",
    "\n",
    "\n",
    "    def save_models_to_file(self, save_path, models_dict, model_type):\n",
    "        self.logger.info(f'Start saving model to file...')\n",
    "        if model_type=='Training Models':\n",
    "            for tsfi, ts_dat in models_dict.items():\n",
    "                for trfi, tr_dat in ts_dat.items():\n",
    "                    for modi, mod in tr_dat.items():\n",
    "                        new_save_path = f'{save_path}_{tsfi}_{trfi}_{modi}.dat'\n",
    "                        try:\n",
    "                            with open(new_save_path, 'wb') as f:\n",
    "                                pickle.dump(mod, f)\n",
    "                                print( f'{model_type} is written to the file: {new_save_path}\\n' )\n",
    "                        except:\n",
    "                            print( f'Problem creating {model_type} file: {new_save_path}\\n' )\n",
    "        else:\n",
    "            for tsfi, ts_dat in models_dict.items():\n",
    "                for modi, mod in ts_dat.items():\n",
    "                    new_save_path = f'{save_path}_{tsfi}_{modi}.dat'\n",
    "                    try:\n",
    "                        with open(new_save_path, 'wb') as f:\n",
    "                            pickle.dump(mod, f)\n",
    "                            print( f'{model_type} is written to the file: {new_save_path}\\n' )\n",
    "                    except:\n",
    "                        print( f'Problem creating {model_type} file: {new_save_path}\\n' )\n",
    "        self.logger.info(f'Finish saving model to file...')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def evaluate_test_data(self, ind, model_list, test_ids, best_tr_model):\n",
    "        self.logger.info(f\"\"\"\n",
    "        ### MODEL EVALUATION PHASE \n",
    "        EVALUATION {ind} START... XXXXX \n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        \"\"\") \n",
    "        #print(\"HHHHHH3333\", best_tr_model)\n",
    "        \n",
    "        X_test, y_test, meta_dat_df, _ = self._get_data_from_indices(test_ids, from_where=f'_TS_ts_{ind}')\n",
    "        #print(\"meta33->\", meta_dat_df)     \n",
    "#         ### Call features selection algorithm \n",
    "#         if self.selected_features: \n",
    "#             X_test = X_test[self.selected_features]\n",
    "        \n",
    "        \n",
    "        ts_score_df, ts_prediction_df = pd.DataFrame(), pd.DataFrame() \n",
    "        # all_ts_model, all_ts_scores_df, all_ts_prediction_df = {}, pd.DataFrame(), pd.DataFrame()\n",
    "        for (modi, model), classifier_method in zip(best_tr_model.items(), model_list) : \n",
    "            y_pred = model.predict(X_test) \n",
    "            y_pred_proba = model.predict_proba(X_test) \n",
    "            if modi==1:\n",
    "                # meta_dat_df.reset_index(drop=True, inplace=True) \n",
    "                ts_prediction_df = pd.concat([ts_prediction_df, meta_dat_df]) \n",
    "                ts_prediction_df.reset_index(drop=True, inplace=True) \n",
    "                ts_prediction_df[self.class_name] = y_test \n",
    "                \n",
    "            ts_prediction_df[f\"Prediction_{ML_Classifiers.get_short_form(str(classifier_method.value))}\"] = y_pred \n",
    "            for p_ind in range(y_pred_proba.shape[1]):\n",
    "                ts_prediction_df[f\"Prediction_Proba_{p_ind}_{ML_Classifiers.get_short_form(str(classifier_method.value))}\"] = y_pred_proba[:, p_ind].tolist()\n",
    "            \n",
    "            scores_df = self.calculate_model_scores(model, y_test, y_pred, y_pred_proba)\n",
    "            scores_df.insert(0, \"Model_No\", modi) \n",
    "            ts_score_df = pd.concat([ts_score_df, scores_df]) \n",
    "            \n",
    "        ts_score_df.insert(0, \"Test_No\", ind) \n",
    "        ts_prediction_df.insert(0, \"Test_No\", ind) \n",
    "        return ts_score_df, ts_prediction_df \n",
    "    \n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "        # splitter = self.get_data_splitter(self.splitting_crieteria[0], stratified=True, custom=True) #Test split cusomised: usually LOSO or 10-fold \n",
    "        # splitter = self.get_data_splitter(self.splitting_crieteria[0], stratified=True, custom=False) #Test split: usually LOSO or 10-fold \n",
    "        splitter = self.get_data_splitter(self.splitting_crieteria[0], stratified=True, custom=self.custom_splitter) #Test split cusomised: usually LOSO or 10-fold \n",
    "        split_data_list = self.dataset[self.split_column].values.tolist() \n",
    "        class_data_list = self.dataset[self.class_name].values.tolist() \n",
    "        \n",
    "        all_ts_model, all_ts_scores_df, all_ts_prediction_df, all_ts_fold_info_df = {}, pd.DataFrame(), pd.DataFrame(), pd.DataFrame()  \n",
    "        cum_best_tr_model, cum_tr_model, cum_tr_scores_df, cum_tr_prediction_df, cum_tr_fold_info_df = {}, {}, pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "        for tsi, (train_all_ids, test_ids) in enumerate(splitter.split(split_data_list, class_data_list)): \n",
    "            ### Train-test split: fold based \n",
    "#             ts_dat = self.dataset[self.split_column][test_ids].values.tolist() \n",
    "#             tr_all_dat = self.dataset[self.split_column][train_all_ids].values.tolist() \n",
    "#             ts_dat = self.dataset.loc[test_ids, self.split_column].values.tolist() \n",
    "#             tr_all_dat = self.dataset.loc[train_all_ids, self.split_column].values.tolist() \n",
    "            ts_dat = self.dataset.iloc[test_ids][self.split_column].values.tolist() \n",
    "            tr_all_dat = self.dataset.iloc[train_all_ids][self.split_column].values.tolist() \n",
    "            print('test- tsi, train_all_ids, test_ids, tr_all_dat, ts_dat', tsi, train_all_ids, test_ids, tr_all_dat, ts_dat) \n",
    "#             print(f\"QQQQQQQQQQQQQ {train_all_ids}, {self.dataset[self.split_column][train_all_ids].values.tolist()}, {self.dataset[self.split_column].values.tolist() }\")\n",
    "            ind = tsi+1 \n",
    "            self.selected_features = None ### Resetting feature selection list \n",
    "            self.logger.info(f\"\"\"\n",
    "            ### MODEL TEST PHASE \n",
    "            TEST {ind} START... XXXXX \n",
    "            ===================================================================================================\n",
    "            Test=> {len(ts_dat)} {(ts_dat)} \n",
    "            Training (Including Validation)=> {len(tr_all_dat)} {(tr_all_dat)} \n",
    "            \"\"\") \n",
    "            best_tr_model, all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_tr_fold_info_df = self.train(ind, model_list, train_all_ids, tr_splitting_crieteria=self.splitting_crieteria[1]) #tr_splitting_crieteria: <0 random split, >0 fold\n",
    "#             continue\n",
    "            cum_tr_model[ind] = all_tr_model \n",
    "            cum_best_tr_model[ind] = best_tr_model \n",
    "            if self.is_validate_models:\n",
    "                all_tr_scores_df.insert(0, \"Test_No\", ind) \n",
    "                all_tr_prediction_df.insert(0, \"Test_No\", ind) \n",
    "            all_tr_fold_info_df.insert(0, \"Test_No\", ind) \n",
    "            all_tr_fold_info_df.insert(4, \"Test\", [ts_dat]*all_tr_fold_info_df.shape[0]) \n",
    "            all_tr_fold_info_df.insert(4, \"Selected_Features\", [self.selected_features]*all_tr_fold_info_df.shape[0]) \n",
    "            \n",
    "            cum_tr_scores_df = pd.concat([cum_tr_scores_df, all_tr_scores_df]) \n",
    "            cum_tr_prediction_df = pd.concat([cum_tr_prediction_df, all_tr_prediction_df])\n",
    "            all_ts_fold_info_df = pd.concat([all_ts_fold_info_df, all_tr_fold_info_df])\n",
    "            \n",
    "            cum_tr_scores_df.reset_index(drop=True, inplace=True) \n",
    "            cum_tr_prediction_df.reset_index(drop=True, inplace=True) \n",
    "            all_ts_fold_info_df.reset_index(drop=True, inplace=True) \n",
    "            \n",
    "#             print(\"TTTT\", best_tr_model.keys(), best_tr_model, all_tr_scores_df.shape, all_tr_scores_df.columns, all_tr_prediction_df.shape, all_tr_prediction_df.columns) \n",
    "                        \n",
    "            ###############\n",
    "            ### Model evaluation with the test data using the best trained model  \n",
    "            all_ts_model[ind] = best_tr_model\n",
    "            ts_score_df, ts_prediction_df = self.evaluate_test_data(ind, model_list, test_ids, best_tr_model) \n",
    "            \n",
    "            all_ts_scores_df = pd.concat([all_ts_scores_df, ts_score_df]) \n",
    "            all_ts_prediction_df = pd.concat([all_ts_prediction_df, ts_prediction_df])\n",
    "            \n",
    "            all_ts_scores_df.reset_index(drop=True, inplace=True) \n",
    "            all_ts_prediction_df.reset_index(drop=True, inplace=True) \n",
    "        \n",
    "            self.logger.info(f\"\"\"\n",
    "            ===================================================================================================\n",
    "            TEST {ind} END...\n",
    "            \"\"\") \n",
    "            \n",
    "        ### Sorting scores\n",
    "#             print( 'TTRR', cum_tr_scores_df.columns.values.tolist(), cum_tr_prediction_df.columns.values.tolist() )\n",
    "        cum_tr_scores_df.sort_values(['Model_No', 'Test_No', 'Training_No'], ascending = [True, True, True], inplace=True)  \n",
    "        cum_tr_prediction_df.sort_values(['Test_No', 'Training_No'], ascending = [True, True], inplace=True)  \n",
    "\n",
    "#             print( 'TTSS', all_ts_scores_df.columns.values.tolist(), all_ts_prediction_df.columns.values.tolist() )\n",
    "        all_ts_scores_df.sort_values(['Model_No', 'Test_No'], ascending = [True, True], inplace=True) \n",
    "        all_ts_prediction_df.sort_values(['Test_No'], ascending = [True], inplace=True) \n",
    "        all_ts_fold_info_df.sort_values(['Model_No', 'Test_No', 'Training_No'], ascending = [True, True, True], inplace=True) \n",
    "        \n",
    "        all_ts_fold_info_df\n",
    "        \n",
    "        return cum_best_tr_model, cum_tr_model, cum_tr_scores_df, cum_tr_prediction_df, all_ts_model, all_ts_scores_df, all_ts_prediction_df, all_ts_fold_info_df  \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self, tsi, model_list, train_all_ids, tr_splitting_crieteria):           \n",
    "        # tr_splitter = self.get_data_splitter(tr_splitting_crieteria, stratified=True, custom=True) #Training split customised: usually 5-fold or Random 20% split\n",
    "        tr_splitter = self.get_data_splitter(tr_splitting_crieteria, stratified=True, custom=False) #Training split: usually 5-fold or Random 20% split\n",
    "        all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_tr_fold_info_df = {}, pd.DataFrame(), pd.DataFrame(), pd.DataFrame()  \n",
    "        best_tr_model, best_acc, best_rec, best_prec = {}, [], [], []  \n",
    "        class_data_list2 = self.dataset.iloc[train_all_ids][self.class_name].values.tolist() \n",
    "        split_data_list2 = self.dataset.iloc[train_all_ids][self.split_column].values.tolist() \n",
    "#         split_data_list2 = self.dataset[self.split_column][train_all_ids].values.tolist()\n",
    "#         class_data_list2 = self.dataset[self.class_name][train_all_ids].values.tolist() \n",
    "#         class_data_list2 = self.dataset.loc[train_all_ids, self.class_name].values.tolist() \n",
    "#         split_data_list2 = self.dataset.loc[train_all_ids, self.split_column].values.tolist()\n",
    "#         print(f\"QQQQQQQQQQQQQ 222222 {train_all_ids}, {self.dataset[self.split_column][train_all_ids].values.tolist()}, {self.dataset[self.split_column].values.tolist()}\")\n",
    "        # print('QQQQQQQQQQQ', split_data_list2, class_data_list) \n",
    "        for tri, (tmp_train_ids, tmp_val_ids) in enumerate(tr_splitter.split(split_data_list2, class_data_list2)): \n",
    "        # for tri, (tmp_train_ids, tmp_val_ids) in enumerate(tr_splitter.split(split_data_list2)): \n",
    "            \n",
    "#             if not should_validate: \n",
    "#                 tmp_train_ids = np.concatenate((tmp_train_ids, tmp_val_ids)) \n",
    "#             else:\n",
    "#                 tmp_train_ids = np.concatenate((tmp_train_ids, tmp_val_ids)) \n",
    "#                 tmp_val_ids = tmp_train_ids.copy() \n",
    "            tmp_train_ids = np.concatenate((tmp_train_ids, tmp_val_ids)) \n",
    "            tmp_val_ids = tmp_train_ids.copy() \n",
    "            \n",
    "            ### This tmp_train_ids and tmp_val_ids are indexes re-starting from 0 again, but we need to get the actual indexes in the dataset \n",
    "            xx = (np.array(split_data_list2)[tmp_val_ids]).tolist() \n",
    "            val_ids = self.dataset[self.dataset[self.split_column].isin(xx)].index.tolist()\n",
    "            xx = (np.array(split_data_list2)[tmp_train_ids]).tolist() \n",
    "            train_ids = self.dataset[self.dataset[self.split_column].isin(xx)].index.tolist()\n",
    "            \n",
    "            ### Validation-train split: random percentage based \n",
    "            val_dat = self.dataset[self.split_column][val_ids].values.tolist() \n",
    "            tr_dat = self.dataset[self.split_column][train_ids].values.tolist() \n",
    "#             val_dat = self.dataset.loc[val_ids, self.split_column].values.tolist() \n",
    "#             tr_dat = self.dataset.loc[train_ids, self.split_column].values.tolist() \n",
    "#             val_dat = self.dataset.iloc[val_ids][self.split_column].values.tolist() \n",
    "#             tr_dat = self.dataset.iloc[train_ids][self.split_column].values.tolist() \n",
    "            print('train- tri, tmp_train_ids, tmp_val_ids, tr_dat, val_dat', tri, tmp_train_ids, tmp_val_ids, tr_dat, val_dat) \n",
    "            ind = tri+1 \n",
    "            self.logger.info(f\"\"\"\n",
    "            *** MODEL TRAINING AND VALIDATION PHASE FOR TEST {tsi} \n",
    "            TRAINING {tri+1} START... XXXXX \n",
    "            ***************************************************************************************************\n",
    "            Validation=> {len(val_dat)} {(val_dat)} \n",
    "            Training=> {len(tr_dat)} {(tr_dat)} \n",
    "            \"\"\")            \n",
    "#             continue\n",
    "            all_mtr_model, all_mtr_scores_df, all_mtr_prediction_df, all_mtr_fold_info_df = self.train_models(model_list, train_ids, val_ids, tsi, ind)            \n",
    "            all_tr_model[ind] = all_mtr_model \n",
    "            if self.is_validate_models:\n",
    "                all_mtr_scores_df.insert(0, \"Training_No\", ind) \n",
    "                all_mtr_prediction_df.insert(0, \"Training_No\", ind) \n",
    "            all_mtr_fold_info_df.insert(0, \"Training_No\", ind) \n",
    "                \n",
    "            all_tr_scores_df = pd.concat([all_tr_scores_df, all_mtr_scores_df]) \n",
    "            all_tr_prediction_df = pd.concat([all_tr_prediction_df, all_mtr_prediction_df])\n",
    "            all_tr_fold_info_df = pd.concat([all_tr_fold_info_df, all_mtr_fold_info_df])\n",
    "            \n",
    "            all_tr_scores_df.reset_index(drop=True, inplace=True) \n",
    "            all_tr_scores_df.reset_index(drop=True, inplace=True)    \n",
    "            all_tr_fold_info_df.reset_index(drop=True, inplace=True)          \n",
    "            \n",
    "            self.logger.info(f\"\"\"\n",
    "            ---------------------------------------------------------------------------------------------------\n",
    "            Best model index calculation  \n",
    "            \"\"\")             \n",
    "#             print(\"PPPPP\", tri, ind, all_mtr_model.keys(), all_mtr_model, all_mtr_scores_df)\n",
    "            if tri==0:\n",
    "                best_tr_model = all_mtr_model.copy() \n",
    "                print(\"WWWWWWWWWWWWWWWWWWW\", ML_Performace_Metrics.RECL.value, all_mtr_scores_df)\n",
    "                #print(\"HHHH\", all_mtr_scores_df.columns)\n",
    "                # best_acc, best_prec, best_rec = all_mtr_scores_df[ML_Performace_Metrics.ACC.value], all_mtr_scores_df[ML_Performace_Metrics.PREC.value], all_mtr_scores_df[ML_Performace_Metrics.RECL.value] \n",
    "                best_rec = all_mtr_scores_df[ML_Performace_Metrics.RECL.value].values.tolist()  \n",
    "            else:                \n",
    "                for jj, mn in enumerate(model_list):\n",
    "                    mod_name = ML_Classifiers.get_short_form(str(mn.value))\n",
    "                    tm_df = all_mtr_scores_df[(all_mtr_scores_df[\"Model_Name\"]==mod_name)] \n",
    "                    new = tm_df[ML_Performace_Metrics.RECL.value].values.tolist()[0] \n",
    "                    if new>best_rec[jj]: \n",
    "                        best_rec[jj] = new \n",
    "                        best_tr_model[tm_df[\"Model_No\"].values.tolist()[0]] = all_mtr_model[tm_df[\"Model_No\"].values.tolist()[0]]  \n",
    "                        \n",
    "            \n",
    "            self.logger.info(f\"\"\"\n",
    "            ***************************************************************************************************\n",
    "            TRAINING {ind} END... \n",
    "            \"\"\") \n",
    "        return best_tr_model, all_tr_model, all_tr_scores_df, all_tr_prediction_df, all_tr_fold_info_df \n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_models(self, model_list, train_ids, val_ids, ts_serial, tr_serial):\n",
    "        all_mtr_model, all_mtr_scores_df, all_mtr_prediction_df, all_mtr_fold_info_df = {}, pd.DataFrame(), pd.DataFrame(), pd.DataFrame() \n",
    "        for modi, classifier_method in enumerate(model_list): \n",
    "            ind = modi+1 \n",
    "            self.logger.info(f\"\"\"\n",
    "            *** ML MODEL FOR TEST:{ts_serial}, TRAINING:{tr_serial} AND MODEL: {ML_Classifiers.get_short_form(str(classifier_method.value))} \n",
    "            ---------------------------------------------------------------------------------------------------\n",
    "            \"\"\")\n",
    "            mtr_model, mtr_scores_df, mtr_prediction_df, mtr_fold_info_df = self.start_training(classifier_method, train_ids, val_ids, ts_serial=ts_serial, tr_serial=tr_serial, mod_serial=ind) \n",
    "            #print(\"HELLO2222\", mtr_model, mtr_scores_df, mtr_prediction_df) \n",
    "            all_mtr_model[ind] = mtr_model \n",
    "            #all_mtr_model[\"Model_Name\"] = ML_Classifiers.get_short_form(str(classifier_method.value))  \n",
    "            if self.is_validate_models:                   \n",
    "                if modi>0:\n",
    "                    mtr_prediction_df.drop(self.metadata_column, axis=1, inplace=True)\n",
    "                    mtr_prediction_df.drop([self.class_name], axis=1, inplace=True)                \n",
    "                               \n",
    "                mtr_scores_df.insert(0, \"Model_No\", ind) \n",
    "                mtr_scores_df.insert(1, \"Model_Name\", ML_Classifiers.get_short_form(str(classifier_method.value)))  \n",
    "#                 mtr_scores_df.insert(2, \"Selected_Features\", [self.selected_features]*mtr_scores_df.shape[0]) \n",
    "                    \n",
    "                all_mtr_scores_df = pd.concat([all_mtr_scores_df, mtr_scores_df]) \n",
    "                all_mtr_prediction_df = pd.concat([all_mtr_prediction_df, mtr_prediction_df], axis=1) \n",
    "                \n",
    "                all_mtr_scores_df.reset_index(drop=True, inplace=True) \n",
    "                all_mtr_prediction_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            mtr_fold_info_df.insert(0, \"Model_No\", ind)    \n",
    "            mtr_fold_info_df.insert(1, \"Model_Name\", ML_Classifiers.get_short_form(str(classifier_method.value))) \n",
    "            all_mtr_fold_info_df = pd.concat([all_mtr_fold_info_df, mtr_fold_info_df]) \n",
    "            all_mtr_fold_info_df.reset_index(drop=True, inplace=True)  \n",
    "                    \n",
    "        #print(\"HELLO\", all_mtr_model, all_mtr_scores_df, all_mtr_prediction_df) \n",
    "        return all_mtr_model, all_mtr_scores_df, all_mtr_prediction_df, all_mtr_fold_info_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_data_splitter(self, split_crit_tuple, stratified=False, custom=False):\n",
    "        \n",
    "        if custom:\n",
    "            self.logger.info(f\"Custom splitter testing...\") \n",
    "#             splitter = MyCustomSplitter(n_splits=5)\n",
    "#             splitter.set_criteria(split_crit_tuple) \n",
    "            splitter = MyCustomSplitter(split_crit_tuple, groups=self.split_balance_pattern) ### , groups=[['n'], ['SC', 'ST']] for binary and [['n'], ['SC', 'ST']] for multi-class \n",
    "            return splitter\n",
    "        \n",
    "        spl_rand = self.random_state_value ##random.randint(1, 1000)\n",
    "        splitter = None\n",
    "        split_crit = split_crit_tuple[0] ### Fold \n",
    "        split_perc = split_crit_tuple[1] ### Fold \n",
    "        \n",
    "        if split_crit==0:\n",
    "            self.logger.info(f\"Leave-one-subject-out testing...\") \n",
    "            split_num = 1 \n",
    "            splitter = LeavePOut(p=split_num) if stratified else LeavePOut(p=split_num) \n",
    "            # splitter = StratifiedLeavePOut(p=split_num) if stratified else LeavePOut(p=split_num) \n",
    "            # splitter = LeavePOut(p=split_num) \n",
    "            # splitter = StratifiedLeavePOut(p=split_num) #Stratified\n",
    "        elif split_crit>0:\n",
    "            if split_perc<=0:\n",
    "                self.logger.info(f\"{split_crit}-fold testing\") \n",
    "                split_num = 5\n",
    "                if split_crit != split_num:\n",
    "                    split_num = split_crit \n",
    "                splitter = StratifiedKFold(n_splits=split_num, shuffle=False) if stratified else KFold(n_splits=split_num, shuffle=False) \n",
    "                # splitter = StratifiedKFold(n_splits=split_num, shuffle=True, random_state=spl_rand) if stratified else KFold(n_splits=split_num, shuffle=True, random_state=spl_rand) \n",
    "                # splitter = KFold(n_splits=split_num, shuffle=True, random_state=spl_rand)\n",
    "                # splitter = StratifiedKFold(n_splits=split_num, shuffle=True, random_state=spl_rand)\n",
    "                #splitter = KFold(n_splits=split_num, random_state=spl_rand)\n",
    "                #splitter = KFold(n_splits=split_num)\n",
    "            else:\n",
    "                split_num = split_crit \n",
    "                split_ratio = split_perc \n",
    "                self.logger.info(f\"Random {split_ratio} percentage splitting testing...\") \n",
    "                splitter = StratifiedShuffleSplit(n_splits=split_num, test_size=split_ratio, random_state=spl_rand) if stratified else ShuffleSplit(n_splits=split_num, test_size=split_ratio, random_state=spl_rand) \n",
    "                # splitter = ShuffleSplit(n_splits=split_num, test_size=split_ratio, random_state=spl_rand) #rs\n",
    "                # splitter = StratifiedShuffleSplit(n_splits=split_num, test_size=split_ratio, random_state=spl_rand) #rs\n",
    "                # splitter = ShuffleSplit(n_splits=split_num, test_size=split_ratio) #rs\n",
    "        else:\n",
    "            self.logger.info(f\"Problem with the splitting with the splitting criteria {split_crit_tuple}...\") \n",
    "            \n",
    "        # self.data_splitter = splitter \n",
    "        return splitter \n",
    "    \n",
    "    \n",
    "\n",
    "    def start_training(self, classifier_method, train_ids, val_ids, ts_serial, tr_serial, mod_serial):\n",
    "        parameters = self.get_parameters_for_ml_models(classifier_method) \n",
    "        print(\"Parameters: \", parameters)\n",
    "        model, model_scores, target_and_prediction = None, None, None\n",
    "        model, model_scores, target_and_prediction, fold_info_df = self.call_all_model_optimization(classifier_method, parameters, train_ids, val_ids, ts_serial, tr_serial, mod_serial, parameter_optimization=1)\n",
    "        return model, model_scores, target_and_prediction, fold_info_df \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _get_feature_importance_from_AUC_score(self, indices):       \n",
    "        self.logger.info(f\"\"\"\n",
    "        Here to calculate feature importance using AUC\n",
    "        \"\"\")\n",
    "        data = copy.deepcopy(self.dataset).iloc[indices] \n",
    "        \n",
    "        target = data[self.class_name] \n",
    "        metadata_df = data[self.metadata_column] \n",
    "        features = data.drop([self.class_name]+self.metadata_column, axis=1) \n",
    "        \n",
    "        selected_feats_list, importance_scores = self.select_appropriate_features(features, target, num_features=None, selection_criteria={'auc':0.5}) ### selection_criteria=None/{'auc':0.7} #'corr', 'p', 'auc', 'pandauc' \n",
    "        return importance_scores\n",
    "    \n",
    "    \n",
    "    def balance_data(self, features_, target_):\n",
    "#         oversample = SMOTE()\n",
    "#         X, y = oversample.fit_resample(X, y)\n",
    "        tdf = pd.concat([features_, target_], axis=1)\n",
    "        print()\n",
    "        dis_dat = tdf[tdf[self.class_name]!=0]\n",
    "        heal_dat = tdf[tdf[self.class_name]==0]\n",
    "        dis_cnt = dis_dat.shape[0]\n",
    "        heal_cnt = heal_dat.shape[0]\n",
    "        new_df = pd.DataFrame()\n",
    "\n",
    "        self.logger.info(f'Imbalanced data size: disorders= {dis_dat.shape}, healthy= {heal_dat.shape}')\n",
    "        \n",
    "        upsampled_dat = resample(heal_dat, replace=True, n_samples=dis_cnt)\n",
    "        new_df = pd.concat([dis_dat, upsampled_dat]) \n",
    "        \n",
    "        self.logger.info(f'After balancing data size: disorders= {dis_dat.shape}, healthy= {upsampled_dat.shape}')\n",
    "        target = tdf[self.class_name] \n",
    "        features = tdf.loc[:, tdf.columns != self.class_name]\n",
    "            \n",
    "        return features, target\n",
    "\n",
    "\n",
    "    \n",
    "    def _get_data_from_indices(self, indices, from_training=False, from_where='_TR_x'):\n",
    "        data = copy.deepcopy(self.dataset).iloc[indices] \n",
    "#         data.to_csv(f\"{self.directory}/{from_where}.csv\", index=False)\n",
    "        ### Downsample he raining data: 1=down, 2=up, 3=bound sampling\n",
    "        #if from_training:\n",
    "        #    self.logger.info(f'Resampling training data...')\n",
    "        #    data = self.preprocessor.get_resamplled_data(data, self.class_name, self.pat_id_col, random_sampling=True, up_or_down_sampling=1, min_scale=2.0, max_scale=3.0) ## 0-no, 1-down, 2-up, 3-bound\n",
    "                    \n",
    "        self.logger.info(f\"\"\"\n",
    "        From training? {from_training}, Data shape: {data.shape}, Indices: {indices}\n",
    "        All Columns: {data.columns.values.tolist()}\n",
    "        \"\"\") \n",
    "        \n",
    "        all_cols = data.columns.values.tolist() \n",
    "        ft_start_ind = all_cols.index(self.class_name)+1\n",
    "        \n",
    "        target = data[self.class_name] \n",
    "        metadata_df = data[self.metadata_column] \n",
    "#         features = data.drop([self.class_name]+self.metadata_column, axis=1) \n",
    "        features = data.iloc[:, ft_start_ind:]\n",
    "        importance_scores = None\n",
    "        \n",
    "        ### Call features selection algorithm \n",
    "        if self.apply_feature_selection and from_training and self.selected_features is None: \n",
    "            self.logger.info(f\"\"\"\"\n",
    "            Here comes to feature selection...\n",
    "            \"\"\") \n",
    "            selected_feats_list, importance_scores = self.select_appropriate_features(features, target, num_features=None, selection_criteria={'auc':0.5}) ### selection_criteria=None/{'auc':0.7} #'corr', 'p', 'auc', 'pandauc' \n",
    "            self.selected_features = selected_feats_list.copy() \n",
    "            self.logger.info(f\"\"\"\"\n",
    "            Selected features: {self.selected_features}\n",
    "            \"\"\") \n",
    "        elif from_training and self.selected_features is None: \n",
    "            self.selected_features = features.columns.values.tolist() \n",
    "            self.logger.info(f\"\"\"\"\n",
    "            Selected features: {self.selected_features}\n",
    "            \"\"\") \n",
    "            \n",
    "        if 'index' in self.selected_features: #feature_names\n",
    "            self.selected_features.remove('index')\n",
    "            \n",
    "        if self.selected_features is not None:\n",
    "            # features = features.loc[:, ~features.columns.isin(self.selected_features) ]\n",
    "            features = features.loc[:, features.columns.isin(self.selected_features) ]\n",
    "            \n",
    "            \n",
    "#         ### Call data balancing using SMOTE \n",
    "#         features, target = self.balance_data(features, target) \n",
    "        \n",
    "        \n",
    "        self.logger.info(f\"\"\"\n",
    "        Feature shape: {features.shape}, Target shape: {target.shape}, Metadata: {metadata_df.shape} \n",
    "        \"\"\") \n",
    "        \n",
    "        target = target.values.tolist() \n",
    "        features = features.values \n",
    "        \n",
    "        return features, target, metadata_df, importance_scores  \n",
    "    \n",
    "    \n",
    "    \n",
    "    def select_appropriate_features(self, X_dat, y_dat, num_features=None, selection_criteria=None): ### selection_criteria={'auc':0.7} #'corr', 'p', 'auc', 'pandauc' \n",
    "        selected_features = [] \n",
    "        importance_scores = None \n",
    "        crit_name = selection_criteria.keys() \n",
    "        crit_name = list(crit_name)[0] \n",
    "        dpp_obj = DataPreprocessor() \n",
    "        \n",
    "        if crit_name=='corr':\n",
    "            pass\n",
    "        elif crit_name=='p':\n",
    "            selected_features, importance_scores = dpp_obj.select_pandauc_based_features(X_dat, y_dat, binary_class=self.is_binary_classification, p_threshold=0.05, auc_threshold=None, sort=True) \n",
    "        elif crit_name=='auc':\n",
    "            selected_features, importance_scores = dpp_obj.select_pandauc_based_features(X_dat, y_dat, binary_class=self.is_binary_classification, p_threshold=None, auc_threshold=0.5, sort=True) \n",
    "            pass\n",
    "        elif crit_name=='pandauc':\n",
    "            selected_features, importance_scores = dpp_obj.select_pandauc_based_features(X_dat, y_dat, binary_class=self.is_binary_classification, p_threshold=0.05, auc_threshold=0.5, sort=True) \n",
    "            \n",
    "        if num_features:\n",
    "            selected_features = selected_features[:num_features]\n",
    "        \n",
    "        return selected_features, importance_scores \n",
    "    \n",
    "    \n",
    "\n",
    "    def run_model_gridSearch(self, classifier_method, params, train_ids, val_ids, ts_serial, tr_serial, mod_serial):\n",
    "        tmp_train_ids, tmp_val_ids = train_ids.copy(), val_ids.copy()  \n",
    "        should_validate = self.is_validate_models \n",
    "        # should_validate = True \n",
    "        \n",
    "#         if not should_validate:\n",
    "#             # tmp_train_ids.extend(tmp_val_ids) \n",
    "#             tmp_train_ids = np.concatenate((tmp_train_ids, tmp_val_ids)) \n",
    "#         tmp_train_ids = np.concatenate((tmp_train_ids, tmp_val_ids)) \n",
    "        \n",
    "        print('run_model_gridSearch- val_ids, train_ids', val_ids, train_ids)\n",
    "        ### Validation-train split: random percentage based \n",
    "        val_dat = self.dataset[self.split_column][val_ids].values.tolist() \n",
    "        tr_dat = self.dataset[self.split_column][train_ids].values.tolist() \n",
    "#         val_dat = self.dataset.iloc[val_ids, self.split_column].values.tolist() \n",
    "#         tr_dat = self.dataset.iloc[train_ids, self.split_column].values.tolist() \n",
    "        print('run_model_gridSearch- val_dat, tr_dat', val_dat, tr_dat) \n",
    "        fold_info_df = pd.DataFrame([[val_dat, tr_dat]], columns=['Validation', 'Training']) \n",
    "        \n",
    "        X_train, y_train, _, importance_scores = self._get_data_from_indices(tmp_train_ids, from_training=True, from_where=f'_TR_ts_{ts_serial}_tr_{tr_serial}_mod_{mod_serial}') \n",
    "#         if importance_scores is None:\n",
    "#             importance_scores = self._get_feature_importance_from_AUC_score(tmp_train_ids)\n",
    "        importance_scores = self._get_feature_importance_from_AUC_score(tmp_train_ids)\n",
    "        \n",
    "        mods = self.get_ml_model_instances(classifier_method)\n",
    "        self.logger.info(f\"\"\"\n",
    "        GridSearch: {ML_Classifiers.get_short_form(str(classifier_method.value))} - {params} \n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        \"\"\")\n",
    "        parameters = {}\n",
    "        model = mods\n",
    "        model_scores = None\n",
    "        if self.should_use_params:\n",
    "            parameters = params\n",
    "\n",
    "        scoring, refit = self.get_ml_scoring_metrices(self.best_model_scoring_metrics[0]) \n",
    "#         scoring, refit = 'f1', True\n",
    "        self.logger.info(f\"\"\"Refitting the model with best parameter {scoring} == {refit}\"\"\")\n",
    "        \n",
    "        model = GridSearchCV(mods, parameters, scoring=scoring, cv=5, refit=refit, return_train_score=True, n_jobs=50, verbose=2)\n",
    "        # model = GridSearchCV(mods, parameters, scoring=scoring, cv=5, refit=refit, return_train_score=True, error_score='raise', n_jobs=50, verbose=2)\n",
    "        # model = GridSearchCV(mods, parameters, scoring=scoring, cv=5, refit=refit, return_train_score=True, n_jobs=mp.cpu_count(), verbose=2)\n",
    "\n",
    "        # ### Scoring from custom method\n",
    "        # score = make_scorer(self.custom_precision_func, greater_is_better=False)\n",
    "        # # scoring = {'precision': score, 'f1':make_scorer(f1_score)}\n",
    "        # model = GridSearchCV(mods, parameters, scoring=score, cv=self.cross_validation_rounds, refit=refit, return_train_score=True, n_jobs=-1, verbose=2)\n",
    "        \n",
    "        # X_train = np.nan_to_num(X_train)\n",
    "        model = model.fit(X_train, y_train) \n",
    "        mod = copy.deepcopy(model) \n",
    "        mod_est = model.best_estimator_ \n",
    "        mod_par = model.best_params_\n",
    "        \n",
    "        # print('KKKKKKKKK-->>>', model, mod_est, mod_par)\n",
    "        model_scores = None \n",
    "        target_and_prediction_df = pd.DataFrame() \n",
    "        \n",
    "#         ### Rebuild the model with best parameter         \n",
    "#         # if should_validate:\n",
    "#         bst_parameters = model.best_params_\n",
    "#         self.logger.info(f\"\"\"Refitting the model with best parameter\"\"\")\n",
    "#         # mod = mod.set_params(**bst_parameters)\n",
    "#         mod = mod.best_estimator_.set_params(**bst_parameters)\n",
    "#         tmp_train_ids2 = np.concatenate((train_ids.copy(), val_ids.copy()))\n",
    "#         # X_train2, y_train2, _, _ = self._get_data_from_indices(tmp_train_ids2, from_training=True)  \n",
    "#         X_train2, y_train2, _, _ = self._get_data_from_indices(tmp_train_ids2, from_training=False)    \n",
    "#         mod = mod.fit(X_train2, y_train2)\n",
    "            \n",
    "#         X_val, y_val, meta_dat, _ = self._get_data_from_indices(val_ids) \n",
    "        X_val, y_val, meta_dat, _ = self._get_data_from_indices(tmp_val_ids, from_where=f'_VAL_ts_{ts_serial}_tr_{tr_serial}_mod_{mod_serial}')   \n",
    "#         X_val, y_val, meta_dat, _ = self._get_data_from_indices(tmp_train_ids)   \n",
    "        y_pred = mod.predict(X_val)  \n",
    "        y_pred_proba = mod.predict_proba(X_val) \n",
    "        target_and_prediction_df.reset_index(drop=True, inplace=True) \n",
    "        meta_dat.reset_index(drop=True, inplace=True) \n",
    "        target_and_prediction_df = pd.concat([target_and_prediction_df, meta_dat]) \n",
    "        target_and_prediction_df[self.class_name] = y_val \n",
    "        # target_and_prediction_df[f\"Prediction_{str(model.__class__.__name__)}\"] = y_pred \n",
    "        target_and_prediction_df[f\"Prediction_{ML_Classifiers.get_short_form(str(classifier_method.value))}\"] = y_pred  \n",
    "        for p_ind in range(y_pred_proba.shape[1]):\n",
    "            target_and_prediction_df[f\"Prediction_Proba_{p_ind}_{ML_Classifiers.get_short_form(str(classifier_method.value))}\"] = y_pred_proba[:, p_ind].tolist()\n",
    "            \n",
    "        model.feature_names = self.selected_features \n",
    "        model.feature_importance_scores = importance_scores\n",
    "        model_scores = self.calculate_model_scores(model, y_val, y_pred, y_pred_proba) \n",
    "\n",
    "        self.logger.info(f\"\"\"\n",
    "        Best model (GriveSearchCV): {model} \n",
    "        Best model: {mod} \n",
    "        Best estimator of the model: {mod_est} \n",
    "        Best parameters of the model: {mod_par} \n",
    "        Best model scores: {model_scores} \n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        \"\"\")\n",
    "\n",
    "        return model, model_scores, target_and_prediction_df, fold_info_df \n",
    "        # return model, model_scores, target_and_prediction_df, fold_info_df\n",
    "\n",
    "\n",
    "    def call_all_model_optimization(self, classifier_method, parameters, train_ids, val_ids, ts_serial, tr_serial, mod_serial, parameter_optimization):\n",
    "        model, model_scores, target_and_prediction, fold_info_df = None, None, None, None \n",
    "        if parameter_optimization == 1:\n",
    "            model, model_scores, target_and_prediction, fold_info_df = self.run_model_gridSearch(classifier_method, parameters, train_ids, val_ids, ts_serial, tr_serial, mod_serial)\n",
    "        elif parameter_optimization == 2:\n",
    "            model, model_scores, target_and_prediction, fold_info_df = self.run_model_randomizedSearch(classifier_method, parameters, train_ids, val_ids, ts_serial, tr_serial, mod_serial)\n",
    "        elif parameter_optimization == 3:\n",
    "            model, model_scores, target_and_prediction, fold_info_df = self.run_model_baysianSearch(classifier_method, parameters, train_ids, val_ids, ts_serial, tr_serial, mod_serial)\n",
    "        elif parameter_optimization == 4:\n",
    "            model, model_scores, target_and_prediction, fold_info_df = self.run_model_customGridSearch(classifier_method, parameters, train_ids, val_ids, ts_serial, tr_serial, mod_serial)\n",
    "        return model, model_scores, target_and_prediction, fold_info_df\n",
    "\n",
    "\n",
    "    def get_ml_model_instances(self, classifier_method, parameters=None):\n",
    "        classifier = None\n",
    "\n",
    "        ### GPU code START\n",
    "        global GPUs\n",
    "        global HAS_GPU\n",
    "\n",
    "        # GPUs = GPUtil.getGPUs()\n",
    "        # tot_gpus = len(GPUs)\n",
    "        # HAS_GPU = True if len(GPUs) > 0 else False\n",
    "        # avl_GPUIDs = GPUtil.getAvailable(order = 'first', limit = tot_gpus, maxLoad = 0.5, maxMemory = 0.5, includeNan=False, excludeID=[], excludeUUID=[])\n",
    "        # tot_avl_gpus = len(avl_GPUIDs)\n",
    "        # print(f'For GPU based tasks. There are {tot_gpus} GPUs in the system and {tot_avl_gpus} are available. \\nAvailable GPU IDs are: {avl_GPUIDs}')\n",
    "        allGPUs, bestGPU = HumachLab_Global.get_gpu_details(show_logs=False)\n",
    "        ### GPU code END\n",
    "\n",
    "        # ####### rf #######\n",
    "        # rf - random_forest classifier\n",
    "        if classifier_method == ML_Classifiers.RF:\n",
    "            classifier = RandomForestClassifier() if (parameters is None) else RandomForestClassifier(parameters)\n",
    "        # ####### knn #######\n",
    "        # knn - k_neares_neighbours classifier\n",
    "        elif classifier_method == ML_Classifiers.kNN:\n",
    "            classifier = KNeighborsClassifier() if (parameters is None) else KNeighborsClassifier(parameters)\n",
    "        # ####### nb #######\n",
    "        # knn - naieve bayes classifier\n",
    "        elif classifier_method == ML_Classifiers.NB:\n",
    "            classifier = GaussianNB() if (parameters is None) else GaussianNB(parameters)\n",
    "        # ####### svm/svc #######\n",
    "        # knn - support vector classifier\n",
    "        elif classifier_method == ML_Classifiers.SVC:\n",
    "            classifier = SVC() if (parameters is None) else SVC(parameters)\n",
    "        # ####### knn #######\n",
    "        # knn - k_neares_neighbours classifier\n",
    "        elif classifier_method == ML_Classifiers.DT:\n",
    "            classifier = DecisionTreeClassifier() if (parameters is None) else DecisionTreeClassifier(parameters)\n",
    "        # ####### LogReg #######\n",
    "        # LogReg - logistic regression classifier\n",
    "        elif classifier_method == ML_Classifiers.LogReg:\n",
    "            classifier = LogisticRegression() if (parameters is None) else LogisticRegression(parameters)\n",
    "        # ####### GBoost #######\n",
    "        # GBoost - gradient boosting classifier\n",
    "        elif classifier_method == ML_Classifiers.GBoost:\n",
    "            classifier = GradientBoostingClassifier() if (parameters is None) else GradientBoostingClassifier(parameters)\n",
    "        # ####### XGBoost #######\n",
    "        # GBoost - eXtreme gradient boosting classifier\n",
    "        elif classifier_method == ML_Classifiers.XGBoost:\n",
    "            classifier = XGBClassifier() if (parameters is None) else XGBClassifier(parameters)\n",
    "\n",
    "        ### GPU code - Comment it if no gpu available or not linux system or no support for RapidsAI package\n",
    "        # ####### gpu-rf #######\n",
    "        # gpu-rf - gpu-random_forest classifier\n",
    "        # elif classifier_method == ML_Classifiers.GPURF and tot_avl_gpus>0:\n",
    "        #     classifier = gpuRandomForestClassifier() if (parameters is None) else gpuRandomForestClassifier(parameters)\n",
    "\n",
    "        # ####### None #######\n",
    "        # No classifier\n",
    "        else:\n",
    "            self.logger.info(f'No classifier is selected...')\n",
    "\n",
    "        # ####### ####### #######\n",
    "        return classifier\n",
    "\n",
    "\n",
    "    def get_ml_scoring_metrices(self, reft=None):\n",
    "        model_scoring_mets = [ML_Performace_Metrics.ACC, ML_Performace_Metrics.PREC, ML_Performace_Metrics.RECL,\n",
    "                              ML_Performace_Metrics.SEN, ML_Performace_Metrics.SPEC, ML_Performace_Metrics.FPR,\n",
    "                              ML_Performace_Metrics.FNR, ML_Performace_Metrics.F1, ML_Performace_Metrics.ROC_AUC]\n",
    "\n",
    "        scoring = [ML_Performace_Metrics.ACC.value]\n",
    "        bst_mod_mets_1 = None\n",
    "        i = 0\n",
    "        for met in self.best_model_scoring_metrics:\n",
    "            if i==0:\n",
    "                scoring.clear()\n",
    "                if (reft is not None):\n",
    "                    if reft == ML_Performace_Metrics.F1SCR:\n",
    "                        reft = ML_Performace_Metrics.F1\n",
    "                    if (reft not in model_scoring_mets):\n",
    "                        reft = None\n",
    "\n",
    "            if met == ML_Performace_Metrics.F1SCR:\n",
    "                met = ML_Performace_Metrics.F1\n",
    "\n",
    "            if met in model_scoring_mets:\n",
    "                scoring.append(met.value)\n",
    "            i += 1\n",
    "\n",
    "        refit = (scoring[0]) if reft is None else reft.value\n",
    "        \n",
    "#         scoring = [ML_Performace_Metrics.F1]\n",
    "#         refit = True\n",
    "\n",
    "        return scoring, refit\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    def get_parameters_for_ml_models(self, classifier_method):\n",
    "        parameters = {}\n",
    "        if not self.should_use_params:\n",
    "            return parameters\n",
    "\n",
    "        # Parameter generation method name\n",
    "        method_name = f'{str(classifier_method.value)}_parameters'\n",
    "\n",
    "        try:\n",
    "            method = getattr(self, method_name)\n",
    "            # Call method for parameter generation\n",
    "            self.logger.info(f'Calling method: {method_name}')\n",
    "            parameters = method()\n",
    "        except AttributeError:\n",
    "            self.logger.warning(f'No such method exists with the name: {method_name}')\n",
    "            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.__class__.__name__, method_name))\n",
    "\n",
    "        # ####### ####### #######\n",
    "        return parameters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    def generate_parameter_dictionary(self, par_names, par_vals, par_ind):\n",
    "        self.logger.info(f'All parameters: {par_names}, {par_vals}, {par_ind}')\n",
    "        final_par_names = []\n",
    "        par_dict = {}\n",
    "\n",
    "#         for i in par_ind:\n",
    "#             pn = par_names[i]\n",
    "#             pv = par_vals[i]\n",
    "#             exec(f'{pn}={pv}')\n",
    "#             final_par_names.append(pn)\n",
    "        \n",
    "        sel_par = [pp for ii,pp in enumerate(par_names) if ii in par_ind] \n",
    "        for (pn, pv) in zip(sel_par, par_vals):\n",
    "            exec(f'{pn}={pv}')\n",
    "            final_par_names.append(pn)\n",
    "\n",
    "        for par in final_par_names:\n",
    "            par_dict[par] = eval(par)\n",
    "\n",
    "        return par_dict\n",
    "\n",
    "\n",
    "    # def float_range(self, start, stop, step):\n",
    "    #     start = decimal.Decimal(start)\n",
    "    #     stop = decimal.Decimal(stop)\n",
    "    #     while start < stop:\n",
    "    #         yield float(start)\n",
    "    #         start *= decimal.Decimal(step)\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    # Model parameter settings\n",
    "    # #########################################################################\n",
    "    # ### ML Classifier Method Parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def logistic_regression_parameters(self):\n",
    "\n",
    "        # ### Parameter generation using function\n",
    "        par_names = ['penalty', 'solver', 'max_iter', 'C']\n",
    "        par_vals = [['l1', 'l2', 'elasticnet'],\n",
    "                    ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "                    list(range(50, 5000, 10)),\n",
    "                    list(HumachLab_StaticMethods.float_range('0.001', '1.0', '0.01'))]\n",
    "\n",
    "        par_vals = [[30, 50, 75, 100, 200, 500, 750, 1000], [2, 3, 5, 7], [5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000]]\n",
    "        par_vals = [[5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000]]\n",
    "        par_vals = [[15, 21, 30, 50, 75, 100, 200, 500]]\n",
    "        par_vals = [[50, 75, 100]]\n",
    "        par_vals = [[15, 21, 30, 50, 75, 100, 200, 500], [3, 5, 7, 10, 15, 20, 25, 30]]\n",
    "        par_vals = [['l1', 'l2', 'elasticnet'],[50, 100, 130, 150, 170, 200, 250, 350, 500, 750, 1000]]\n",
    "        par_vals = [['l1', 'l2', 'elasticnet'],[50, 100, 130, 150, 170, 200],[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]]\n",
    "        par_vals = [['l2', 'elasticnet'],[50, 100, 130, 150, 170, 200],[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]]\n",
    "        par_ind = [0, 2, 3]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def k_nearest_neighbors_parameters(self):\n",
    "\n",
    "        # ### Parameter generation using function\n",
    "        par_names = ['n_neighbors', 'p', 'metric', 'n_splits']\n",
    "        par_vals = [list(range(2, 100)),\n",
    "                    list(range(2, 100)),\n",
    "                    ['manhattan', 'minkowski', 'euclidean'],\n",
    "                    list(range(2, 10))]\n",
    "\n",
    "        par_vals = [list(range(100, 1000, 50)), list(range(2, 11, 1)), [2, 3, 5, 9, 13, 19, 29]]\n",
    "        par_vals = [[2, 3, 5, 9, 13, 19, 29]]\n",
    "        par_vals = [[2, 3, 5, 10, 15, 25, 35], ['manhattan', 'minkowski', 'euclidean']]\n",
    "        par_ind = [0, 2]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def naive_bayes_parameters(self):\n",
    "\n",
    "        # ### Parameter generation using function\n",
    "        par_names = ['var_smoothing']\n",
    "        par_vals = [list(np.logspace(0, -9, num=100))]\n",
    "        par_vals = [list(np.logspace(0, -9, num=100))]\n",
    "\n",
    "        # par_vals = []\n",
    "        # par_vals = []\n",
    "        par_vals = [list(np.logspace(0,-9, num=5))]\n",
    "        par_ind = [0]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "\n",
    "\n",
    "    def support_vector_classifier_parameters(self):\n",
    "\n",
    "        # ### Parameter generation using function #, probability=True\n",
    "        par_names = ['probability', 'C', 'kernel', 'gamma', 'degree', 'class_weightdict']\n",
    "        par_vals = [[True],\n",
    "                    list(HumachLab_StaticMethods.float_range('0.001', '1', '0.01')),\n",
    "                    ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "                    list(HumachLab_StaticMethods.float_range('0.000001', '1', '10')),\n",
    "                    list(range(1, 10)),\n",
    "                    [None, 'balanced']]\n",
    "\n",
    "        # par_vals = [list(HumachLab_StaticMethods.float_range('0.000001', '1', '10')), list(HumachLab_StaticMethods.float_range('0.00001', '1', '10')), list(HumachLab_StaticMethods.float_range('0.0001', '1', '10'))]\n",
    "        par_vals = [list(HumachLab_StaticMethods.float_range('0.001', '1.', '0.1')), ['linear', 'rbf', 'poly']]\n",
    "        par_vals = [[True],[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0], ['linear', 'rbf', 'poly']]\n",
    "        par_ind = [0, 1, 2]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def decision_tree_parameters(self):\n",
    "\n",
    "        # ### Parameter generation using function\n",
    "        par_names = ['max_depth', 'criterion', 'splitter', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes']\n",
    "        par_vals = [list(range(1, 100)),\n",
    "                    ['gini', 'entropy', 'log_loss'],\n",
    "                    ['best', 'random'],\n",
    "                    list(range(1, 10)),\n",
    "                    list(range(1, 10)),\n",
    "                    list(range(1, 100))]\n",
    "\n",
    "        par_vals = [list(range(1, 100)), list(range(1, 100, 2)), list(range(1, 100, 3))]\n",
    "        par_vals = [list(range(1, 100))]\n",
    "        par_vals = [[2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']]\n",
    "        par_ind = [0, 1]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def random_forest_parameters(self): \n",
    "\n",
    "        # ### Parameter generation using function\n",
    "        par_names = ['n_estimators', 'max_depth', 'criterion', 'splitter', 'max_features', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes']\n",
    "        par_vals = [list(range(1, 500, 5)),\n",
    "                    list(range(1, 100)),\n",
    "                    ['gini', 'entropy', 'log_loss'], \n",
    "                    ['best', 'random'],\n",
    "                    list(range(2, 20, 1)),\n",
    "                    list(range(1, 10)),\n",
    "                    list(range(1, 10)),\n",
    "                    list(range(1, 100))]\n",
    "\n",
    "        par_vals = [[30, 50, 75, 100, 200, 500, 750, 1000], [2, 3, 5, 7], [5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000]]\n",
    "        par_vals = [[5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000]]\n",
    "        par_vals = [[15, 21, 30, 50, 75, 100, 200, 500]]\n",
    "        par_vals = [[50, 75, 100]]\n",
    "        par_vals = [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], ['gini', 'entropy', 'log_loss']]\n",
    "        # par_vals = [[2, 3, 5, 7]]\n",
    "        par_ind = [0, 1, 2]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def graphics_processing_unit_random_forest(self):\n",
    "\n",
    "        # ### Parameter generation using function\n",
    "        par_names = ['n_estimators', 'n_bins', 'n_streams', 'max_depth', 'max_features', 'criterion', 'splitter', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes']\n",
    "        par_vals = [list(range(1, 500, 5)),\n",
    "                    list(range(1, 100)),\n",
    "                    list(range(2, 20, 1)),\n",
    "                    ['gini', 'entropy', 'log_loss'], \n",
    "                    ['best', 'random'],\n",
    "                    list(range(1, 10)),\n",
    "                    list(range(1, 10)),\n",
    "                    list(range(1, 100))]\n",
    "\n",
    "        par_vals = [[30, 50, 75, 100, 200, 500, 750, 1000], [2, 3, 5, 7], [5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000]]\n",
    "        par_vals = [[5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000], 15, 8]\n",
    "        # par_vals = [[2, 3, 5, 7]]\n",
    "        par_ind = [0, 1, 2]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def gradient_boosting_parameters(self):\n",
    "\n",
    "        # ### Parameter generation using function \n",
    "        par_names = ['n_estimators', 'max_depth', 'learning_rate', 'max_features', 'loss', 'min_samples_split', 'min_samples_leaf']\n",
    "        par_vals = [list(range(1, 500, 5)),\n",
    "                    list(range(1, 100)),\n",
    "                    list(HumachLab_StaticMethods.float_range('0.001', '1', '0.01')), \n",
    "                    list(range(2, 20, 1)),\n",
    "                    ['log_loss', 'exponential'],\n",
    "                    list(range(1, 10)),\n",
    "                    list(range(1, 10))]\n",
    "\n",
    "        par_vals = [[30, 50, 75, 100, 200, 500, 750, 1000], [2, 3, 5, 7], [5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000]]\n",
    "        par_vals = [[5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000]]\n",
    "        par_vals = [[15, 21, 30, 50, 75, 100, 200, 500]]\n",
    "        par_vals = [[50, 75, 100]]\n",
    "        par_vals = [[15, 21, 30, 50, 75, 100, 200, 500], [3, 5, 7, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.5, 1.0]]\n",
    "        par_vals = [[3, 5, 10, 15, 21, 30, 50, 75, 100], [2, 3, 5, 7, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.5, 1.0]]\n",
    "        # par_vals = [[2, 3, 5, 7]]\n",
    "        par_ind = [0, 1, 2]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def xtreme_gradient_boosting_parameters(self): #xtreme_gradient_boosting\n",
    "\n",
    "        # ### Parameter generation using function \n",
    "        par_names = ['max_depth', 'eta', 'max_leaves']\n",
    "        par_vals = [list(range(1, 100)),\n",
    "                    list(HumachLab_StaticMethods.float_range('0.001', '1', '0.01')),\n",
    "                    list(range(0, 20, 1)) ]\n",
    "\n",
    "        par_vals = [[3, 6, 10, 15, 25, 40, 100, 250, 500, 750, 1000], [0.01, 0.05, 0.1, 0.3, 0.5, 1.0], [0, 5, 7, 11, 15, 21, 30, 50, 75, 100, 200, 500, 750, 1000]]\n",
    "        par_vals = [[2, 3, 6, 10, 15, 20, 25, 30], [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]]\n",
    "        # par_vals = [[2, 3, 5, 7]]\n",
    "        par_ind = [0, 1]\n",
    "        parameters = self.generate_parameter_dictionary(par_names, par_vals, par_ind)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    # Calculate and save classification details and model scores\n",
    "    # #########################################################################\n",
    "    #############################\n",
    "\n",
    "    def calculate_model_scores(self, mods, y_test, y_pred, y_pred_proba): \n",
    "#         print(y_test, '\\n', y_pred, '\\n', y_pred_proba, '\\n')\n",
    "#         target_labels = np.unique(np.array(y_test)).tolist() \n",
    "        target_labels = sorted( self.dataset[self.class_name].unique().tolist() )\n",
    "        \n",
    "        y_pred = y_pred.tolist() \n",
    "        perf_scores = self.calculate_performance_scores(y_test, y_pred, y_pred_proba, labels=target_labels)  # average = 'weighted', 'macro', 'micro' \n",
    "        confMat = perf_scores['Conf_Mat']\n",
    "\n",
    "        acc = round(perf_scores['ACC'], 3) #Accuracy score or total correct prediction rate \n",
    "        prec = round(perf_scores['PREC'], 3) #precision or positive predictive value (PPV)\n",
    "        reca_sens = round(perf_scores['REC'], 3) #sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "        spec = round(perf_scores['SPE'], 3) #specificity, selectivity or true negative rate (TNR)\n",
    "        f1sc = round(perf_scores['F1SCR'], 3)\n",
    "        auc_s = round(perf_scores['AUC'], 3) \n",
    "        \n",
    "        scr_dict = {'method': str(mods), 'model': mods.best_estimator_, 'model_parameters': mods.best_params_, \n",
    "                    'model_scores': round(mods.best_score_*100,2),\n",
    "                    ML_Performace_Metrics.CONF_MAT.value: confMat, ML_Performace_Metrics.ACC.value: acc, ML_Performace_Metrics.PREC.value: prec,\n",
    "                    ML_Performace_Metrics.RECL.value: reca_sens, ML_Performace_Metrics.SEN.value: reca_sens, ML_Performace_Metrics.SPEC.value: spec,\n",
    "                    ML_Performace_Metrics.F1SCR.value: f1sc, ML_Performace_Metrics.ROC_AUC.value: auc_s}\n",
    "        \n",
    "#         scr_dict = {'method_class': str(mods.__class__.__name__), 'model_name': mods.best_estimator_, 'model_parameters': mods.best_params_, \n",
    "#                     'model_scores': round(mods.best_score_*100,2),\n",
    "#                     ML_Performace_Metrics.CONF_MAT.value: confMat, ML_Performace_Metrics.ACC.value: acc, ML_Performace_Metrics.PREC.value: prec,\n",
    "#                     ML_Performace_Metrics.RECL.value: reca_sens, ML_Performace_Metrics.SEN.value: reca_sens, ML_Performace_Metrics.SPEC.value: spec,\n",
    "#                     ML_Performace_Metrics.F1SCR.value: f1sc, ML_Performace_Metrics.ROC_AUC.value: auc_s}\n",
    "#         scr_dict = {'model_class': str(mods.__class__.__name__), 'method_name': mods.estimator, 'method_parameters': mods.best_params_, \n",
    "#                     'method_scores': round(mods.best_score_*100,2),\n",
    "#                     ML_Performace_Metrics.CONF_MAT.value: confMat, ML_Performace_Metrics.ACC.value: acc, ML_Performace_Metrics.PREC.value: prec,\n",
    "#                     ML_Performace_Metrics.RECL.value: reca_sens, ML_Performace_Metrics.SEN.value: reca_sens, ML_Performace_Metrics.SPEC.value: spec,\n",
    "#                     ML_Performace_Metrics.FPR.value: fpr, ML_Performace_Metrics.FNR.value: fnr, ML_Performace_Metrics.F1SCR.value: f1sc, ML_Performace_Metrics.ROC_AUC.value: auc_s}\n",
    "#         scr_dict = {'model_class': str(mods.__class__.__name__), 'method_name': str(mods), 'method_parameters': mods.best_params_, \n",
    "#                     'method_scores': round(mods.best_score_*100,2),\n",
    "#                     ML_Performace_Metrics.CONF_MAT.value: confMat, ML_Performace_Metrics.ACC.value: acc, ML_Performace_Metrics.PREC.value: prec,\n",
    "#                     ML_Performace_Metrics.RECL.value: reca_sens, ML_Performace_Metrics.SEN.value: reca_sens, ML_Performace_Metrics.SPEC.value: spec,\n",
    "#                     ML_Performace_Metrics.FPR.value: fpr, ML_Performace_Metrics.FNR.value: fnr, ML_Performace_Metrics.F1SCR.value: f1sc, ML_Performace_Metrics.ROC_AUC.value: auc_s}\n",
    "        scr_df = pd.DataFrame([list( scr_dict.values() )], columns=list( scr_dict.keys() )) \n",
    "        self.logger.info(f\"\"\"Score columns: {scr_df.shape} {scr_df.columns.values.tolist()}\"\"\") \n",
    "\n",
    "        return scr_df\n",
    "    \n",
    "    \n",
    "    def calculate_performance_scores(self, y_true, y_pred, y_pred_proba, labels=[0, 1], verbose=2, average='weighted'): # average = 'macro', 'micro', 'weighted' \n",
    "        #### SOURCES: https://www.youtube.com/watch?v=PCHf_7jBor8 \n",
    "        # https://www.mariakhalusova.com/posts/2019-04-17-ml-model-evaluation-metrics-p2/ \n",
    "        # https://www.mariakhalusova.com/posts/2019-04-11-ml-model-evaluation-metrics-p1/\n",
    "        # https://www.evidentlyai.com/classification-metrics/multi-class-metrics \n",
    "        # https://www.kaggle.com/code/nkitgupta/evaluation-metrics-for-multi-class-classification \n",
    "\n",
    "        model_scores = []\n",
    "        true_label_uniq = np.unique(np.array(y_true)).tolist() \n",
    "        print(np.unique(np.array(y_true)), np.unique(np.array(y_pred)))\n",
    "        print(y_true, y_pred) \n",
    "        conf_matrix = confusion_matrix(y_true, y_pred, labels=labels).tolist()\n",
    "        print(np.array(conf_matrix) )\n",
    "\n",
    "        ### For micro averaging and binary class \n",
    "        conf_matrix_arr = np.array(conf_matrix) \n",
    "        one_vs_all_confMat = []     \n",
    "        for label in labels:\n",
    "            tp_lbl = conf_matrix_arr[label, label] \n",
    "            fp_lbl = np.sum(conf_matrix_arr[:, label])-tp_lbl \n",
    "            fn_lbl = np.sum(conf_matrix_arr[label, :])-tp_lbl \n",
    "            tn_lbl = np.sum(conf_matrix_arr)-(tp_lbl+fp_lbl+fn_lbl) \n",
    "            one_vs_all_confMat.append([tn_lbl, fp_lbl, fn_lbl, tp_lbl]) \n",
    "        print(np.array(one_vs_all_confMat)) \n",
    "\n",
    "        tn_tot = np.sum( np.array(one_vs_all_confMat)[:, 0] ) \n",
    "        fp_tot = np.sum( np.array(one_vs_all_confMat)[:, 1] )  \n",
    "        fn_tot = np.sum( np.array(one_vs_all_confMat)[:, 2] )  \n",
    "        tp_tot = np.sum( np.array(one_vs_all_confMat)[:, 3] )\n",
    "\n",
    "        conf_matrix_tol = [[tn_tot, fp_tot], [fn_tot, tp_tot]] \n",
    "        print(np.array(conf_matrix_tol)) \n",
    "\n",
    "        if len(labels)==2:\n",
    "            tn_tot = one_vs_all_confMat[1][0] \n",
    "            fp_tot = one_vs_all_confMat[1][1] \n",
    "            fn_tot = one_vs_all_confMat[1][2] \n",
    "            tp_tot = one_vs_all_confMat[1][3] \n",
    "            average = \"micro\"\n",
    "\n",
    "        result = [] \n",
    "        for label in labels:\n",
    "            precision, recall, fscore, support = precision_recall_fscore_support( np.array(y_true)==label, np.array(y_pred)==label ) \n",
    "            # tmp_fpr, tmp_tpr, tmp_thresholds = roc_curve(np.array(y_true)==label, np.array(y_pred)==label, pos_label=label) \n",
    "            tmp_fpr, tmp_tpr, tmp_thresholds = roc_curve(np.array(y_true)==label, np.array(y_pred)==label) \n",
    "            auc_score = auc(tmp_fpr, tmp_tpr)*100 \n",
    "\n",
    "            if label in true_label_uniq: \n",
    "                result.append( [label, precision[1], recall[1], recall[1], recall[0], fscore[1], auc_score, support[1]] ) \n",
    "            else:\n",
    "                result.append( [label, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0] ) \n",
    "\n",
    "            accuracy = accuracy_score(np.array(y_true)==label, np.array(y_pred)==label)*100 \n",
    "            if verbose>1:\n",
    "                print(\n",
    "                    f'Class-wise info: For multilevel internal scores fo label {label}: \\n', \n",
    "                    f'Accuracy = {accuracy}\\n', \n",
    "                    f'Precision = {precision}\\n', \n",
    "                    f'Recall = {recall}\\n', \n",
    "                    f'F1 score = {fscore}\\n', \n",
    "                    f'AUC score = {auc_score}\\n', \n",
    "                    f'Support = {support}\\n', \n",
    "                )\n",
    "        tdf = pd.DataFrame(result, columns=['Label', 'Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', 'AUC', 'Support']) \n",
    "\n",
    "        if average=='macro': #average = \"weighted\", \"macro\", \"micro\" \n",
    "            tdf = tdf[['Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', 'AUC']].apply(lambda col:np.mean(col), axis=0) \n",
    "        elif average=='micro':\n",
    "            prc = (tp_tot / (tp_tot+fp_tot))*100 if (tp_tot+fp_tot)!=0 else 0.0 #precision or positive predictive value (PPV)\n",
    "            rec = (tp_tot / (tp_tot+fn_tot))*100 if (tp_tot+fn_tot)!=0 else 0.0 #sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "            sns = rec #sensitivity same as recall \n",
    "            spc = (tn_tot / (tn_tot+fp_tot))*100 if (tn_tot+fp_tot)!=0 else 0.0 #specificity, selectivity or true negative rate (TNR)\n",
    "            f1s = (2*tp_tot / (2*tp_tot+fp_tot+fn_tot))*100 if (2*tp_tot+fp_tot+fn_tot)!=0 else 0.0 #specificity, selectivity or true negative rate (TNR)  \n",
    "            auc_s = roc_auc_score(y_true, y_pred) if len(labels)==2 else roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average=average) #multi_class='ovo', 'ovr' \n",
    "#             auc_s = 0.5\n",
    "#             if len(labels)==2:\n",
    "#                 auc_s = roc_auc_score(y_true, y_pred)  \n",
    "#             else:\n",
    "#                 auc_s = roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average=average) #multi_class='ovo', 'ovr' \n",
    "            tdf = pd.Series([prc, rec, sns, spc, f1s, auc_s], index=['Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', 'AUC'])  \n",
    "        else: ## Default = weighted\n",
    "            class_weights = tdf['Support']/tdf['Support'].sum() \n",
    "            tdf = tdf[['Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', 'AUC']].apply(lambda col:np.sum(col*class_weights), axis=0) \n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)*100 \n",
    "        tdf_summary = pd.Series([conf_matrix, acc, tdf['Precision'], tdf['Recall'], tdf['Sensitivity'], tdf['Specificity'], tdf['F1 Score'], tdf['AUC']],\n",
    "                               index=['Conf_Mat', 'ACC', 'PREC', 'REC', 'SEN', 'SPE', 'F1SCR', 'AUC'])\n",
    "\n",
    "        if verbose>1:\n",
    "            confMat = tdf_summary['Conf_Mat']\n",
    "            acc = round(tdf_summary['ACC'], 3) #Accuracy score or total correct prediction rate \n",
    "            prec = round(tdf_summary['PREC'], 3) #precision or positive predictive value (PPV)\n",
    "            reca_sens = round(tdf_summary['REC'], 3) #sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "            spec = round(tdf_summary['SPE'], 3) #specificity, selectivity or true negative rate (TNR)\n",
    "            f1sc = round(tdf_summary['F1SCR'], 3)\n",
    "            auc_s = round(tdf_summary['AUC'], 3) \n",
    "            print(\n",
    "                f'CLASSIFICATION MERICS:\\n',\n",
    "                f'{\"_\"*55}\\n',\n",
    "                f'Confusion Matrix: \\n{np.array(conf_matrix)}\\n',\n",
    "                f'Accuracy (acc): {acc}\\n',\n",
    "                f'Precision (prc): {prec}\\n',\n",
    "                f'Recall (rec): {reca_sens}\\n',\n",
    "                f'Sensitivity (sns): {reca_sens}\\n',\n",
    "                f'Specificity (spc): {spec}\\n',\n",
    "                f'F1 Score (f1s): {f1sc}\\n',\n",
    "                f'ROC AUC (AUC): {auc_s}',\n",
    "            )\n",
    "\n",
    "        return tdf_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b00e030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018300a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
